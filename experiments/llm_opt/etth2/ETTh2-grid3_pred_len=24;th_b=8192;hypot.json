[
  {
    "trial": 1,
    "timestamp": "2025-10-28T07:08:38.612197",
    "mae_val": 0.37560105323791504,
    "mse_val": 0.2205420434474945,
    "mae_test": 0.331920325756073,
    "mse_test": 0.19285167753696442,
    "seq_len": 96,
    "label_len": 48,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 5,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "This configuration uses a moderate input sequence length and standard dimension size to balance computational cost and performance, assuming 96 hours of input is sufficient context for hourly ETTh data."
  },
  {
    "trial": 2,
    "timestamp": "2025-10-28T07:13:17.821123",
    "mae_val": 0.4544942080974579,
    "mse_val": 0.3157709836959839,
    "mae_test": 0.4860756993293762,
    "mse_test": 0.35753193497657776,
    "seq_len": 336,
    "label_len": 96,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 3,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "We test the impact of a significantly longer context (336 hours) combined with a deep, narrow model (d_model=256, high e_layers) to capture complex long-term patterns, utilizing higher dropout to mitigate overfitting."
  },
  {
    "trial": 3,
    "timestamp": "2025-10-28T07:33:15.461719",
    "mae_val": 0.5876988768577576,
    "mse_val": 0.507473349571228,
    "mae_test": 0.524624228477478,
    "mse_test": 0.4008888304233551,
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 8,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "Test the limits of long sequence modeling by using the maximum available input context (720) and the largest model dimensions, combined with a low dropout rate, aiming to capture the most comprehensive long-range dependencies in the hourly data."
  },
  {
    "trial": 4,
    "timestamp": "2025-10-28T07:36:24.681197",
    "mae_val": 0.2549651861190796,
    "mse_val": 0.10392702370882034,
    "mae_test": 0.3082154095172882,
    "mse_test": 0.1788458675146103,
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 10,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "Explore the performance of a highly efficient model focusing on short-term prediction (48 hours), utilizing a shallow encoder and a relatively deep decoder, while employing the highest ProbSparse factor (10) for computational savings."
  },
  {
    "trial": 5,
    "timestamp": "2025-10-28T07:41:27.382518",
    "mae_val": 0.37144696712493896,
    "mse_val": 0.21128039062023163,
    "mae_test": 0.37848687171936035,
    "mse_test": 0.23090659081935883,
    "seq_len": 168,
    "label_len": 48,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 5,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "Test a deep but narrow model (high e_layers, low d_model) using a medium sequence length (168 hours) to see if maximum encoder depth can effectively extract features from a weekly context, maintaining computational efficiency."
  },
  {
    "trial": 6,
    "timestamp": "2025-10-28T07:56:49.573881",
    "mae_val": 0.570127546787262,
    "mse_val": 0.44091978669166565,
    "mae_test": 0.6482431292533875,
    "mse_test": 0.581476628780365,
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 3,
    "d_model": 512,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Investigate if utilizing the maximum context length (720 hours) requires high regularization (high dropout) and sufficient model depth (d_layers=3), while using the minimum sparsity factor (3) to effectively process complex long-range dependencies."
  },
  {
    "trial": 7,
    "timestamp": "2025-10-28T08:09:28.266637",
    "mae_val": 0.414836049079895,
    "mse_val": 0.2775074541568756,
    "mae_test": 0.44876518845558167,
    "mse_test": 0.3168169856071472,
    "seq_len": 336,
    "label_len": 48,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 8,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "We investigate the performance of a wide model (d_model=768) on a bi-weekly context (336), using a highly sparse attention mechanism (factor=8) and low dropout, aiming to capture complex patterns with maximal representation power in a shallow decoder setup."
  },
  {
    "trial": 8,
    "timestamp": "2025-10-28T08:14:43.210851",
    "mae_val": 0.4087747037410736,
    "mse_val": 0.2688145339488983,
    "mae_test": 0.36429378390312195,
    "mse_test": 0.2376231700181961,
    "seq_len": 96,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 3,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "This configuration explores if maximum depth (e_layers=6, d_layers=3) can compensate for minimum model width (d_model=256) when predicting a short horizon (24) from a moderate context (96), enforcing strict attention (factor=3) and high regularization."
  },
  {
    "trial": 9,
    "timestamp": "2025-10-28T08:24:41.626432",
    "mae_val": 0.3458625078201294,
    "mse_val": 0.18691934645175934,
    "mae_test": 0.36534619331359863,
    "mse_test": 0.20770908892154694,
    "seq_len": 336,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 5,
    "d_model": 768,
    "d_ff": 2048,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "We investigate if a significantly wide model (d_model=768) combined with a bi-weekly context (336 hours) can yield superior results for short-term prediction (24 hours), balancing large representational capacity with standard encoder/decoder depth."
  },
  {
    "trial": 10,
    "timestamp": "2025-10-28T08:30:46.003627",
    "mae_val": 0.434669554233551,
    "mse_val": 0.27920952439308167,
    "mae_test": 0.38474032282829285,
    "mse_test": 0.2336071878671646,
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 8,
    "d_model": 512,
    "d_ff": 3072,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "This configuration tests maximum encoder depth (e_layers=6) and a highly sparse attention mechanism (factor=8) on a weekly context (168) for medium-range prediction (96), relying on the deep encoder and low dropout to capture necessary features despite a shallow decoder."
  },
  {
    "trial": 11,
    "timestamp": "2025-10-28T08:37:03.052691",
    "mae_val": 0.6438806056976318,
    "mse_val": 0.5893883109092712,
    "mae_test": 0.6007683873176575,
    "mse_test": 0.5565619468688965,
    "seq_len": 720,
    "label_len": 96,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 5,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "We explore if maximum encoder depth (e_layers=6) and maximum context length (720 hours) can effectively extract features for medium-range prediction (96 hours), even when using the minimum model width (d_model=256) and standard dropout."
  },
  {
    "trial": 12,
    "timestamp": "2025-10-28T08:42:28.254110",
    "mae_val": 0.2829028367996216,
    "mse_val": 0.12923157215118408,
    "mae_test": 0.3016546666622162,
    "mse_test": 0.157507985830307,
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 3,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "This configuration tests maximum model width (d_model=768) and minimal sparsity (factor=3) on a short sequence (48 hours), aiming for high expressiveness with a deep encoder and minimal regularization, relying on the short context to prevent immediate overfitting."
  },
  {
    "trial": 13,
    "timestamp": "2025-10-28T08:51:29.921608",
    "mae_val": 0.42065656185150146,
    "mse_val": 0.2695786952972412,
    "mae_test": 0.39155277609825134,
    "mse_test": 0.24615292251110077,
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 5,
    "d_model": 768,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "We test if a powerful, wide model (d_model=768) combined with maximum decoder depth (d_layers=3) can effectively handle the challenging medium-range forecast (96 hours) based on a full weekly context (168 hours)."
  },
  {
    "trial": 14,
    "timestamp": "2025-10-28T08:55:57.252880",
    "mae_val": 0.41577523946762085,
    "mse_val": 0.2654317617416382,
    "mae_test": 0.4927961528301239,
    "mse_test": 0.39044806361198425,
    "seq_len": 720,
    "label_len": 48,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 10,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "We test a highly constrained, efficient model (minimal depth/width/complexity factor) on the longest sequence length (720 hours) to determine if short-term prediction (48 hours) benefits from maximum context, even with resource limitations."
  },
  {
    "trial": 15,
    "timestamp": "2025-10-28T09:00:29.565691",
    "mae_val": 0.3047780990600586,
    "mse_val": 0.1517353504896164,
    "mae_test": 0.2982257306575775,
    "mse_test": 0.16074110567569733,
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 5,
    "d_model": 512,
    "d_ff": 1024,
    "d_layers": 1,
    "dropout": 0.2,
    "hypothesis": "Investigate if maximizing encoder depth (e_layers=6) and utilizing high regularization (dropout=0.2) can effectively extract features from a short 48-hour context for short-term prediction, while maintaining efficiency with a shallow decoder and minimal FFN dimension."
  },
  {
    "trial": 16,
    "timestamp": "2025-10-28T09:11:35.789041",
    "mae_val": 0.30152979493141174,
    "mse_val": 0.14071640372276306,
    "mae_test": 0.358649343252182,
    "mse_test": 0.21332043409347534,
    "seq_len": 168,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 10,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.05,
    "hypothesis": "Test the limits of maximal model capacity (max d_model, max d_ff, max d_layers) combined with maximum sparsity (factor=10) when predicting a short horizon (24 hours) from a weekly context (168 hours), minimizing regularization to maximize representation power."
  },
  {
    "trial": 17,
    "timestamp": "2025-10-28T09:19:04.193103",
    "mae_val": 0.29816222190856934,
    "mse_val": 0.136165589094162,
    "mae_test": 0.3306206166744232,
    "mse_test": 0.18249139189720154,
    "seq_len": 168,
    "label_len": 48,
    "e_layers": 2,
    "n_heads": 16,
    "factor": 8,
    "d_model": 768,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "We test a highly expressive but shallow encoder (e_layers=2, d_model=768) combined with a maximal decoder depth (d_layers=3) on a weekly context (168) for medium-short prediction (48), relying on high dropout to handle the large model capacity."
  },
  {
    "trial": 18,
    "timestamp": "2025-10-28T09:28:38.626643",
    "mae_val": 0.5053879022598267,
    "mse_val": 0.3789045214653015,
    "mae_test": 0.4915217161178589,
    "mse_test": 0.365090936422348,
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 10,
    "d_model": 512,
    "d_ff": 3072,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "We explore the challenge of predicting a very long horizon (168 hours) based on a two-week input (336), maximizing sparsity (factor=10) and FFN capacity (d_ff=3072) while utilizing minimal decoder depth and low dropout."
  },
  {
    "trial": 19,
    "timestamp": "2025-10-28T09:31:44.890782",
    "mae_val": 0.31500574946403503,
    "mse_val": 0.15025335550308228,
    "mae_test": 0.3678860664367676,
    "mse_test": 0.2097320258617401,
    "seq_len": 336,
    "label_len": 96,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 10,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "We explore if minimum architectural complexity (low e_layers, d_model, d_ff) coupled with maximum sparsity (factor=10) can efficiently handle a bi-weekly context (336) for predicting a medium horizon (96 hours)."
  },
  {
    "trial": 20,
    "timestamp": "2025-10-28T09:40:26.685093",
    "mae_val": 0.326988160610199,
    "mse_val": 0.17560365796089172,
    "mae_test": 0.3193569779396057,
    "mse_test": 0.18759310245513916,
    "seq_len": 96,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 5,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.05,
    "hypothesis": "We test the effect of maximizing model capacity (d_model=768, d_ff=3072, d_layers=3) and minimizing regularization (dropout=0.05) on a highly successful short prediction task (24 hours) using a moderate context (96 hours)."
  },
  {
    "trial": 21,
    "timestamp": "2025-10-28T09:51:27.199463",
    "mae_val": 0.48813360929489136,
    "mse_val": 0.3616565763950348,
    "mae_test": 0.4685038924217224,
    "mse_test": 0.34699204564094543,
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 10,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "We test the combination of maximum encoder and decoder depth (e_layers=6, d_layers=3) and maximum sparsity (factor=10) on a challenging weekly prediction task (label_len=168), aiming to maximize feature extraction efficiency for long horizons based on bi-weekly input."
  },
  {
    "trial": 22,
    "timestamp": "2025-10-28T09:54:13.597707",
    "mae_val": 0.34785687923431396,
    "mse_val": 0.18767552077770233,
    "mae_test": 0.39603081345558167,
    "mse_test": 0.2469760924577713,
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 3,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 1,
    "dropout": 0.2,
    "hypothesis": "We explore a highly constrained and regularized model (min width/FFN, max dropout) on a short sequence (48 hours), using maximum encoder depth (e_layers=6) and minimum sparsity (factor=3) to maximize feature extraction despite the limited model capacity and context."
  },
  {
    "trial": 23,
    "timestamp": "2025-10-28T09:59:08.957308",
    "mae_val": 0.328934907913208,
    "mse_val": 0.17183688282966614,
    "mae_test": 0.3251766264438629,
    "mse_test": 0.18215888738632202,
    "seq_len": 96,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 10,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 2,
    "dropout": 0.05,
    "hypothesis": "We test if a balanced, moderately deep model (e_layers=4, d_model=512) optimized for maximum sparsity (factor=10) and minimal regularization can achieve superior performance on the highly successful short-term prediction task (label_len=24) using a 96-hour context."
  },
  {
    "trial": 24,
    "timestamp": "2025-10-28T10:07:02.835091",
    "mae_val": 0.568120539188385,
    "mse_val": 0.4597220718860626,
    "mae_test": 0.6313228607177734,
    "mse_test": 0.5903766751289368,
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 3,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "We explore whether maximizing model depth (e_layers=6, d_layers=3) and applying strong regularization can successfully extract features from the maximum context (720 hours) to predict the difficult long horizon (336 hours), despite limiting the model width (d_model=256) and minimizing sparsity (factor=3)."
  },
  {
    "trial": 25,
    "timestamp": "2025-10-28T10:15:19.273301",
    "mae_val": 0.5419490337371826,
    "mse_val": 0.4275898337364197,
    "mae_test": 0.6918472647666931,
    "mse_test": 0.7322287559509277,
    "seq_len": 720,
    "label_len": 96,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 10,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "We test if maximizing architectural depth (e_layers=6, d_layers=3) and sparsity (factor=10) can efficiently leverage the maximum historical context (720 hours) to predict a medium horizon (96 hours), despite using the minimum model width (d_model=256)."
  },
  {
    "trial": 26,
    "timestamp": "2025-10-28T10:37:16.553956",
    "mae_val": 0.5301657915115356,
    "mse_val": 0.4030682146549225,
    "mae_test": 0.6285238265991211,
    "mse_test": 0.5499544143676758,
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 5,
    "d_model": 768,
    "d_ff": 2048,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "We explore a configuration tailored for extremely long-range forecasting (336 hours prediction from 720 hours context) by maximizing model width (d_model=768) and using a balanced, standard depth and regularization, aiming for maximum expressive power on this challenging task."
  },
  {
    "trial": 27,
    "timestamp": "2025-10-28T10:42:00.819200",
    "mae_val": 0.3185153007507324,
    "mse_val": 0.16142700612545013,
    "mae_test": 0.3040894567966461,
    "mse_test": 0.16205309331417084,
    "seq_len": 96,
    "label_len": 24,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 8,
    "d_model": 512,
    "d_ff": 3072,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "We test a balanced architecture (d_model=512, d_layers=2) with high attention capacity (n_heads=16) and maximum FFN dimension, seeking improved performance for short-term prediction (24 hours) from a moderate 96-hour context, utilizing high sparsity."
  },
  {
    "trial": 28,
    "timestamp": "2025-10-28T10:45:18.621151",
    "mae_val": 0.38548099994659424,
    "mse_val": 0.2242080420255661,
    "mae_test": 0.4200649559497833,
    "mse_test": 0.2795471251010895,
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 3,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 2,
    "dropout": 0.2,
    "hypothesis": "We test the limits of an extremely efficient (min width/FFN, shallow encoder) and highly regularized model on the challenging task of weekly prediction (168 hours) from bi-weekly data, using minimum attention sparsity (factor=3) for comprehensive attention coverage."
  },
  {
    "trial": 29,
    "timestamp": "2025-10-28T10:50:02.167154",
    "mae_val": 0.3762831687927246,
    "mse_val": 0.21112178266048431,
    "mae_test": 0.3564056158065796,
    "mse_test": 0.20695236325263977,
    "seq_len": 168,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 8,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "We maximize architectural depth (e_layers=6, d_layers=3) while minimizing width (d_model=256, d_ff=1024) to test if efficient, deep feature processing of a weekly context (168) is sufficient for optimal short-term forecasting (24 hours)."
  },
  {
    "trial": 30,
    "timestamp": "2025-10-28T10:58:33.360814",
    "mae_val": 0.35056039690971375,
    "mse_val": 0.1895640343427658,
    "mae_test": 0.3761070966720581,
    "mse_test": 0.2247149646282196,
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 2,
    "n_heads": 16,
    "factor": 10,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "We test a minimal depth, maximal width architecture (d_model=768, e_layers=2, d_layers=1) combined with maximum sparsity (factor=10) and low regularization, to see if raw representational power can efficiently handle the challenging weekly prediction (168) from a limited bi-weekly context (336)."
  },
  {
    "trial": 31,
    "timestamp": "2025-10-28T11:05:06.270288",
    "mae_val": 0.43263447284698486,
    "mse_val": 0.30848070979118347,
    "mae_test": 0.36506709456443787,
    "mse_test": 0.22084881365299225,
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 10,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.05,
    "hypothesis": "We test if coupling a shallow encoder with maximum decoder depth and maximum sparsity can efficiently handle a complex medium-range forecast (96 hours) based on a weekly context, relying on low dropout for full capacity utilization."
  },
  {
    "trial": 32,
    "timestamp": "2025-10-28T11:08:13.299533",
    "mae_val": 0.39532530307769775,
    "mse_val": 0.23838071525096893,
    "mae_test": 0.4533151686191559,
    "mse_test": 0.29806989431381226,
    "seq_len": 720,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 3,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 1,
    "dropout": 0.2,
    "hypothesis": "We examine if a combination of maximum regularization (dropout=0.2) and comprehensive attention (factor=3) allows a highly efficient, narrow model to leverage maximum input context (720 hours) effectively for a critical short-term forecast (24 hours)."
  },
  {
    "trial": 33,
    "timestamp": "2025-10-28T11:17:39.314731",
    "mae_val": 0.5187245607376099,
    "mse_val": 0.37469711899757385,
    "mae_test": 0.42453041672706604,
    "mse_test": 0.2822034955024719,
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 3,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "We investigate if high model capacity (d_model=768, d_ff=3072, n_heads=16) combined with minimum ProbSparse factor (3) can effectively handle the challenging medium-range 96-hour prediction using a full weekly context (168)."
  },
  {
    "trial": 34,
    "timestamp": "2025-10-28T11:24:33.915297",
    "mae_val": 0.5952531695365906,
    "mse_val": 0.48563170433044434,
    "mae_test": 0.6084710955619812,
    "mse_test": 0.5500738620758057,
    "seq_len": 720,
    "label_len": 48,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 8,
    "d_model": 256,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.05,
    "hypothesis": "We test if maximizing encoder and decoder depth (e_layers=6, d_layers=3) combined with high sparsity (factor=8) allows a narrow model (d_model=256) to efficiently leverage the maximum context (720 hours) for a short-to-medium 48-hour forecast, using minimal regularization."
  },
  {
    "trial": 35,
    "timestamp": "2025-10-28T11:32:28.865189",
    "mae_val": 0.2670356035232544,
    "mse_val": 0.11222456395626068,
    "mae_test": 0.3154217600822449,
    "mse_test": 0.17611287534236908,
    "seq_len": 96,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 8,
    "d_model": 768,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Test a highly expressive model (max d_model, max d_layers, n_heads=16) on a successful short-term forecasting context (96/24), using maximum dropout to handle the large capacity and prevent overfitting."
  },
  {
    "trial": 36,
    "timestamp": "2025-10-28T11:34:32.941711",
    "mae_val": 0.387938529253006,
    "mse_val": 0.22788259387016296,
    "mae_test": 0.3911128640174866,
    "mse_test": 0.23516692221164703,
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 10,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 1,
    "dropout": 0.2,
    "hypothesis": "Evaluate the performance of a highly constrained and regularized model (min depth/width, max dropout) coupled with maximum sparsity (factor=10) on a challenging medium-range forecast (168/96), aiming for maximum computational efficiency."
  }
]