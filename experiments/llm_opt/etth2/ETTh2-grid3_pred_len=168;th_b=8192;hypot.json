[
  {
    "trial": 1,
    "timestamp": "2025-10-29T14:29:53.257967",
    "mae_val": 0.7326710820198059,
    "mse_val": 0.7374096512794495,
    "mae_test": 0.5808320045471191,
    "mse_test": 0.4952849745750427,
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 5,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 1,
    "dropout": 0.1,
    "hypothesis": "Эта конфигурация использует среднюю длину входной последовательности (168) для захвата недельных паттернов, а также стандартные настройки модели (d_model=512, dropout=0.1). Простая архитектура декодера (d_layers=1) должна обеспечить быструю сходимость и хорошую производительность для почасовых данных."
  },
  {
    "trial": 2,
    "timestamp": "2025-10-29T15:02:37.473684",
    "mae_val": 0.6345169544219971,
    "mse_val": 0.5600487589836121,
    "mae_test": 0.6946157217025757,
    "mse_test": 0.6757116317749023,
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 10,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 2,
    "dropout": 0.05,
    "hypothesis": "Данная конфигурация предназначена для анализа самых длинных зависимостей (seq_len=720) с использованием большого и глубокого трансформера (d_model=768, e_layers=4, n_heads=16). Высокий фактор (10) и большая емкость модели должны помочь эффективно обрабатывать обширный контекст, минимизируя потери информации."
  },
  {
    "trial": 3,
    "timestamp": "2025-10-29T15:04:20.953533",
    "mae_val": 0.9383957982063293,
    "mse_val": 1.1265065670013428,
    "mae_test": 0.6137551069259644,
    "mse_test": 0.5538709759712219,
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 3,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 1,
    "dropout": 0.2,
    "hypothesis": "Эта конфигурация тестирует, насколько эффективно минимально ресурсоемкая модель (d_model=256, e_layers=2) со сжатым контекстом (seq_len=48) может улавливать сильные ежедневные паттерны в почасовых данных. Использование высокого dropout (0.2) направлено на улучшение обобщающей способности маленькой модели."
  },
  {
    "trial": 4,
    "timestamp": "2025-10-29T15:14:53.516612",
    "mae_val": 0.6720100045204163,
    "mse_val": 0.640020489692688,
    "mae_test": 0.48289141058921814,
    "mse_test": 0.3618601858615875,
    "seq_len": 96,
    "label_len": 48,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 8,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "Данная конфигурация исследует влияние максимальной доступной глубины энкодера (e_layers=6) и декодера (d_layers=3) на производительность при умеренной длине входной последовательности (seq_len=96). Ожидается, что высокая емкость модели поможет эффективно обрабатывать сложные, но ограниченные временные зависимости."
  },
  {
    "trial": 5,
    "timestamp": "2025-10-29T15:31:19.662083",
    "mae_val": 0.566531240940094,
    "mse_val": 0.4725918173789978,
    "mae_test": 0.5068245530128479,
    "mse_test": 0.3901187479496002,
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 3,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "Эта конфигурация тестирует умеренный, но достаточно длинный контекст (336 часов) с максимальной емкостью модели (d_model=768) и минимальным фактором разреженности (factor=3), чтобы проверить, как это влияет на производительность при почасовых данных."
  },
  {
    "trial": 6,
    "timestamp": "2025-10-29T15:35:24.131777",
    "mae_val": 0.7480596899986267,
    "mse_val": 0.7591001391410828,
    "mae_test": 0.5178322792053223,
    "mse_test": 0.4070912003517151,
    "seq_len": 168,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 5,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.05,
    "hypothesis": "Данная конфигурация исследует, может ли максимальная глубина энкодера и декодера (e_layers=6, d_layers=3) скомпенсировать минимальную размерность модели (d_model=256) при использовании недельного контекста и агрессивно короткого начального токена декодера (label_len=24)."
  },
  {
    "trial": 7,
    "timestamp": "2025-10-29T16:00:00.935877",
    "mae_val": 0.6643961071968079,
    "mse_val": 0.6043825745582581,
    "mae_test": 0.6642580032348633,
    "mse_test": 0.6997659206390381,
    "seq_len": 720,
    "label_len": 48,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 8,
    "d_model": 512,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Эта конфигурация исследует, может ли максимальная глубина энкодера и декодера (e_layers=6, d_layers=3) эффективно обрабатывать максимальный контекст (720 часов), используя при этом умеренную размерность модели (d_model=512) и высокий уровень регуляризации (dropout=0.2)."
  },
  {
    "trial": 8,
    "timestamp": "2025-10-29T16:09:33.934932",
    "mae_val": 0.8105202913284302,
    "mse_val": 0.8683346509933472,
    "mae_test": 0.553583562374115,
    "mse_test": 0.46314358711242676,
    "seq_len": 336,
    "label_len": 96,
    "e_layers": 2,
    "n_heads": 16,
    "factor": 3,
    "d_model": 768,
    "d_ff": 2048,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "Эта конфигурация тестирует комбинацию минимальной глубины архитектуры (e_layers=2, d_layers=1) с максимальной емкостью модели (d_model=768) на длинном контексте (336 часов), чтобы проверить, могут ли широкие, но неглубокие слои эффективно обрабатывать данные ETTh с низким уровнем регуляризации."
  },
  {
    "trial": 9,
    "timestamp": "2025-10-29T16:45:34.199235",
    "mae_val": 0.7111955285072327,
    "mse_val": 0.6886553168296814,
    "mae_test": 0.6608114838600159,
    "mse_test": 0.6396313905715942,
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 5,
    "d_model": 768,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "Эта конфигурация исследует максимальный контекст (720) с умеренным фактором разреженности и глубоким декодером (d_layers=3), используя при этом максимальную размерность модели, что должно обеспечить эффективное извлечение сложных долгосрочных зависимостей."
  },
  {
    "trial": 10,
    "timestamp": "2025-10-29T16:51:24.437330",
    "mae_val": 0.7115424871444702,
    "mse_val": 0.7035334706306458,
    "mae_test": 0.5198482275009155,
    "mse_test": 0.4089626669883728,
    "seq_len": 96,
    "label_len": 24,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 10,
    "d_model": 768,
    "d_ff": 1024,
    "d_layers": 2,
    "dropout": 0.05,
    "hypothesis": "Эта конфигурация проверяет, может ли высокая емкость модели (d_model=768) с минимальной FFN и агрессивным разреженным вниманием (factor=10) эффективно работать на коротких последовательностях (96), используя минимальный стартовый токен декодера."
  },
  {
    "trial": 11,
    "timestamp": "2025-10-29T17:07:06.696824",
    "mae_val": 0.7397592663764954,
    "mse_val": 0.7340584993362427,
    "mae_test": 0.6363382935523987,
    "mse_test": 0.6436763405799866,
    "seq_len": 720,
    "label_len": 48,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 3,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "Тестирование высокой емкости внимания (n_heads=16) с максимальным контекстом (720) и минимальной разреженностью (factor=3), чтобы обеспечить высокую точность захвата долгосрочных зависимостей при умеренной глубине модели."
  },
  {
    "trial": 12,
    "timestamp": "2025-10-29T17:15:44.289228",
    "mae_val": 0.5043485760688782,
    "mse_val": 0.39713624119758606,
    "mae_test": 0.5188460350036621,
    "mse_test": 0.4552319347858429,
    "seq_len": 336,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 10,
    "d_model": 256,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Конфигурация исследует, может ли максимальная глубина архитектуры (e_layers=6, d_layers=3) с максимальной регуляризацией (dropout=0.2) эффективно компенсировать минимальную размерность модели (d_model=256) для умеренно длинного контекста."
  },
  {
    "trial": 13,
    "timestamp": "2025-10-29T17:40:25.619122",
    "mae_val": 0.5577232837677002,
    "mse_val": 0.4699060022830963,
    "mae_test": 0.5941842198371887,
    "mse_test": 0.5492222905158997,
    "seq_len": 720,
    "label_len": 96,
    "e_layers": 2,
    "n_heads": 16,
    "factor": 5,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 1,
    "dropout": 0.1,
    "hypothesis": "Эта конфигурация исследует эффект широкой, но мелкой архитектуры (d_model=768, e_layers=2, d_layers=1) при максимальной длине контекста (720 часов). Мы проверяем, может ли высокая емкость одного слоя внимания и FFN эффективно извлекать долгосрочные зависимости без необходимости глубокого стека."
  },
  {
    "trial": 14,
    "timestamp": "2025-10-29T17:46:25.279241",
    "mae_val": 0.6413199305534363,
    "mse_val": 0.573403537273407,
    "mae_test": 0.4983924925327301,
    "mse_test": 0.3753293752670288,
    "seq_len": 168,
    "label_len": 48,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 3,
    "d_model": 512,
    "d_ff": 1024,
    "d_layers": 2,
    "dropout": 0.05,
    "hypothesis": "Эта конфигурация использует недельный контекст (168) с минимальным фактором разреженности (factor=3) для более плотного внимания и минимальным dropout (0.05). Мы тестируем, достаточно ли умеренной глубины и размерности (e_layers=4, d_model=512) для точного прогнозирования при короткой FFN (d_ff=1024)."
  },
  {
    "trial": 15,
    "timestamp": "2025-10-29T18:04:05.628344",
    "mae_val": 0.6742605566978455,
    "mse_val": 0.6210863590240479,
    "mae_test": 0.5108086466789246,
    "mse_test": 0.40829816460609436,
    "seq_len": 168,
    "label_len": 48,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 10,
    "d_model": 768,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Эта конфигурация тестирует комбинацию максимальной ширины модели (d_model=768) и максимальной глубины декодера (d_layers=3) на недельном контексте (seq_len=168). Высокий фактор разреженности (factor=10) и высокий dropout (0.2) должны проверить устойчивость модели к агрессивной регуляризации при обработке средних временных зависимостей."
  },
  {
    "trial": 16,
    "timestamp": "2025-10-29T18:10:08.107947",
    "mae_val": 0.7625798583030701,
    "mse_val": 0.9196441769599915,
    "mae_test": 0.5445082187652588,
    "mse_test": 0.4806763827800751,
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 5,
    "d_model": 512,
    "d_ff": 3072,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "Эта конфигурация исследует, может ли умеренно глубокая и широкая архитектура (d_model=512, e_layers=4, d_ff=3072) успешно улавливать краткосрочные ежедневные паттерны при минимальной длине контекста (seq_len=48). Мы проверяем, насколько эффективно увеличенная емкость модели может улучшить производительность на коротких последовательностях по сравнению с предыдущими экспериментами."
  },
  {
    "trial": 17,
    "timestamp": "2025-10-29T21:24:58.983295",
    "mae_val": 0.852886438369751,
    "mse_val": 0.9601600766181946,
    "mae_test": 0.5095564723014832,
    "mse_test": 0.4110787510871887,
    "seq_len": 96,
    "label_len": 48,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 3,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 2,
    "dropout": 0.05,
    "hypothesis": "Эта конфигурация максимизирует емкость (d_model=768, n_heads=16) и плотность внимания (factor=3) при умеренной глубине для короткого контекста (96), используя минимальный dropout (0.05) для точного извлечения краткосрочных зависимостей."
  },
  {
    "trial": 18,
    "timestamp": "2025-10-29T21:28:49.182235",
    "mae_val": 0.765302300453186,
    "mse_val": 0.7686794400215149,
    "mae_test": 0.9357553124427795,
    "mse_test": 1.191163182258606,
    "seq_len": 720,
    "label_len": 24,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 8,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 1,
    "dropout": 0.2,
    "hypothesis": "Конфигурация проверяет, может ли максимальная длина контекста (720) компенсировать минимальную ширину и глубину модели (d_model=256, E2/D1), используя высокий уровень регуляризации (0.2) и разреженное внимание для снижения вычислительной нагрузки."
  },
  {
    "trial": 19,
    "timestamp": "2025-10-29T21:39:41.345244",
    "mae_val": 0.4951362907886505,
    "mse_val": 0.3776443302631378,
    "mae_test": 0.5525155067443848,
    "mse_test": 0.4743400514125824,
    "seq_len": 336,
    "label_len": 96,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 5,
    "d_model": 512,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "Эта конфигурация исследует, может ли максимальная глубина архитектуры (E6/D3) эффективно обрабатывать умеренно длинный контекст (336) с большим окном прогнозирования (96) при умеренной размерности модели. Использование минимальной FFN (1024) предназначено для ускорения обучения при сохранении глубокой структуры."
  },
  {
    "trial": 20,
    "timestamp": "2025-10-29T21:51:01.478362",
    "mae_val": 0.7390277981758118,
    "mse_val": 0.7600247263908386,
    "mae_test": 0.6360417008399963,
    "mse_test": 0.6027101874351501,
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 3,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.05,
    "hypothesis": "Эта конфигурация исследует, может ли максимальная емкость модели (d_model=768, n_heads=16) и плотное внимание (factor=3) улавливать краткосрочные ежедневные паттерны (seq_len=48) при минимальном уровне регуляризации. Мы проверяем, способна ли глубокая архитектура декодера (d_layers=3) улучшить прогнозирование на основе минимального контекста."
  },
  {
    "trial": 21,
    "timestamp": "2025-10-29T22:21:07.586354",
    "mae_val": 0.7398321628570557,
    "mse_val": 0.7150082588195801,
    "mae_test": 0.6120707988739014,
    "mse_test": 0.5560591816902161,
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 5,
    "d_model": 768,
    "d_ff": 2048,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "Эта конфигурация исследует максимальный контекст (720) с самой глубокой архитектурой энкодера (e_layers=6) и максимальной емкостью модели (d_model=768, n_heads=16). Ожидается, что сочетание глубокого энкодера и широкой модели улучшит извлечение долгосрочных зависимостей по сравнению с неглубокими (E2/D1) аналогами."
  },
  {
    "trial": 22,
    "timestamp": "2025-10-29T22:27:20.377510",
    "mae_val": 0.7135788202285767,
    "mse_val": 0.7314169406890869,
    "mae_test": 0.528512179851532,
    "mse_test": 0.4205043613910675,
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 8,
    "d_model": 256,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Данная конфигурация тестирует эффект мелкого энкодера (E2) в сочетании с глубоким декодером (D3) при умеренном контексте (168). Мы используем минимальную размерность модели (d_model=256) и высокий dropout (0.2), чтобы проверить, могут ли глубокие слои декодера компенсировать недостаток емкости энкодера и эффективно обрабатывать прогнозирование."
  },
  {
    "trial": 23,
    "timestamp": "2025-10-29T22:38:08.019771",
    "mae_val": 0.7939248085021973,
    "mse_val": 0.831545352935791,
    "mae_test": 0.7639791369438171,
    "mse_test": 0.8384185433387756,
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 10,
    "d_model": 256,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "Данная конфигурация тестирует предел глубины (E3/D3) и разреженности (factor=10) для обработки максимального контекста (720) и длинного горизонта прогнозирования (168), компенсируя минимальную ширину модели (d_model=256) максимальной FFN емкостью."
  },
  {
    "trial": 24,
    "timestamp": "2025-10-29T22:44:36.119325",
    "mae_val": 0.5462650060653687,
    "mse_val": 0.45087379217147827,
    "mae_test": 0.5136350989341736,
    "mse_test": 0.4224991202354431,
    "seq_len": 96,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 5,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Конфигурация проверяет, может ли умеренно глубокий энкодер (E4) в сочетании с максимальной глубиной декодера (D3) и сильной регуляризацией (dropout=0.2) эффективно обрабатывать краткосрочные паттерны (seq_len=96) для прогнозирования на 24 часа."
  },
  {
    "trial": 25,
    "timestamp": "2025-10-30T12:05:08.231747",
    "mae_val": 0.6568455100059509,
    "mse_val": 0.5917536020278931,
    "mae_test": 0.6280721426010132,
    "mse_test": 0.5639845728874207,
    "seq_len": 720,
    "label_len": 48,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 8,
    "d_model": 768,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.05,
    "hypothesis": "Тестирование комбинации максимальной глубины (E6/D3) и максимальной ширины (Dm=768, H=16) на самом длинном контексте (720), используя минимальный FFN и низкий уровень регуляризации для оценки способности модели обрабатывать обширный контекст при высокой емкости."
  },
  {
    "trial": 26,
    "timestamp": "2025-10-30T12:08:38.095921",
    "mae_val": 0.7093169689178467,
    "mse_val": 0.7140683531761169,
    "mae_test": 0.5595000982284546,
    "mse_test": 0.4670516550540924,
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 3,
    "d_model": 256,
    "d_ff": 2048,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "Конфигурация исследует, может ли относительно небольшая модель (d_model=256, E3) эффективно работать с плотным вниманием (factor=3) на умеренно длинном контексте (336) с большим окном прогнозирования (168), проверяя, перевешивает ли плотность внимания недостаток архитектурной ширины."
  },
  {
    "trial": 27,
    "timestamp": "2025-10-30T12:15:33.713294",
    "mae_val": 0.6947234869003296,
    "mse_val": 0.6515488028526306,
    "mae_test": 0.740706741809845,
    "mse_test": 0.7896225452423096,
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 5,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "Эта конфигурация тестирует максимальное окно прогнозирования (336 часов) с полным контекстом (720), но с минимальной размерностью модели (d_model=256) и минимальной FFN. Мы оцениваем, сохраняется ли эффективность Informer для долгосрочного прогнозирования при сильных ресурсных ограничениях."
  },
  {
    "trial": 28,
    "timestamp": "2025-10-30T12:27:51.584110",
    "mae_val": 0.7256336212158203,
    "mse_val": 0.7286555171012878,
    "mae_test": 0.48819342255592346,
    "mse_test": 0.37630173563957214,
    "seq_len": 96,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 8,
    "d_model": 768,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.05,
    "hypothesis": "Эта конфигурация максимизирует архитектурную глубину (E6/D3) и ширину (d_model=768) для обработки коротких последовательностей (96 часов). Мы проверяем, может ли максимальная емкость и глубина модели, используемые с минимальной регуляризацией, улучшить краткосрочное прогнозирование на данных ETTh."
  },
  {
    "trial": 29,
    "timestamp": "2025-10-30T12:46:16.475802",
    "mae_val": 0.6458399891853333,
    "mse_val": 0.5598599910736084,
    "mae_test": 0.7064867615699768,
    "mse_test": 0.7458354234695435,
    "seq_len": 720,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 8,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "Эта конфигурация тестирует сочетание максимальной глубины архитектуры (E6/D3) и максимального контекста (720) с минимальным начальным токеном декодера (label_len=24). Мы используем умеренную размерность (d_model=512) и высокий фактор разреженности (factor=8), чтобы оценить способность глубокой, но не максимально широкой модели извлекать долгосрочные зависимости."
  },
  {
    "trial": 30,
    "timestamp": "2025-10-30T12:54:35.797630",
    "mae_val": 0.5964193344116211,
    "mse_val": 0.518073320388794,
    "mae_test": 0.5110448002815247,
    "mse_test": 0.3851827085018158,
    "seq_len": 336,
    "label_len": 48,
    "e_layers": 2,
    "n_heads": 16,
    "factor": 5,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "Данная конфигурация исследует эффект максимально широкой, но мелкой архитектуры (d_model=768, E2/D1) на умеренно длинном контексте (336 часов) для короткого прогноза (48 часов). Мы используем максимальную емкость FFN и минимальный dropout, чтобы проверить, достаточно ли одного широкого слоя для эффективного извлечения недельных и двухнедельных паттернов."
  },
  {
    "trial": 31,
    "timestamp": "2025-10-30T13:03:49.459827",
    "mae_val": 0.7324460744857788,
    "mse_val": 0.7947807312011719,
    "mae_test": 0.49791717529296875,
    "mse_test": 0.3956140875816345,
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 3,
    "d_model": 768,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "Тестирование максимально глубокой (E6/D3) и широкой (Dm=768, H=16) архитектуры на минимальном контексте (48) с плотным вниманием (factor=3) и минимальной FFN для проверки способности извлекать краткосрочные паттерны при высокой емкости."
  },
  {
    "trial": 32,
    "timestamp": "2025-10-30T13:10:30.178440",
    "mae_val": 0.7836880683898926,
    "mse_val": 0.8442537188529968,
    "mae_test": 0.740548849105835,
    "mse_test": 0.7898041009902954,
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 10,
    "d_model": 256,
    "d_ff": 2048,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "Исследование эффективности Informer для максимального контекста (720) и длинного прогноза (168) при использовании минимальной глубины (E2/D1) и ширины (Dm=256), но с агрессивной разреженностью (factor=10) и минимальной регуляризацией."
  },
  {
    "trial": 33,
    "timestamp": "2025-10-30T13:39:06.720684",
    "mae_val": 0.6836645603179932,
    "mse_val": 0.6591845750808716,
    "mae_test": 0.7623634338378906,
    "mse_test": 0.8268358111381531,
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 3,
    "d_model": 512,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "Эта конфигурация максимизирует как контекст (720), так и горизонт прогнозирования (336), используя при этом глубокую (E6/D3) и высокопараметризованную архитектуру. Минимальный фактор разреженности (factor=3) предназначен для обеспечения наиболее плотного внимания при обработке самых длинных зависимостей."
  },
  {
    "trial": 34,
    "timestamp": "2025-10-30T13:43:09.590323",
    "mae_val": 0.7833016514778137,
    "mse_val": 0.8747884035110474,
    "mae_test": 0.5078603029251099,
    "mse_test": 0.40554606914520264,
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 2,
    "n_heads": 16,
    "factor": 8,
    "d_model": 768,
    "d_ff": 2048,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "Эта конфигурация тестирует эффект использования максимально широкой, но мелкой архитектуры (D_m=768, E2/D1) для захвата краткосрочных ежедневных паттернов (seq_len=48). Мы проверяем, может ли высокая емкость одного слоя внимания, работающего с минимальной регуляризацией, обеспечить эффективное и быстрое обучение."
  },
  {
    "trial": 35,
    "timestamp": "2025-10-30T14:20:46.670051",
    "mae_val": 0.5726860761642456,
    "mse_val": 0.4664170742034912,
    "mae_test": 0.6071165800094604,
    "mse_test": 0.5392864346504211,
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 10,
    "d_model": 768,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Эта конфигурация тестирует способность глубокого декодера (D3) и максимально широкой модели (d_model=768) обрабатывать самые длинные зависимости (720/336) при минимальной FFN емкости и высокой разреженности для улучшения вычислительной эффективности."
  },
  {
    "trial": 36,
    "timestamp": "2025-10-30T14:25:06.939105",
    "mae_val": 0.6810077428817749,
    "mse_val": 0.6592407822608948,
    "mae_test": 0.521481454372406,
    "mse_test": 0.41739997267723083,
    "seq_len": 96,
    "label_len": 48,
    "e_layers": 2,
    "n_heads": 16,
    "factor": 3,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "Данная конфигурация исследует, может ли широкая, но мелкая архитектура (E2/D1, d_model=768, d_ff=3072) с плотным вниманием (factor=3) эффективно захватывать краткосрочные паттерны (seq_len=96) с минимальной регуляризацией."
  },
  {
    "trial": 37,
    "timestamp": "2025-10-30T14:41:29.220795",
    "mae_val": 0.6029321551322937,
    "mse_val": 0.5248621106147766,
    "mae_test": 0.47439301013946533,
    "mse_test": 0.34546974301338196,
    "seq_len": 96,
    "label_len": 48,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 10,
    "d_model": 768,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Эта конфигурация максимизирует емкость (ширина и глубина) и использует сильную регуляризацию (dropout=0.2), чтобы проверить, может ли глубокий и широкий Informer эффективно справляться с умеренным контекстом (96) при агрессивной разреженности (factor=10)."
  },
  {
    "trial": 38,
    "timestamp": "2025-10-30T14:57:44.167840",
    "mae_val": 0.7850760221481323,
    "mse_val": 0.8131892681121826,
    "mae_test": 0.7884343862533569,
    "mse_test": 0.8931747078895569,
    "seq_len": 720,
    "label_len": 48,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 3,
    "d_model": 512,
    "d_ff": 3072,
    "d_layers": 2,
    "dropout": 0.05,
    "hypothesis": "Данная конфигурация исследует, насколько эффективно максимальный контекст (720) помогает прогнозировать короткий горизонт (48), используя умеренную глубину, плотное внимание (factor=3) и минимальный dropout, чтобы обеспечить высокую точность извлечения долгосрочных зависимостей."
  }
]