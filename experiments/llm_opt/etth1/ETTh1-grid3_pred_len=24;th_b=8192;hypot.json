[
  {
    "trial": 1,
    "timestamp": "2025-10-29T21:42:14.667571",
    "mae_val": 0.3176504075527191,
    "mse_val": 0.16971886157989502,
    "mae_test": 0.2300546020269394,
    "mse_test": 0.08701653778553009,
    "seq_len": 96,
    "label_len": 48,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 5,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "Эта конфигурация использует сбалансированную длину входной последовательности (96) для захвата суточных и коротких недельных паттернов в часовых данных, сочетая умеренную глубину архитектуры и стандартный дропаут для стабильной производительности."
  },
  {
    "trial": 2,
    "timestamp": "2025-10-29T21:46:55.693541",
    "mae_val": 0.30131733417510986,
    "mse_val": 0.15138953924179077,
    "mae_test": 0.23878516256809235,
    "mse_test": 0.09054599702358246,
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 3,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "Мы проверяем, может ли максимальная ширина и глубина энкодера (768/3072, 6 слоев, 16 голов) компенсировать использование короткой входной последовательности (48), при этом сохраняя низкий дропаут для лучшей адаптации большой модели."
  },
  {
    "trial": 3,
    "timestamp": "2025-10-29T21:50:24.851738",
    "mae_val": 0.19320878386497498,
    "mse_val": 0.06262499839067459,
    "mae_test": 0.19962355494499207,
    "mse_test": 0.06350316852331161,
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 10,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Мы проверяем, может ли максимальная длина входной последовательности (336) и высокая степень разреженности (factor=10) компенсировать минимальную архитектурную емкость (d_model=256, 2 слоя энкодера), используя при этом глубокий декодер (3 слоя) для генерации сложных прогнозов."
  },
  {
    "trial": 4,
    "timestamp": "2025-10-29T22:00:42.208865",
    "mae_val": 0.6472786068916321,
    "mse_val": 0.5425413846969604,
    "mae_test": 0.32589614391326904,
    "mse_test": 0.18066397309303284,
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 8,
    "d_model": 768,
    "d_ff": 2048,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "Эта конфигурация балансирует между достаточной длиной последовательности (168 часов для недельной сезонности) и высокой емкостью модели (d_model=768, 16 голов, 4 слоя энкодера) для мощного извлечения признаков при стандартном уровне дропаута."
  },
  {
    "trial": 5,
    "timestamp": "2025-10-29T22:18:16.415800",
    "mae_val": 0.2807390093803406,
    "mse_val": 0.12338732182979584,
    "mae_test": 0.25337129831314087,
    "mse_test": 0.10150172561407089,
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 5,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "Мы тестируем максимальную длину входной последовательности (720 часов) для захвата долгосрочных ежемесячных зависимостей, используя при этом сбалансированную архитектуру средней емкости (512/2048, 3 слоя энкодера) для стабильной производительности."
  },
  {
    "trial": 6,
    "timestamp": "2025-10-29T22:31:56.678814",
    "mae_val": 0.38917970657348633,
    "mse_val": 0.2312149852514267,
    "mae_test": 0.29594045877456665,
    "mse_test": 0.13652759790420532,
    "seq_len": 336,
    "label_len": 96,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 3,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.05,
    "hypothesis": "Эта конфигурация исследует эффект использования длинного контекста (336) с максимальной архитектурной емкостью (6 слоев энкодера, d_model=768) и низким дропаутом, чтобы определить, может ли высокая мощность модели улучшить результаты для сложных временных рядов."
  },
  {
    "trial": 7,
    "timestamp": "2025-10-29T22:40:04.048814",
    "mae_val": 0.2388564944267273,
    "mse_val": 0.09996193647384644,
    "mae_test": 0.26741066575050354,
    "mse_test": 0.11294161528348923,
    "seq_len": 168,
    "label_len": 48,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 5,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Мы исследуем, может ли комбинация недельного контекста (168) с максимальной архитектурной емкостью и высоким уровнем дропаута (0.20) улучшить результаты, предотвращая переобучение, которое могло стать причиной плохой производительности похожих конфигураций в истории."
  },
  {
    "trial": 8,
    "timestamp": "2025-10-29T22:54:32.650262",
    "mae_val": 0.30078646540641785,
    "mse_val": 0.15655268728733063,
    "mae_test": 0.2968143820762634,
    "mse_test": 0.13656966388225555,
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 10,
    "d_model": 768,
    "d_ff": 2048,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "Проверяем эффект максимального контекста (720) и высокой разреженности (factor=10) в сочетании с минимальной глубиной энкодера (2 слоя) и низким дропаутом, используя при этом максимальную ширину модели (768) для извлечения долгосрочных зависимостей."
  },
  {
    "trial": 9,
    "timestamp": "2025-10-29T23:03:47.890540",
    "mae_val": 0.26369407773017883,
    "mse_val": 0.11836942285299301,
    "mae_test": 0.2959684729576111,
    "mse_test": 0.1299336552619934,
    "seq_len": 96,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 10,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Тестируем, может ли комбинация умеренного контекста (96) с максимальной архитектурной емкостью, высокой разреженностью (factor=10) и агрессивным дропаутом (0.2) обеспечить наилучшие результаты, предотвращая переобучение при высокой мощности модели."
  },
  {
    "trial": 10,
    "timestamp": "2025-10-29T23:08:19.154127",
    "mae_val": 0.46207067370414734,
    "mse_val": 0.3028659522533417,
    "mae_test": 0.35863006114959717,
    "mse_test": 0.20468652248382568,
    "seq_len": 336,
    "label_len": 48,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 3,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "Исследуем эффект двухнедельного контекста (336) сбалансированной средней емкости, используя при этом низкую разреженность (factor=3) и минимальный дропаут для максимизации точности извлечения зависимостей с минимальной глубиной декодера."
  },
  {
    "trial": 11,
    "timestamp": "2025-10-30T15:26:38.505933",
    "mae_val": 0.3132447600364685,
    "mse_val": 0.15920507907867432,
    "mae_test": 0.2948160171508789,
    "mse_test": 0.1338529884815216,
    "seq_len": 720,
    "label_len": 48,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 3,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "Мы тестируем эффект максимальной емкости модели (d_model=768, 4 слоя энкодера, 3 слоя декодера) и максимального контекста (720) в сочетании с минимальной разреженностью (factor=3), чтобы агрессивно захватить и обработать сложные долгосрочные зависимости при стандартной регуляризации."
  },
  {
    "trial": 12,
    "timestamp": "2025-10-30T15:28:46.170395",
    "mae_val": 0.3004617393016815,
    "mse_val": 0.1377468705177307,
    "mae_test": 0.22074314951896667,
    "mse_test": 0.07846659421920776,
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 8,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 1,
    "dropout": 0.2,
    "hypothesis": "Исследуем, может ли минимальная архитектурная емкость (d_model=256, 2 слоя энкодера, 1 слой декодера) с недельным контекстом (168) и высокой степенью разреженности (factor=8) достичь высокой производительности за счет агрессивного дропаута (0.2), фокусируясь на обобщении."
  },
  {
    "trial": 13,
    "timestamp": "2025-10-30T15:35:36.778154",
    "mae_val": 0.5616503953933716,
    "mse_val": 0.3943421542644501,
    "mae_test": 0.30036622285842896,
    "mse_test": 0.14398318529129028,
    "seq_len": 168,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 8,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "Мы исследуем эффективность использования недельного контекста (168) для краткосрочного прогноза (24) с максимальной архитектурной емкостью, высокой разреженностью (factor=8) и стандартным дропаутом (0.1), чтобы проверить, как большая модель справляется с извлечением признаков при умеренной регуляризации."
  },
  {
    "trial": 14,
    "timestamp": "2025-10-30T15:40:36.457467",
    "mae_val": 0.29508650302886963,
    "mse_val": 0.14026185870170593,
    "mae_test": 0.27462807297706604,
    "mse_test": 0.12220392376184464,
    "seq_len": 720,
    "label_len": 96,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 5,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.05,
    "hypothesis": "Мы проверяем, может ли использование максимального контекста (720) в сочетании с минимальной шириной модели (d_model=256) и глубоким декодером (3 слоя) эффективно захватывать и обрабатывать долгосрочные зависимости при минимальной регуляризации (0.05), имитируя стратегию успеха Top 1."
  },
  {
    "trial": 15,
    "timestamp": "2025-10-30T15:50:55.372735",
    "mae_val": 0.23939192295074463,
    "mse_val": 0.09938663244247437,
    "mae_test": 0.20698556303977966,
    "mse_test": 0.06984622031450272,
    "seq_len": 720,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 5,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Мы исследуем, может ли максимальная глубина энкодера (6 слоев) и максимальный контекст (720) в сочетании с агрессивным дропаутом (0.2) эффективно извлекать долгосрочные признаки для очень краткосрочного прогноза (24), используя при этом сбалансированную емкость модели."
  },
  {
    "trial": 16,
    "timestamp": "2025-10-30T15:54:07.072297",
    "mae_val": 0.3549650311470032,
    "mse_val": 0.1936473548412323,
    "mae_test": 0.2500963807106018,
    "mse_test": 0.10483400523662567,
    "seq_len": 96,
    "label_len": 48,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 8,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 1,
    "dropout": 0.1,
    "hypothesis": "Проверяем эффективность высокоширокой (768) и минимально глубокой (2 энкодер, 1 декодер) архитектуры на умеренном контексте (96) с высоким коэффициентом разреженности (8), чтобы найти эффективное и быстрое решение для среднесрочного прогнозирования."
  },
  {
    "trial": 17,
    "timestamp": "2025-10-30T15:58:21.054009",
    "mae_val": 0.24497491121292114,
    "mse_val": 0.11129377037286758,
    "mae_test": 0.24695158004760742,
    "mse_test": 0.08940599113702774,
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 8,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 2,
    "dropout": 0.2,
    "hypothesis": "Исследуем влияние максимального контекста (720) и длинного прогноза (168) на минимальную ширину модели (d_model=256), используя среднюю глубину и высокий коэффициент разреженности (factor=8) в сочетании с агрессивным дропаутом для лучшей обобщающей способности."
  },
  {
    "trial": 18,
    "timestamp": "2025-10-30T16:07:23.680576",
    "mae_val": 0.2871493101119995,
    "mse_val": 0.13970667123794556,
    "mae_test": 0.2328488826751709,
    "mse_test": 0.08413852006196976,
    "seq_len": 96,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 3,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "Проверяем, может ли использование максимальной архитектурной емкости (6 слоев, 768/3072) на умеренном контексте (96) для краткосрочного прогнозирования (24) с низкой разреженностью (factor=3) и умеренным дропаутом обеспечить высокую точность за счет интенсивного обучения."
  },
  {
    "trial": 19,
    "timestamp": "2025-10-30T16:35:58.295950",
    "mae_val": 0.409694641828537,
    "mse_val": 0.2468256950378418,
    "mae_test": 0.3179263472557068,
    "mse_test": 0.15834805369377136,
    "seq_len": 720,
    "label_len": 96,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 10,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.05,
    "hypothesis": "Мы тестируем максимальную архитектурную емкость (6 слоев энкодера, 3 слоя декодера, d_model=768) с максимальным контекстом (720) и максимальной разреженностью (factor=10), используя минимальный дропаут (0.05), чтобы проверить, способна ли высокая мощность модели улавливать долгосрочные зависимости без агрессивной регуляризации."
  },
  {
    "trial": 20,
    "timestamp": "2025-10-30T16:40:12.924785",
    "mae_val": 0.279075562953949,
    "mse_val": 0.13479319214820862,
    "mae_test": 0.20439095795154572,
    "mse_test": 0.06890199333429337,
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 8,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Исследуем эффективность умеренной архитектуры (d_model=512, 4 слоя энкодера) с глубоким декодером (3 слоя) на минимальном контексте (48 часов) для краткосрочного прогнозирования (24 часа), используя высокую степень разреженности (factor=8) и агрессивный дропаут (0.2) для лучшей обобщающей способности."
  },
  {
    "trial": 21,
    "timestamp": "2025-10-30T16:47:38.311448",
    "mae_val": 0.24598123133182526,
    "mse_val": 0.09952156245708466,
    "mae_test": 0.31820517778396606,
    "mse_test": 0.1604343205690384,
    "seq_len": 720,
    "label_len": 96,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 10,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Мы исследуем, может ли минимальная емкость модели (d_model=256) с максимальным контекстом (720) и высокой разреженностью (factor=10) достичь высокой производительности для среднесрочного прогноза (96) за счет максимальной глубины энкодера (4) и агрессивного дропаута (0.2)."
  },
  {
    "trial": 22,
    "timestamp": "2025-10-30T16:55:59.449580",
    "mae_val": 0.3700515627861023,
    "mse_val": 0.2069055736064911,
    "mae_test": 0.26629990339279175,
    "mse_test": 0.11362850666046143,
    "seq_len": 168,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 3,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.05,
    "hypothesis": "Проверяем, может ли максимальная архитектурная емкость (6 слоев энкодера, d_model=768, d_layers=3) с недельным контекстом (168) и минимальной разреженностью (factor=3) обеспечить высокую точность для краткосрочного прогноза (24), используя минимальный дропаут для полной адаптации модели."
  },
  {
    "trial": 23,
    "timestamp": "2025-10-30T17:06:08.448502",
    "mae_val": 0.3872152864933014,
    "mse_val": 0.23054374754428864,
    "mae_test": 0.3547764718532562,
    "mse_test": 0.20903968811035156,
    "seq_len": 336,
    "label_len": 96,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 5,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Мы исследуем влияние максимальной глубины энкодера (6 слоев) и агрессивной регуляризации (0.2) на двухнедельном контексте (336) для среднесрочного прогноза (96), используя сбалансированную среднюю ширину модели."
  },
  {
    "trial": 24,
    "timestamp": "2025-10-30T17:12:00.347327",
    "mae_val": 0.27816450595855713,
    "mse_val": 0.11785861104726791,
    "mae_test": 0.21460427343845367,
    "mse_test": 0.07409963756799698,
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 10,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 2,
    "dropout": 0.2,
    "hypothesis": "Мы исследуем, как максимальная емкость модели (d_model=768, 16 голов) в сочетании с высокой разреженностью (factor=10) и агрессивным дропаутом (0.2) справляется с извлечением признаков из минимального контекста (48) для краткосрочного прогноза (24)."
  },
  {
    "trial": 25,
    "timestamp": "2025-10-30T17:33:43.500851",
    "mae_val": 0.304186075925827,
    "mse_val": 0.15489058196544647,
    "mae_test": 0.2927698791027069,
    "mse_test": 0.1279485672712326,
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 3,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 2,
    "dropout": 0.05,
    "hypothesis": "Тестируем способность максимальной емкости модели (768/3072, 4 слоя энкодера, 16 голов) агрессивно захватывать и прогнозировать самые длинные зависимости (720 -> 336) при минимальной разреженности и регуляризации."
  },
  {
    "trial": 26,
    "timestamp": "2025-10-30T17:37:07.919721",
    "mae_val": 0.23505693674087524,
    "mse_val": 0.09055629372596741,
    "mae_test": 0.22076156735420227,
    "mse_test": 0.08146769553422928,
    "seq_len": 168,
    "label_len": 48,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 10,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Применяем успешную стратегию минимальной емкости (d_model=256), глубокого декодера (3 слоя), высокой разреженности (factor=10) и агрессивного дропаута (0.2) к недельному контексту (168) для прогнозирования на 48 часов, фокусируясь на обобщении."
  },
  {
    "trial": 27,
    "timestamp": "2025-10-30T17:44:14.644350",
    "mae_val": 0.22719979286193848,
    "mse_val": 0.08412706851959229,
    "mae_test": 0.1879061609506607,
    "mse_test": 0.05872239172458649,
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 5,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Мы исследуем, может ли комбинация максимальной ширины модели (d_model=768, 16 голов) и глубокого декодера (3 слоя) с агрессивным дропаутом (0.2) эффективно использовать минимальный контекст (48) для высокоточного краткосрочного прогнозирования."
  },
  {
    "trial": 28,
    "timestamp": "2025-10-30T17:50:55.036738",
    "mae_val": 0.4148913621902466,
    "mse_val": 0.24060499668121338,
    "mae_test": 0.2666502594947815,
    "mse_test": 0.1110277995467186,
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 3,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "Тестируем влияние минимальной разреженности (factor=3) на двухнедельном контексте (336) для недельного прогноза (168), используя сбалансированную, но глубокую архитектуру средней емкости при стандартной регуляризации."
  },
  {
    "trial": 29,
    "timestamp": "2025-10-30T18:19:04.785697",
    "mae_val": 0.5738018751144409,
    "mse_val": 0.4207252562046051,
    "mae_test": 0.5153531432151794,
    "mse_test": 0.3569384515285492,
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 8,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Мы тестируем максимальную емкость модели (d_model=768, 4 слоя энкодера, 3 слоя декодера) и агрессивную регуляризацию (0.20) для решения задачи долгосрочного прогнозирования (720->336), используя при этом умеренную разреженность (factor=8) для баланса между точностью и эффективностью."
  },
  {
    "trial": 30,
    "timestamp": "2025-10-30T18:22:28.147292",
    "mae_val": 0.37400540709495544,
    "mse_val": 0.21361730992794037,
    "mae_test": 0.22792166471481323,
    "mse_test": 0.08660300076007843,
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 3,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 1,
    "dropout": 0.1,
    "hypothesis": "Мы проверяем, может ли максимальная глубина энкодера (6 слоев) эффективно извлекать признаки из минимального контекста (48) для краткосрочного прогноза, используя при этом сбалансированную ширину модели, минимальную глубину декодера (1 слой) и низкую разреженность (factor=3) при умеренной регуляризации."
  },
  {
    "trial": 31,
    "timestamp": "2025-10-30T23:26:43.159852",
    "mae_val": 0.4255070686340332,
    "mse_val": 0.2988681495189667,
    "mae_test": 0.2530040740966797,
    "mse_test": 0.10166450589895248,
    "seq_len": 96,
    "label_len": 48,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 3,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 3,
    "dropout": 0.2,
    "hypothesis": "Исследуем, может ли комбинация высокой архитектурной емкости (d_model=768, 16 голов) и глубокого декодера (3 слоя) эффективно обрабатывать умеренный контекст (96) для среднесрочного прогноза (48), используя минимальную разреженность (factor=3) и агрессивный дропаут (0.2) для предотвращения переобучения."
  },
  {
    "trial": 32,
    "timestamp": "2025-10-30T23:33:25.754082",
    "mae_val": 0.3684161901473999,
    "mse_val": 0.20651482045650482,
    "mae_test": 0.3191588222980499,
    "mse_test": 0.15736077725887299,
    "seq_len": 720,
    "label_len": 48,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 8,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "Мы тестируем, может ли максимальная длина контекста (720) для очень краткосрочного прогноза (48) эффективно работать с минимальной глубиной архитектуры (2 слоя энкодера, 1 слой декодера) и низкой регуляризацией (0.05), используя умеренную емкость модели (d_model=512) и высокую разреженность (factor=8)."
  },
  {
    "trial": 33,
    "timestamp": "2025-10-30T23:37:34.909546",
    "mae_val": 0.38809093832969666,
    "mse_val": 0.213599294424057,
    "mae_test": 0.25658661127090454,
    "mse_test": 0.10538437962532043,
    "seq_len": 96,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 3,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 2,
    "dropout": 0.05,
    "hypothesis": "Мы исследуем, может ли комбинация максимальной глубины энкодера (6 слоев) и умеренного контекста (96) сбалансированной средней емкости эффективно извлекать признаки для краткосрочного прогноза (24) при минимальной регуляризации и низкой разреженности, фокусируясь на интенсивном обучении."
  },
  {
    "trial": 34,
    "timestamp": "2025-10-30T23:44:21.425209",
    "mae_val": 0.2409142106771469,
    "mse_val": 0.10643204301595688,
    "mae_test": 0.2574940323829651,
    "mse_test": 0.09464801847934723,
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 10,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 3,
    "dropout": 0.1,
    "hypothesis": "Мы тестируем, может ли комбинация максимального контекста (720) и длинного прогноза (336) с минимальной шириной модели (256) и высокой разреженностью (factor=10) обеспечить высокую производительность при использовании глубокого декодера (3 слоя) и умеренной регуляризации."
  },
  {
    "trial": 35,
    "timestamp": "2025-10-30T23:49:41.151525",
    "mae_val": 0.4110853970050812,
    "mse_val": 0.24772216379642487,
    "mae_test": 0.3168220520019531,
    "mse_test": 0.16092996299266815,
    "seq_len": 336,
    "label_len": 96,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 8,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 1,
    "dropout": 0.1,
    "hypothesis": "Эта конфигурация тестирует, может ли двухнедельный контекст (336) с высокопараллельным и умеренно глубоким энкодером (4 слоя, 16 голов) и высокой разреженностью (factor=8) эффективно извлекать необходимые признаки, используя при этом минимальную глубину декодера для среднесрочного прогнозирования."
  },
  {
    "trial": 36,
    "timestamp": "2025-10-30T23:52:32.991203",
    "mae_val": 0.2131759375333786,
    "mse_val": 0.07486172765493393,
    "mae_test": 0.22532644867897034,
    "mse_test": 0.08059918135404587,
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 3,
    "d_model": 256,
    "d_ff": 1024,
    "d_layers": 1,
    "dropout": 0.05,
    "hypothesis": "Мы исследуем, может ли минимальная архитектурная емкость (d_model=256, 2 слоя энкодера, 1 слой декодера) успешно решать задачу максимального долгосрочного прогнозирования (720->336) при использовании максимального контекста, низкой разреженности (factor=3) и минимального дропаута для интенсивного обучения."
  },
  {
    "trial": 37,
    "timestamp": "2025-10-31T00:00:03.848230",
    "mae_val": 0.37489625811576843,
    "mse_val": 0.23392966389656067,
    "mae_test": 0.2570842504501343,
    "mse_test": 0.10158345848321915,
    "seq_len": 96,
    "label_len": 48,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 10,
    "d_model": 768,
    "d_ff": 3072,
    "d_layers": 2,
    "dropout": 0.2,
    "hypothesis": "Мы проверяем, может ли комбинация максимальной архитектурной емкости (d_model=768, 16 голов) с умеренным контекстом (96) и высоким уровнем регуляризации (dropout=0.2) эффективно обобщать для среднесрочного прогнозирования (48) за счет высокой разреженности (factor=10)."
  },
  {
    "trial": 38,
    "timestamp": "2025-10-31T00:09:04.746200",
    "mae_val": 0.3149498403072357,
    "mse_val": 0.1706112176179886,
    "mae_test": 0.2931249737739563,
    "mse_test": 0.1365448534488678,
    "seq_len": 720,
    "label_len": 24,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 3,
    "d_model": 512,
    "d_ff": 2048,
    "d_layers": 2,
    "dropout": 0.1,
    "hypothesis": "Исследуем, как максимальный контекст (720) для очень краткосрочного прогноза (24) взаимодействует со сбалансированной архитектурой средней емкости, используя при этом низкую разреженность (factor=3) и стандартный дропаут для интенсивного извлечения долгосрочных, но точных, признаков."
  }
]