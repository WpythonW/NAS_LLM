[
  {
    "trial": 1,
    "timestamp": "2025-10-12T14:59:45.851427",
    "mae_val": 0.58696049451828,
    "mse_val": 0.4609556496143341,
    "mae_test": 0.372361958026886,
    "mse_test": 0.21135514974594116,
    "hypothesis": "Using an input length equal to the forecast horizon (168) combined with a deep encoder (4 layers) and high attention complexity (16 heads, factor 8) should effectively capture medium-range dependencies relevant for the prediction window.",
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 8
  },
  {
    "trial": 2,
    "timestamp": "2025-10-12T15:06:41.999638",
    "mae_val": 0.4995538890361786,
    "mse_val": 0.34314242005348206,
    "mae_test": 0.35792362689971924,
    "mse_test": 0.19193078577518463,
    "hypothesis": "By using a significantly longer input sequence (336), the model gains greater context, which might allow a shallower encoder (2 layers) and a highly sparse attention mechanism (factor 3) to efficiently extract necessary features for the 168-hour forecast.",
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 3
  },
  {
    "trial": 3,
    "timestamp": "2025-10-12T15:27:13.233481",
    "mae_val": 0.3697561025619507,
    "mse_val": 0.21970032155513763,
    "mae_test": 0.3702394664287567,
    "mse_test": 0.20805545151233673,
    "hypothesis": "Leveraging the maximum context length (720) combined with a moderate encoder depth (3) and intermediate attention sparsity (factor 5) should allow the model to capture very long-range dependencies efficiently, building upon the success of the 336-hour input length.",
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 5
  },
  {
    "trial": 4,
    "timestamp": "2025-10-12T15:40:25.907336",
    "mae_val": 0.5522761344909668,
    "mse_val": 0.40458279848098755,
    "mae_test": 0.3873542249202728,
    "mse_test": 0.22094596922397614,
    "hypothesis": "By maintaining the effective long input sequence (336) but drastically increasing model capacity through maximum encoder depth (6) and using dense attention (factor 10), we test if high complexity can further refine the features extracted for the 168-hour forecast.",
    "seq_len": 336,
    "label_len": 96,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 10
  },
  {
    "trial": 5,
    "timestamp": "2025-10-12T15:49:40.046568",
    "mae_val": 0.5183411240577698,
    "mse_val": 0.37176668643951416,
    "mae_test": 0.35754457116127014,
    "mse_test": 0.19683527946472168,
    "hypothesis": "Utilizing the effective intermediate context length (336), this configuration employs a moderately deep encoder (4 layers) and high complexity (16 heads, factor 8) to see if a denser, more capable model can extract features efficiently, even with a shorter decoder start sequence (48).",
    "seq_len": 336,
    "label_len": 48,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 8
  },
  {
    "trial": 6,
    "timestamp": "2025-10-12T15:58:14.859217",
    "mae_val": 0.7653744220733643,
    "mse_val": 0.7334801554679871,
    "mae_test": 0.4355732500553131,
    "mse_test": 0.294063538312912,
    "hypothesis": "Investigating the limits of short-term context (96), this setup employs maximum model capacity (6 layers, 16 heads, factor 10) to determine if aggressive feature extraction and deep processing can compensate for a relatively small input sequence length when forecasting 168 hours ahead.",
    "seq_len": 96,
    "label_len": 48,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 10
  },
  {
    "trial": 7,
    "timestamp": "2025-10-12T16:20:31.273997",
    "mae_val": 0.4290754795074463,
    "mse_val": 0.26328662037849426,
    "mae_test": 0.3172715902328491,
    "mse_test": 0.15412627160549164,
    "hypothesis": "By maximizing model capacity (6 layers, 16 heads, factor 10) on the proven effective maximum input sequence length (720), we aim to test if a highly complex model can extract extremely nuanced long-range dependencies for the 168-hour forecast, utilizing an intermediate decoder start length (168).",
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 10
  },
  {
    "trial": 8,
    "timestamp": "2025-10-12T16:29:12.572529",
    "mae_val": 0.5262445211410522,
    "mse_val": 0.39672183990478516,
    "mae_test": 0.35607093572616577,
    "mse_test": 0.19022372364997864,
    "hypothesis": "Combining the second-best input length (336) with the complexity profile of the overall best performer (3 layers, 8 heads, factor 5) aims to find an optimal balance between context size and model efficiency, potentially surpassing previous 336-length experiments.",
    "seq_len": 336,
    "label_len": 96,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 5
  },
  {
    "trial": 9,
    "timestamp": "2025-10-12T16:39:46.809182",
    "mae_val": 0.4314678907394409,
    "mse_val": 0.2843656539916992,
    "mae_test": 0.391679048538208,
    "mse_test": 0.24078024923801422,
    "hypothesis": "Exploiting the proven efficiency of the (3, 8, 5) configuration profile on the maximum input length (720), this experiment tests the limit of decoder reliance by minimizing the starting sequence length (48). If the encoder successfully captures 720 hours of dependencies, a short label length should suffice.",
    "seq_len": 720,
    "label_len": 48,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 5
  },
  {
    "trial": 10,
    "timestamp": "2025-10-12T16:47:11.941674",
    "mae_val": 0.656730592250824,
    "mse_val": 0.5307420492172241,
    "mae_test": 0.37032243609428406,
    "mse_test": 0.2118651270866394,
    "hypothesis": "We use a proven effective input length (336) and combine it with a moderately deep encoder (4) and high attention capacity (16 heads, factor 5). This setup tests whether slightly less aggressive depth and intermediate sparsity, compared to previously failing high-capacity tests (6 layers, factor 10), can better leverage the 336-hour context for improved feature extraction.",
    "seq_len": 336,
    "label_len": 96,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 5
  },
  {
    "trial": 11,
    "timestamp": "2025-10-12T16:56:25.444305",
    "mae_val": 0.562101423740387,
    "mse_val": 0.4315008521080017,
    "mae_test": 0.3777410089969635,
    "mse_test": 0.21615198254585266,
    "hypothesis": "By combining the proven efficient encoding configuration (E=3, H=8, F=5) with the intermediate context length (336), and minimizing the decoder starting sequence (label_len=24), we test if this setup is sufficient to efficiently capture relevant features for the 168-hour forecast without relying heavily on the initial decoder input.",
    "seq_len": 336,
    "label_len": 24,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 5
  },
  {
    "trial": 12,
    "timestamp": "2025-10-12T17:12:17.983360",
    "mae_val": 0.38681215047836304,
    "mse_val": 0.2380102127790451,
    "mae_test": 0.3238225281238556,
    "mse_test": 0.15898025035858154,
    "hypothesis": "Utilizing the maximum effective context length (720), this configuration employs an intermediate encoder depth (4) and high capacity attention (16 heads, factor 8) to test if a slightly less complex model than the previous best (E=6, F=10) can still achieve top performance by leveraging the rich input context and a moderate label length (96).",
    "seq_len": 720,
    "label_len": 96,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 8
  },
  {
    "trial": 13,
    "timestamp": "2025-10-12T21:36:35.275871",
    "mae_val": 0.3846486508846283,
    "mse_val": 0.24071651697158813,
    "mae_test": 0.3370581269264221,
    "mse_test": 0.17760156095027924,
    "hypothesis": "Utilizing the maximum proven context length (720) with the shallowest encoder (2 layers) and most aggressive sparsity (factor 3) aims to establish if extreme long-range dependency extraction requires minimal model complexity, relying heavily on the rich input context and minimizing computational cost.",
    "seq_len": 720,
    "label_len": 24,
    "e_layers": 2,
    "n_heads": 16,
    "factor": 3
  },
  {
    "trial": 14,
    "timestamp": "2025-10-12T21:45:26.753498",
    "mae_val": 0.5092040300369263,
    "mse_val": 0.38656002283096313,
    "mae_test": 0.41672763228416443,
    "mse_test": 0.2537194490432739,
    "hypothesis": "Building on the success of efficient configurations for 336-hour input (E=2, F=3), this configuration increases the encoder depth slightly to 3 layers while maintaining maximum sparsity and low head count (8, 3), testing if a modestly deeper, highly efficient structure can outperform previous 336 attempts with a shorter decoder start sequence.",
    "seq_len": 336,
    "label_len": 48,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 3
  },
  {
    "trial": 15,
    "timestamp": "2025-10-12T22:04:28.026865",
    "mae_val": 0.4410174489021301,
    "mse_val": 0.2723369002342224,
    "mae_test": 0.3661019206047058,
    "mse_test": 0.20252364873886108,
    "hypothesis": "Leveraging the maximum context (720) combined with the highest possible model complexity (E6, H16, F10) aims to capture the most detailed long-range dependencies, testing if this aggressive feature extraction allows the decoder to succeed with a minimal start sequence (24).",
    "seq_len": 720,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 10
  },
  {
    "trial": 16,
    "timestamp": "2025-10-12T22:09:11.973096",
    "mae_val": 0.7620125412940979,
    "mse_val": 0.6903257966041565,
    "mae_test": 0.3366687297821045,
    "mse_test": 0.17737574875354767,
    "hypothesis": "Re-testing the 168-hour input length, this configuration deploys the minimum model complexity (E2, H8, F3) to determine if a highly sparse and efficient structure can quickly capture necessary short-to-medium dependencies, compensating for the limited context compared to the successful 720-hour runs.",
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 3
  },
  {
    "trial": 17,
    "timestamp": "2025-10-12T22:28:18.673503",
    "mae_val": 0.5024800300598145,
    "mse_val": 0.36782774329185486,
    "mae_test": 0.4619539678096771,
    "mse_test": 0.3303682804107666,
    "hypothesis": "Focusing on the successful maximum context (720), we aim to test a balanced configuration using the efficient E3 depth, increased attention capacity (16 heads, factor 8) compared to the current best, and an intermediate label length (168) to guide the decoder effectively.",
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 8
  },
  {
    "trial": 18,
    "timestamp": "2025-10-12T22:34:56.402187",
    "mae_val": 0.5080928206443787,
    "mse_val": 0.3409382998943329,
    "mae_test": 0.3662070631980896,
    "mse_test": 0.1955682337284088,
    "hypothesis": "Given the successful performance of highly efficient (low factor, low layers) models on long input sequences, this configuration tests whether maximizing encoder depth (6 layers) combined with maximum sparsity (factor 3) and minimal attention heads (8) can efficiently process 336 hours of input, compensating for the shorter context length relative to the 720-hour runs.",
    "seq_len": 336,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 3
  },
  {
    "trial": 19,
    "timestamp": "2025-10-12T22:49:51.727343",
    "mae_val": 0.5277029871940613,
    "mse_val": 0.39105290174484253,
    "mae_test": 0.5003070831298828,
    "mse_test": 0.36447247862815857,
    "hypothesis": "By combining the maximum context length (720) with the most computationally efficient architecture (E=2, H=8, F=3), we test if the minimal complexity can still extract necessary long-range features, relying on a moderate decoder start sequence (168) for guidance.",
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 3
  },
  {
    "trial": 20,
    "timestamp": "2025-10-12T23:00:29.075911",
    "mae_val": 0.5087749361991882,
    "mse_val": 0.36011868715286255,
    "mae_test": 0.37678518891334534,
    "mse_test": 0.21092940866947174,
    "hypothesis": "Building on the strong performance of deep encoders on 336h context, we maximize depth (E=6) and increase attention capacity (H=16, F=5) to see if this highly complex model can extract more features than previous successful low-capacity 336 runs, using a long decoder start sequence (168).",
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 5
  },
  {
    "trial": 21,
    "timestamp": "2025-10-12T23:17:14.441749",
    "mae_val": 0.3741956949234009,
    "mse_val": 0.20666442811489105,
    "mae_test": 0.31110888719558716,
    "mse_test": 0.1448008418083191,
    "hypothesis": "Combining the maximum effective context (720) and long label length (336) with the deepest possible encoder (6 layers) and maximum attention sparsity (factor 3) tests if the massive input context allows a highly efficient, yet deep, model to capture the most complex long-range dependencies while minimizing computation.",
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 3
  },
  {
    "trial": 22,
    "timestamp": "2025-10-12T23:27:35.850818",
    "mae_val": 0.5546369552612305,
    "mse_val": 0.4206937551498413,
    "mae_test": 0.38902294635772705,
    "mse_test": 0.23152658343315125,
    "hypothesis": "Investigating the 336-hour input context, this configuration uses a moderately deep encoder (4 layers) combined with high attention capacity (16 heads, factor 8) to aggressively extract features, testing if this refined encoding allows for accurate 168-hour prediction despite relying on a minimal decoder start sequence (24).",
    "seq_len": 336,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 8
  },
  {
    "trial": 23,
    "timestamp": "2025-10-12T23:46:26.614889",
    "mae_val": 0.38864603638648987,
    "mse_val": 0.23140227794647217,
    "mae_test": 0.3379591107368469,
    "mse_test": 0.176346093416214,
    "hypothesis": "Based on the success of deep encoders (E=6) on maximum context (720), we test increasing both attention heads (16) and complexity factor (5) compared to the previous best (H=8, F=3), aiming for a highly detailed long-range feature extraction with a moderate label length.",
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 5
  },
  {
    "trial": 24,
    "timestamp": "2025-10-12T23:52:53.163998",
    "mae_val": 0.8231146931648254,
    "mse_val": 0.8492812514305115,
    "mae_test": 0.4705195426940918,
    "mse_test": 0.32662537693977356,
    "hypothesis": "Challenging the dependency on long context, this configuration tests the minimal input length (48) combined with maximum architectural complexity (6 layers, 16 heads, factor 10) to see if aggressive processing can extract critical short-term patterns sufficient for a 168-hour forecast.",
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 10
  },
  {
    "trial": 25,
    "timestamp": "2025-10-13T00:17:38.239445",
    "mae_val": 0.3528810441493988,
    "mse_val": 0.20485427975654602,
    "mae_test": 0.32223138213157654,
    "mse_test": 0.16146381199359894,
    "hypothesis": "By combining the maximum effective context (720) and optimal label length (336) with a deep, high-capacity encoder (E=4, H=16, F=10), we test if maximizing attention detail and layer processing provides a marginal performance gain over sparser configurations (F=3/5) in long-range dependency extraction.",
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 10
  },
  {
    "trial": 26,
    "timestamp": "2025-10-13T00:29:04.583100",
    "mae_val": 0.4879020154476166,
    "mse_val": 0.3798428177833557,
    "mae_test": 0.3802059292793274,
    "mse_test": 0.21877561509609222,
    "hypothesis": "Focusing on the strong intermediate context (336), this configuration utilizes a moderately deep encoder (4 layers) and high attention capacity (16 heads), while maintaining maximum sparsity (F=3) to efficiently extract features, leveraging the longest feasible decoder start sequence (168).",
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 3
  },
  {
    "trial": 27,
    "timestamp": "2025-10-13T00:40:26.468327",
    "mae_val": 0.4091535210609436,
    "mse_val": 0.260329931974411,
    "mae_test": 0.3426463007926941,
    "mse_test": 0.18303680419921875,
    "hypothesis": "Building on the success of 720h input, this configuration tests if a moderate encoder depth (4) combined with maximum sparsity (factor 3) and low heads (8) can maintain high performance, focusing on computational efficiency while leveraging the long context with an intermediate label length.",
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 3
  },
  {
    "trial": 28,
    "timestamp": "2025-10-13T00:59:59.706002",
    "mae_val": 0.3834499716758728,
    "mse_val": 0.22583189606666565,
    "mae_test": 0.33682385087013245,
    "mse_test": 0.17125029861927032,
    "hypothesis": "We explore maximizing encoder depth (6) and attention complexity (factor 10) on the proven 720h context, using fewer heads (8) to reduce overall parameter count, aiming to extract extremely nuanced long-range features with a relatively short decoder start sequence.",
    "seq_len": 720,
    "label_len": 96,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 10
  },
  {
    "trial": 29,
    "timestamp": "2025-10-13T08:38:30.365412",
    "mae_val": 0.3832341134548187,
    "mse_val": 0.23630836606025696,
    "mae_test": 0.36605963110923767,
    "mse_test": 0.1998622715473175,
    "hypothesis": "Given the strong performance of 720-hour context, we test a minimum complexity model (E=2, H=8) combined with moderate attention sparsity (F=5). This configuration aims to prove that the rich input context, guided by an intermediate label length (96), requires minimal layers and capacity for efficient long-range dependency extraction, maximizing computational savings.",
    "seq_len": 720,
    "label_len": 96,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 5
  },
  {
    "trial": 30,
    "timestamp": "2025-10-13T08:47:04.770256",
    "mae_val": 0.6570755243301392,
    "mse_val": 0.5620853900909424,
    "mae_test": 0.37730681896209717,
    "mse_test": 0.22401326894760132,
    "hypothesis": "To compensate for the limited context (168 hours) which previously performed poorly, this configuration maximizes model processing capacity (E=6, H=16) with an intermediate factor (F=5) for detailed feature extraction. We utilize a short label length (48) to force the model to rely primarily on the aggressively encoded input for the 168-hour forecast.",
    "seq_len": 168,
    "label_len": 48,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 5
  },
  {
    "trial": 31,
    "timestamp": "2025-10-13T09:04:14.657034",
    "mae_val": 0.5282763242721558,
    "mse_val": 0.3675556182861328,
    "mae_test": 0.4264756441116333,
    "mse_test": 0.28233668208122253,
    "hypothesis": "Utilizing the proven effective maximum context (720/336), this configuration tests the absolute minimum architectural complexity (E2, H8, F3) to determine if the richness of the input sequence alone is sufficient, maximizing computational efficiency.",
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 3
  },
  {
    "trial": 32,
    "timestamp": "2025-10-13T09:13:11.461782",
    "mae_val": 0.6204936504364014,
    "mse_val": 0.48056137561798096,
    "mae_test": 0.3814544081687927,
    "mse_test": 0.22075219452381134,
    "hypothesis": "Building on successful 336-hour input experiments, this configuration maximizes architectural complexity (E6, H16, F8) to aggressively extract features from the intermediate context length, testing if high capacity can compensate for the shorter input compared to the 720-hour runs.",
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 8
  },
  {
    "trial": 33,
    "timestamp": "2025-10-13T09:32:03.669443",
    "mae_val": 0.4905824363231659,
    "mse_val": 0.3349805474281311,
    "mae_test": 0.38240423798561096,
    "mse_test": 0.22257588803768158,
    "hypothesis": "Building on the successful maximum context (720) and long label length (336), this configuration uses a deep encoder (4 layers) and high head count (16), but tests an intermediate factor (8) to balance complexity and computational efficiency, aiming for marginal gain over the highly dense F=10 configuration.",
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 8
  },
  {
    "trial": 34,
    "timestamp": "2025-10-13T09:40:58.998432",
    "mae_val": 0.5706862211227417,
    "mse_val": 0.4240369200706482,
    "mae_test": 0.42022716999053955,
    "mse_test": 0.2627332806587219,
    "hypothesis": "Building on the observation that deep, highly sparse models (E=6, F=3) performed poorly with minimal label length on 336h input, we increase the decoder start sequence (from 24 to 96) to improve decoder guidance, hoping the aggressive feature extraction from the 336h context can be better utilized.",
    "seq_len": 336,
    "label_len": 96,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 3
  },
  {
    "trial": 35,
    "timestamp": "2025-10-13T10:03:37.021163",
    "mae_val": 0.4122561514377594,
    "mse_val": 0.2673179507255554,
    "mae_test": 0.44648873805999756,
    "mse_test": 0.29947027564048767,
    "hypothesis": "By combining the maximum effective context (720) and long label length (336) with the shallowest encoder (2 layers), we maximize computational efficiency. We offset this minimal depth by maximizing attention capacity (16 heads, factor 10), testing if the rich input context allows a highly detailed, yet shallow, processing pathway to achieve top performance.",
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 2,
    "n_heads": 16,
    "factor": 10
  },
  {
    "trial": 36,
    "timestamp": "2025-10-13T10:16:26.821660",
    "mae_val": 0.55580073595047,
    "mse_val": 0.4354732632637024,
    "mae_test": 0.4214840233325958,
    "mse_test": 0.26671355962753296,
    "hypothesis": "Focusing on the strong intermediate context (336), this configuration employs a moderately deep encoder (3 layers) combined with maximum attention complexity (16 heads, factor 10) to aggressively extract detailed features. We aim to test if this maximal capacity on the 336-hour input, guided by a long label length (168), can close the performance gap with the 720-hour runs.",
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 10
  }
]