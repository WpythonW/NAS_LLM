[
  {
    "trial": 1,
    "timestamp": "2025-10-10T15:31:25.762",
    "mae_val": 0.734349072,
    "mse_val": 0.6608821154,
    "hypothesis": "Эта конфигурация использует умеренно длинный входной горизонт (96ч) и глубокий энкодер (4 слоя), чтобы максимизировать извлечение сложных временных зависимостей. Высокий фактор (10) применяется для обеспечения наименьшей потери информации при разреженном внимании.",
    "seq_len": 96,
    "label_len": 48,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 10
  },
  {
    "trial": 2,
    "timestamp": "2025-10-10T15:34:11.688",
    "mae_val": 0.6108169556,
    "mse_val": 0.4779782295,
    "hypothesis": "Конфигурация нацелена на использование длинного контекста (168ч, неделя) с минимальной глубиной энкодера (2 слоя) для повышения скорости обучения. Увеличенное количество голов (16) и низкий фактор (3) призваны проверить, может ли модель эффективно обрабатывать недельные паттерны, компенсируя неглубокую архитектуру широким вниманием и агрессивной разреженностью.",
    "seq_len": 168,
    "label_len": 24,
    "e_layers": 2,
    "n_heads": 16,
    "factor": 3
  },
  {
    "trial": 3,
    "timestamp": "2025-10-10T15:47:36.868",
    "mae_val": 0.3985698521,
    "mse_val": 0.2305030376,
    "hypothesis": "Utilizing a very long input sequence (336ч) aims to capture bi-weekly patterns inherent in the data. Мы используем среднюю глубину энкодера (3 слоя) и умеренный фактор разреженности (5), чтобы сбалансировать сложность модели и эффективность вычислений, проверяя, требуется ли для такого длинного контекста более консервативная разреженность, чем в лучшем историческом эксперименте.",
    "seq_len": 336,
    "label_len": 96,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 5
  },
  {
    "trial": 4,
    "timestamp": "2025-10-10T15:53:25.990",
    "mae_val": 0.5937220454,
    "mse_val": 0.5171122551,
    "hypothesis": "This configuration tests whether a very deep model (6 слоев) can compensate for a short input sequence (48ч), focusing intensely on extracting short-range dependencies. Увеличенное количество голов (16) и высокий фактор (8) используются для обеспечения детального, но при этом широкого анализа локального контекста.",
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 8
  },
  {
    "trial": 5,
    "timestamp": "2025-10-10T16:09:22.376",
    "mae_val": 0.5163324475,
    "mse_val": 0.3718755245,
    "hypothesis": "We test if utilizing the maximum available context (720ч, месяц) captures essential long-term periodicities, which proved crucial in the best historical experiment (336ч). Мы используем увеличенную глубину энкодера (4 слоя) и большее число голов (16) для эффективной обработки и агрегации информации из этого обширного временного окна, а также более высокий фактор (8) для консервативной разреженности.",
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 8
  },
  {
    "trial": 6,
    "timestamp": "2025-10-10T16:15:59.278",
    "mae_val": 0.667550981,
    "mse_val": 0.5676951408,
    "hypothesis": "This configuration explores if an extremely deep architecture (6 слоев) can maximize feature extraction from a moderate weekly context (168ч), compensating for less context. Мы используем агрессивно низкий фактор разреженности (3) для ускорения вычислений и проверки, способна ли модель поддерживать высокую точность, полагаясь на глубокую обработку, а не на широкий выбор Q-точек.",
    "seq_len": 168,
    "label_len": 48,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 3
  },
  {
    "trial": 7,
    "timestamp": "2025-10-10T16:30:43.787",
    "mae_val": 0.3836722374,
    "mse_val": 0.2291098833,
    "hypothesis": "By maximizing the input sequence length (720ч) and the architectural complexity (6 encoder layers, 16 heads), we aim to capture monthly periodicities and complex dependencies more effectively than previous deep models. We use a moderate factor (5) to balance information retrieval from the extensive context and computational efficiency.",
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 5
  },
  {
    "trial": 8,
    "timestamp": "2025-10-10T16:34:37.617",
    "mae_val": 0.4125988185,
    "mse_val": 0.3302236497,
    "hypothesis": "We test if a shallow model (2 layers) can effectively process a moderate context (96ч) when compensated by a high number of attention heads (16) and the maximum retention factor (10). This setup minimizes computational depth while maximizing the breadth and information density of the sparse attention mechanism.",
    "seq_len": 96,
    "label_len": 24,
    "e_layers": 2,
    "n_heads": 16,
    "factor": 10
  },
  {
    "trial": 9,
    "timestamp": "2025-10-10T16:41:33.783",
    "mae_val": 0.3935006261,
    "mse_val": 0.2383144796,
    "hypothesis": "Мы проверяем, как максимальное удержание информации (factor=10) влияет на очень длинный контекст (336ч) при умеренной глубине энкодера (4 слоя). Это противопоставляется лучшим результатам, где использовался фактор 5, и проверяет, является ли более консервативная разреженность ключом к улучшению извлечения двукратных недельных паттернов.",
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 10
  },
  {
    "trial": 10,
    "timestamp": "2025-10-10T16:48:42.065",
    "mae_val": 0.7203394771,
    "mse_val": 0.6525477171,
    "hypothesis": "Эта конфигурация максимизирует архитектурную сложность (6 слоев, 16 голов) для извлечения недельных закономерностей (168ч), одновременно используя самый агрессивный фактор разреженности (3). Мы проверяем, способна ли глубокая и широкая обработка компенсировать минимальное количество выбранных Q-точек для повышения вычислительной эффективности при сохранении точности.",
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 3
  },
  {
    "trial": 11,
    "timestamp": "2025-10-10T16:56:09.430",
    "mae_val": 0.6185562611,
    "mse_val": 0.5063114762,
    "hypothesis": "Мы тестируем, может ли глубокая архитектура (6 слоев) эффективно обработать недельный контекст (168ч) и достичь топовых результатов, если использовать максимальное количество голов (16) и самый консервативный фактор разреженности (10) для минимизации потери информации.",
    "seq_len": 168,
    "label_len": 48,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 10
  },
  {
    "trial": 12,
    "timestamp": "2025-10-10T17:04:31.281",
    "mae_val": 0.3191345632,
    "mse_val": 0.1758615971,
    "hypothesis": "Эта конфигурация проверяет, является ли максимальная длина входной последовательности (720ч) достаточной для высокой точности, даже при использовании самой мелкой архитектуры энкодера (2 слоя). Мы используем максимальное количество голов (16) и наиболее агрессивный фактор разреженности (3) для достижения высокой вычислительной эффективности.",
    "seq_len": 720,
    "label_len": 48,
    "e_layers": 2,
    "n_heads": 16,
    "factor": 3
  },
  {
    "trial": 13,
    "timestamp": "2025-10-10T17:13:56.842",
    "mae_val": 0.4169357419,
    "mse_val": 0.2725648284,
    "hypothesis": "Мы исследуем, улучшит ли комбинация максимальной длины контекста (720ч) со слегка более глубоким энкодером (3 слоя) и более консервативным фактором разреженности (8) результаты по сравнению с лучшим экспериментом, который использовал минимальную глубину (2 слоя) и агрессивную разреженность. Цель — сбалансировать обширную агрегацию контекста с более высокой емкостью извлечения признаков.",
    "seq_len": 720,
    "label_len": 96,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 8
  },
  {
    "trial": 14,
    "timestamp": "2025-10-10T17:20:52.856",
    "mae_val": 0.4885829985,
    "mse_val": 0.3387320638,
    "hypothesis": "Проверяем, является ли максимальная глубина энкодера (6 слоев) в сочетании с агрессивной разреженностью (factor=3) оптимальной стратегией для обработки двухнедельного контекста (336ч). Эта конфигурация исследует, может ли глубокая и интенсивная обработка компенсировать агрессивный фактор для повышения вычислительной эффективности при сохранении высокой точности.",
    "seq_len": 336,
    "label_len": 48,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 3
  },
  {
    "trial": 15,
    "timestamp": "2025-10-10T17:28:55.891",
    "mae_val": 0.4027498662,
    "mse_val": 0.2508167624,
    "hypothesis": "We combine the proven benefit of maximal context (720ч) and aggressive sparsity (factor=3) from the best experiment, but increase the encoder depth to 3 layers to allow for slightly more complex feature refinement. We reduce the number of attention heads (8) to test if the increased depth compensates for the reduced breadth of attention in this highly efficient setup.",
    "seq_len": 720,
    "label_len": 96,
    "e_layers": 3,
    "n_heads": 8,
    "factor": 3
  },
  {
    "trial": 16,
    "timestamp": "2025-10-10T17:33:04.402",
    "mae_val": 0.5457980037,
    "mse_val": 0.4128620923,
    "hypothesis": "This configuration tests the limit of architectural complexity (6 layers, 16 heads) to extract maximum information from a relatively short input sequence (96ч). We use the maximum conservation factor (10) to minimize information loss in the sparse attention mechanism, aiming to compensate for the limited context length with intense processing.",
    "seq_len": 96,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 10
  },
  {
    "trial": 17,
    "timestamp": "2025-10-10T17:40:53.589",
    "mae_val": 0.451706171,
    "mse_val": 0.294002682,
    "hypothesis": "Исследуем, может ли максимальная глубина энкодера (6 слоев) в сочетании с широким вниманием (16 голов) компенсировать использование двухнедельного контекста (336ч) вместо месячного. Мы используем консервативный фактор (8) для снижения потерь информации при разреженности.",
    "seq_len": 336,
    "label_len": 48,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 8
  },
  {
    "trial": 18,
    "timestamp": "2025-10-10T17:43:02.162",
    "mae_val": 0.6258692741,
    "mse_val": 0.4955691695,
    "hypothesis": "Тестируем, может ли самая простая архитектура энкодера (2 слоя, 8 голов) быть эффективной для обработки минимального контекста (48ч). Мы используем умеренный фактор разреженности (5) для баланса между скоростью и захватом локальных зависимостей.",
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 5
  },
  {
    "trial": 19,
    "timestamp": "2025-10-10T17:51:03.224",
    "mae_val": 0.466235429,
    "mse_val": 0.3060426414,
    "hypothesis": "We explore if combining a bi-weekly context (336ч) with a moderately deep encoder (4 слоев) and a conservative sparsity factor (8) can achieve high performance. This tests whether deeper processing compensates for reduced context length compared to the best performing monthly context models.",
    "seq_len": 336,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 8
  },
  {
    "trial": 20,
    "timestamp": "2025-10-10T17:55:51.966",
    "mae_val": 0.5771762133,
    "mse_val": 0.4440786839,
    "hypothesis": "We test the hypothesis that intense, highly conserved processing of minimal context (48ч, factor=10) can extract critical local dependencies. This uses maximal encoder depth (6 слоев) and contrasts with previous short-sequence failures by maximizing information retention and prioritizing depth over breadth (8 голов).",
    "seq_len": 48,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 10
  },
  {
    "trial": 21,
    "timestamp": "2025-10-10T18:05:28.788",
    "mae_val": 0.3742436767,
    "mse_val": 0.2299329489,
    "hypothesis": "Тестирование влияния более глубокого, но сбалансированного энкодера (4 слоя) и умеренного фактора разреженности (5) на максимальную длину контекста (720ч). Это проверяет, требуется ли для улучшения результатов больше емкости для извлечения признаков и менее агрессивная разреженность, чем в топовом эксперименте.",
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 5
  },
  {
    "trial": 22,
    "timestamp": "2025-10-10T18:11:10.054",
    "mae_val": 0.6939046979,
    "mse_val": 0.6326545477,
    "hypothesis": "Исследование того, может ли умеренно глубокая архитектура (4 слоя) с максимальной широтой внимания (16 голов) и консервативным фактором разреженности (8) эффективно использовать недельный контекст (168ч). Это противопоставляется плохим результатам предыдущих экспериментов на 168ч, которые часто использовали крайние значения глубины и фактора.",
    "seq_len": 168,
    "label_len": 24,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 8
  },
  {
    "trial": 23,
    "timestamp": "2025-10-10T18:16:36.000",
    "mae_val": 0.3354277015,
    "mse_val": 0.1797967106,
    "hypothesis": "We test if a minimal encoder depth (2 layers) can effectively process the two-week context (336ч), provided we maximize attention breadth (16 heads) and use the maximum information retention factor (10). This setup contrasts the high-efficiency/aggressive-sparsity approach of the best model (factor=3) by prioritizing detailed, but shallow, processing.",
    "seq_len": 336,
    "label_len": 96,
    "e_layers": 2,
    "n_heads": 16,
    "factor": 10
  },
  {
    "trial": 24,
    "timestamp": "2025-11-17T18:19:02.143559",
    "mae_val": 0.6135527491569519,
    "mse_val": 0.5141134262084961,
    "mae_test": 0.4031467139720917,
    "mse_test": 0.24667994678020477,
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 2,
    "n_heads": 16,
    "factor": 5,
    "hypothesis": "Мы проверяем, может ли недельный контекст (168ч), который плохо работал ранее, достичь высокой точности, если использовать мелкую, но широкую архитектуру энкодера (2 слоя, 16 голов) в сочетании с умеренным фактором разреженности (5)."
  },
  {
    "trial": 25,
    "timestamp": "2025-11-17T18:29:56.312914",
    "mae_val": 0.3351932764053345,
    "mse_val": 0.18784178793430328,
    "mae_test": 0.33882227540016174,
    "mse_test": 0.16983063519001007,
    "seq_len": 720,
    "label_len": 336,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 10,
    "hypothesis": "Эта конфигурация использует максимальный контекст (720ч) для очень длинного прогноза (336ч) и проверяет, является ли комбинация умеренной глубины энкодера (3 слоя), максимального числа голов (16) и консервативного фактора (10) оптимальной для удержания сложных месячных паттернов."
  },
  {
    "trial": 26,
    "timestamp": "2025-11-17T18:34:10.410065",
    "mae_val": 0.45171043276786804,
    "mse_val": 0.2913622260093689,
    "mae_test": 0.3147488534450531,
    "mse_test": 0.15367311239242554,
    "seq_len": 336,
    "label_len": 24,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 3,
    "hypothesis": "We test whether a moderately deep (3 layers) and broad (16 heads) architecture can efficiently capture bi-weekly patterns (336ч) for a short forecast (24ч) when coupled with the most aggressive sparsity factor (3), seeking a balance between complexity and efficiency."
  },
  {
    "trial": 27,
    "timestamp": "2025-11-17T18:42:38.970886",
    "mae_val": 0.34966734051704407,
    "mse_val": 0.19218796491622925,
    "mae_test": 0.2805027365684509,
    "mse_test": 0.12071845680475235,
    "seq_len": 720,
    "label_len": 24,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 3,
    "hypothesis": "This configuration investigates if maximum architectural depth (6 слоев) is beneficial for processing the monthly context (720ч) for a short forecast (24ч). We pair this depth with reduced breadth (8 голов) and maximum efficiency (factor 3) to test if intensive processing outweighs broad attention."
  },
  {
    "trial": 28,
    "timestamp": "2025-11-17T18:50:46.608715",
    "mae_val": 0.35293009877204895,
    "mse_val": 0.20778900384902954,
    "mae_test": 0.3347627818584442,
    "mse_test": 0.1703055053949356,
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 3,
    "hypothesis": "Мы комбинируем проверенный длинный контекст (720ч) и максимальную эффективность (factor=3) с умеренно глубокой (3 слоя) и широкой (16 голов) архитектурой, стремясь сохранить высокую точность для недельного прогноза, балансируя сложность и скорость."
  },
  {
    "trial": 29,
    "timestamp": "2025-11-17T18:56:34.431135",
    "mae_val": 0.42889708280563354,
    "mse_val": 0.2903291881084442,
    "mae_test": 0.39412441849708557,
    "mse_test": 0.23166677355766296,
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 6,
    "n_heads": 8,
    "factor": 5,
    "hypothesis": "Эта конфигурация тестирует способность самого глубокого энкодера (6 слоев) извлекать сложные паттерны из двухнедельного контекста (336ч) для недельного прогноза, используя уменьшенную ширину внимания (8 голов) и умеренный фактор разреженности (5) для балансировки вычислительной нагрузки."
  },
  {
    "trial": 30,
    "timestamp": "2025-11-17T19:10:09.503076",
    "mae_val": 0.3930223286151886,
    "mse_val": 0.23313835263252258,
    "mae_test": 0.33997926115989685,
    "mse_test": 0.17764057219028473,
    "seq_len": 720,
    "label_len": 168,
    "e_layers": 4,
    "n_heads": 16,
    "factor": 5,
    "hypothesis": "We investigate if combining the maximum context (720ч) with a balanced architecture (4 encoder layers, 16 heads) and moderate sparsity (factor 5) provides optimal feature extraction capacity for weekly forecasts, testing a configuration intermediate between highly aggressive and highly conservative successful models."
  },
  {
    "trial": 31,
    "timestamp": "2025-11-17T19:13:27.598515",
    "mae_val": 0.4158828854560852,
    "mse_val": 0.2657237648963928,
    "mae_test": 0.33534908294677734,
    "mse_test": 0.1808205395936966,
    "seq_len": 336,
    "label_len": 24,
    "e_layers": 2,
    "n_heads": 8,
    "factor": 3,
    "hypothesis": "This configuration explores maximum efficiency (shallow encoder, aggressive sparsity) on a bi-weekly context (336ч) for short-term prediction, testing if minimal depth and narrow attention are sufficient when prioritizing computational speed."
  },
  {
    "trial": 32,
    "timestamp": "2025-11-17T19:16:38.525226",
    "mae_val": 0.6545248627662659,
    "mse_val": 0.621957540512085,
    "mae_test": 0.3726269602775574,
    "mse_test": 0.2250768095254898,
    "seq_len": 96,
    "label_len": 48,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 3,
    "hypothesis": "We test if a moderate context (96ч) can yield high performance when combined with a moderately deep, wide encoder (3 слоев, 16 голов) and maximal sparsity efficiency (factor=3), leveraging capacity and speed to extract local patterns effectively."
  },
  {
    "trial": 33,
    "timestamp": "2025-11-17T19:23:15.072511",
    "mae_val": 0.5139994025230408,
    "mse_val": 0.34150516986846924,
    "mae_test": 0.35281381011009216,
    "mse_test": 0.19394005835056305,
    "seq_len": 336,
    "label_len": 168,
    "e_layers": 6,
    "n_heads": 16,
    "factor": 10,
    "hypothesis": "We investigate if combining the maximum architectural complexity (6 слоев, 16 голов) with the highest information retention factor (10) can effectively process bi-weekly patterns (336ч) for long-range prediction (168ч), prioritizing detailed analysis over computational speed."
  },
  {
    "trial": 34,
    "timestamp": "2025-11-17T19:31:29.228304",
    "mae_val": 0.31830894947052,
    "mse_val": 0.16569386422634125,
    "mae_test": 0.35242944955825806,
    "mse_test": 0.18970626592636108,
    "seq_len": 720,
    "label_len": 96,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 3,
    "hypothesis": "Мы исследуем, может ли баланс между умеренной глубиной энкодера (4 слоя) и узким вниманием (8 голов) обеспечить высокую точность, когда используется максимальный контекст (720ч) в сочетании с максимальной вычислительной эффективностью (factor=3)."
  },
  {
    "trial": 35,
    "timestamp": "2025-11-17T19:36:05.076479",
    "mae_val": 0.738466203212738,
    "mse_val": 0.7063859105110168,
    "mae_test": 0.39457157254219055,
    "mse_test": 0.24554017186164856,
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 4,
    "n_heads": 8,
    "factor": 10,
    "hypothesis": "Конфигурация нацелена на улучшение плохих результатов, полученных ранее на контексте 168ч, путем использования максимального удержания информации (factor=10) и сбалансированной архитектуры (4 слоя, 8 голов), проверяя, критичен ли консервативный подход к разреженности для недельного контекста при длинном прогнозе."
  },
  {
    "trial": 36,
    "timestamp": "2025-11-17T19:41:28.135124",
    "mae_val": 0.7701533436775208,
    "mse_val": 0.7954717874526978,
    "mae_test": 0.4397434592247009,
    "mse_test": 0.309717059135437,
    "seq_len": 168,
    "label_len": 96,
    "e_layers": 3,
    "n_heads": 16,
    "factor": 10,
    "hypothesis": "Мы тестируем, может ли умеренно глубокий энкодер (3 слоя) с максимальной широтой внимания (16 голов) и самым консервативным фактором разреженности (10) улучшить слабые результаты, полученные ранее на недельном контексте (168ч)."
  }
]