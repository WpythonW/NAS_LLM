[
  {
    "trial":1,
    "timestamp":"2025-10-10T14:43:28.560",
    "mae":0.3656995893,
    "mse":0.1946983635,
    "hypothesis":"Эта конфигурация использует относительно короткую входную последовательность (48 часов, т.е. 2 дня) для предсказания следующих 24 часов. Для часовых данных с суточной сезонностью (как ETTh1), 48 часов достаточно, чтобы захватить два полных ежедневных цикла. Используется умеренная глубина (3 слоя) и стандартные настройки внимания (8 голов, фактор 5) для проверки эффективности и обобщающей способности модели на базовом уровне сложности.",
    "seq_len":48,
    "label_len":24,
    "e_layers":3,
    "n_heads":8,
    "factor":5
  },
  {
    "trial":2,
    "timestamp":"2025-10-10T14:50:08.534",
    "mae":0.4066842496,
    "mse":0.2542738616,
    "hypothesis":"Эта конфигурация предназначена для исследования долгосрочных зависимостей путем использования значительно более длинной входной последовательности (336 часов, т.е. 2 недели). Высокая сложность (6 слоев энкодера, 16 голов, фактор 10) выбрана для максимизации способности модели извлекать сложные паттерны из обширной истории. label_len=96 обеспечивает существенный контекст для инициализации декодера.",
    "seq_len":336,
    "label_len":96,
    "e_layers":6,
    "n_heads":16,
    "factor":10
  },
  {
    "trial":3,
    "timestamp":"2025-10-10T14:53:38.084",
    "mae":0.4618688524,
    "mse":0.2897603512,
    "hypothesis":"Эта конфигурация исследует эффект умеренно длинной истории (96 часов, 4 дня) для захвата суточной и, возможно, двухдневной сезонности, что критически важно для часовых данных. Мы используем существенный контекст инициализации декодера (label_len=48), чтобы дать модели достаточно недавних данных для старта предсказания. Увеличение e_layers до 4 и factor до 8 по сравнению с лучшим предыдущим результатом (3 слоя, factor 5) направлено на повышение емкости модели для извлечения более сложных паттернов из этого расширенного временного окна.",
    "seq_len":96,
    "label_len":48,
    "e_layers":4,
    "n_heads":8,
    "factor":8
  },
  {
    "trial":4,
    "timestamp":"2025-10-10T15:01:30.825",
    "mae":0.2858940959,
    "mse":0.1377858818,
    "hypothesis":"Данная конфигурация тестирует влияние максимальной входной длины (720 часов, 30 дней) на краткосрочное предсказание (24 часа). Идея заключается в том, что даже слабые, но важные долгосрочные паттерны (месячные) могут улучшить прогноз. Для борьбы с потенциальным шумом и переобучением на таком большом объеме данных, мы используем минимальную глубину энкодера (e_layers=2) и минимальный factor=3, но компенсируем это высоким количеством голов внимания (n_heads=16) для обеспечения разнообразия представлений. label_len=24 выбрана минимально для инициализации, заставляя декодер опираться в основном на информацию, извлеченную из всей 30-дневной истории.",
    "seq_len":720,
    "label_len":24,
    "e_layers":2,
    "n_heads":16,
    "factor":3
  },
  {
    "trial":5,
    "timestamp":"2025-10-10T15:05:03.581",
    "mae":0.3106723428,
    "mse":0.1607261449,
    "hypothesis":"Эта конфигурация направлена на исследование эффекта захвата полной недельной сезонности (seq_len=168 часов) для краткосрочного прогноза (24 часа). Используя умеренную длину контекста для декодера (label_len=48), мы даем модели достаточно недавних данных для старта, избегая при этом необходимости максимальной длины (720 часов), которая может содержать много шума. Мы используем среднюю глубину энкодера (e_layers=3) и высокую параллельность внимания (n_heads=16, как в лучшей конфигурации) в сочетании с умеренным фактором (factor=5), чтобы обеспечить достаточную емкость для извлечения недельных паттернов, но избежать переобучения, наблюдавшегося при высокой сложности (e_layers=6, factor=10).",
    "seq_len":168,
    "label_len":48,
    "e_layers":3,
    "n_heads":16,
    "factor":5
  },
  {
    "trial":6,
    "timestamp":"2025-10-10T15:07:56.974",
    "mae":0.3435767889,
    "mse":0.1947331876,
    "hypothesis":"Эта конфигурация проверяет, может ли существенное увеличение емкости модели улучшить предсказание, используя только минимально необходимый контекст (seq_len=48, label_len=24), ориентированный на суточную сезонность. Предыдущий эксперимент с seq_len=48 использовал умеренную сложность (3 слоя, factor 5). Мы максимизируем глубину энкодера (e_layers=4) и фактор дистилляции (factor=10) при сохранении стандартного числа голов внимания (n_heads=8), чтобы оценить, насколько мощная модель может извлечь максимум информации из короткого, но релевантного временного окна, и превзойти результаты, полученные с более сложными, но потенциально 'зашумленными' длинными последовательностями.",
    "seq_len":48,
    "label_len":24,
    "e_layers":4,
    "n_heads":8,
    "factor":10
  },
  {
    "trial":7,
    "timestamp":"2025-10-10T15:18:55.385",
    "mae":0.3472147584,
    "mse":0.1953257173,
    "hypothesis":"Эта конфигурация предназначена для тестирования влияния использования максимальной истории (720 часов, 30 дней), что дало лучший результат ранее (MSE 0.2859), но в сочетании с очень длинным контекстом инициализации декодера (label_len=336). Идея заключается в том, что предоставление декодеру значительного объема предварительно обработанных данных (2 недели) может улучшить стабильность прогноза. Мы используем умеренную глубину энкодера (e_layers=3) и фактор дистилляции (factor=5) для повышения емкости по сравнению с минимальной (e_layers=2, factor=3) конфигурацией-лидером, сохраняя при этом высокую параллельность внимания (n_heads=16), которая доказала свою эффективность на длинных последовательностях.",
    "seq_len":720,
    "label_len":336,
    "e_layers":3,
    "n_heads":16,
    "factor":5
  },
  {
    "trial":8,
    "timestamp":"2025-10-10T15:21:39.828",
    "mae":0.345304966,
    "mse":0.1775928885,
    "hypothesis":"Данная конфигурация исследует, можно ли эффективно извлечь недельные сезонные паттерны (seq_len=168), используя минимальный, но высокоэффективный энкодер. Мы устанавливаем минимальную глубину слоев (e_layers=2), как в лучшем эксперименте (seq_len=720), чтобы предотвратить переобучение на относительно коротком окне. Высокий фактор дистилляции (factor=8) призван агрессивно сжимать информацию, а минимальный label_len=24 заставляет декодер полагаться на сжатые признаки, извлеченные энкодером из полной недельной истории, а не на немедленный контекст. Мы используем стандартное число голов (n_heads=8), чтобы оценить, достаточен ли потенциал дистилляции (factor=8) для компенсации меньшего числа голов по сравнению с предыдущими успешными моделями.",
    "seq_len":168,
    "label_len":24,
    "e_layers":2,
    "n_heads":8,
    "factor":8
  },
  {
    "trial":9,
    "timestamp":"2025-10-10T15:28:22.105",
    "mae":0.3118792474,
    "mse":0.1569615006,
    "hypothesis":"Эта конфигурация исследует, можно ли улучшить результат, полученный на максимальной истории (720ч), используя более умеренную, но все еще длинную историю (336 часов, две недели). Мы сохраняем успешную структуру лучшей модели: минимальная глубина энкодера (e_layers=2) для предотвращения переобучения и максимальное число голов внимания (n_heads=16) для разнообразия извлеченных признаков. Увеличение фактора дистилляции до 8 (по сравнению с factor=3 в лучшем эксперименте) призвано проверить, поможет ли более агрессивное сжатие информации лучше выделить ключевые недельные\/двухнедельные паттерны из 336-часовой последовательности. label_len=48 обеспечивает достаточный недавний контекст для инициализации декодера.",
    "seq_len":336,
    "label_len":48,
    "e_layers":2,
    "n_heads":16,
    "factor":8
  },
  {
    "trial":10,
    "timestamp":"2025-10-10T15:31:12.011",
    "mae":0.3876311481,
    "mse":0.2277918905,
    "hypothesis":"Данная конфигурация предназначена для максимизации производительности на минимально возможной истории (48 часов), которая содержит два полных суточных цикла. Предыдущие эксперименты с seq_len=48 использовали 8 голов внимания. Мы максимально увеличиваем сложность модели, используя максимальную глубину энкодера (e_layers=4) и максимальное число голов (n_heads=16), чтобы гарантировать, что модель сможет извлечь максимум информации из этого короткого окна. При этом мы устанавливаем минимальный фактор дистилляции (factor=3), чтобы избежать потери важной краткосрочной информации, что является противоположностью успешной конфигурации (e_layers=4, factor=10) на той же длине последовательности.",
    "seq_len":48,
    "label_len":24,
    "e_layers":4,
    "n_heads":16,
    "factor":3
  },
  {
    "trial":11,
    "timestamp":"2025-10-10T15:35:21.934",
    "mae":0.3845355809,
    "mse":0.2241282016,
    "hypothesis":"Эта конфигурация предназначена для максимизации емкости модели на умеренно длинной входной последовательности (96 часов, 4 дня), которая ранее показывала плохие результаты (MSE 0.4619). Мы используем максимальное число слоев энкодера (e_layers=6), максимальное число голов внимания (n_heads=16) и максимальный фактор дистилляции (factor=10). Идея состоит в том, чтобы проверить, может ли агрессивная сложность модели компенсировать потенциально недостаточную длину входной последовательности для извлечения всех необходимых суточных\/многодневных паттернов, используя при этом минимальный контекст инициализации декодера (label_len=24).",
    "seq_len":96,
    "label_len":24,
    "e_layers":6,
    "n_heads":16,
    "factor":10
  },
  {
    "trial":12,
    "timestamp":"2025-10-10T15:38:53.961",
    "mae":0.3653084636,
    "mse":0.1911386549,
    "hypothesis":"Данная конфигурация исследует эффективность сочетания захвата полной недельной сезонности (seq_len=168) с очень длинным контекстом инициализации декодера (label_len=96), что ранее не тестировалось на этом окне. Мы используем минимальную глубину энкодера (e_layers=2) и максимальное число голов (n_heads=16), основываясь на успехе этих параметров на самых длинных последовательностях (720ч и 336ч), чтобы предотвратить переобучение. Увеличение фактора дистилляции до максимального (factor=10) позволит проверить, насколько агрессивное сжатие информации совместимо с передачей большого контекста в декодер.",
    "seq_len":168,
    "label_len":96,
    "e_layers":2,
    "n_heads":16,
    "factor":10
  },
  {
    "trial":13,
    "timestamp":"2025-10-10T15:45:45.364",
    "mae":0.3489030004,
    "mse":0.1848177314,
    "hypothesis":"Эта конфигурация предназначена для исследования влияния максимальной емкости энкодера на двухнедельной истории (seq_len=336). Ранее, длинные последовательности в сочетании с минимальной глубиной энкодера (e_layers=2, factor=3) показывали лучший результат (MSE 0.2859). Здесь мы используем высокую сложность (e_layers=4, factor=10) и максимальное число голов (n_heads=16) для агрессивного извлечения и дистилляции сложных недельных\/двухнедельных паттернов. Минимальный label_len=24 используется, чтобы заставить декодер опираться исключительно на высоко сжатые признаки, извлеченные из длинной последовательности, а не на немедленный контекст.",
    "seq_len":336,
    "label_len":24,
    "e_layers":4,
    "n_heads":16,
    "factor":10
  },
  {
    "trial":14,
    "timestamp":"2025-10-10T15:50:46.765",
    "mae":0.2294844091,
    "mse":0.0827615634,
    "hypothesis":"Данная конфигурация исследует стратегию максимизации контекста инициализации декодера (label_len=168, одна неделя) при использовании двухнедельной истории (seq_len=336). В отличие от успешных экспериментов, где label_len был минимальным, здесь мы проверяем, может ли очень длинный контекст стабилизировать прогноз. Мы используем минимальную глубину энкодера (e_layers=2), чтобы избежать переобучения на столь большом объеме данных, и стандартное число голов (n_heads=8) в сочетании с умеренным фактором дистилляции (factor=5). Цель — проверить, превзойдет ли стабильность, предоставляемая длинным label_len, эффективность агрессивно сжатых признаков.",
    "seq_len":336,
    "label_len":168,
    "e_layers":2,
    "n_heads":8,
    "factor":5
  },
  {
    "trial":15,
    "timestamp":"2025-10-10T16:01:29.620",
    "mae":0.3765134215,
    "mse":0.2186137736,
    "hypothesis":"Эта конфигурация нацелена на сочетание максимальной входной последовательности (720 часов, 30 дней) с успешной стратегией очень длинного контекста инициализации декодера (label_len=168, одна неделя), что привело к лучшему результату в истории (MSE 0.2295). Чтобы гарантировать, что модель сможет эффективно обработать и сжать информацию из 30 дней, мы используем высокий уровень сложности энкодера (e_layers=4) и максимальное число голов (n_heads=16), а также высокий фактор дистилляции (factor=8). Это агрессивная комбинация, не тестировавшаяся ранее на максимальной входной длине, и она призвана проверить, может ли избыточная емкость модели улучшить извлечение долгосрочных паттернов при наличии стабилизирующего длительного контекста.",
    "seq_len":720,
    "label_len":168,
    "e_layers":4,
    "n_heads":16,
    "factor":8
  },
  {
    "trial":16,
    "timestamp":"2025-10-10T16:08:01.456",
    "mae":0.6049605608,
    "mse":0.4681370556,
    "hypothesis":"Данная конфигурация исследует эффект максимальной глубины энкодера (e_layers=6) для недельной последовательности (seq_len=168). В то время как успешные длинные последовательности использовали минимальную глубину (e_layers=2 или 3) для предотвращения переобучения, мы проверяем гипотезу, что для данных с сильной недельной сезонностью (ETTh1) более глубокий энкодер может извлечь более сложные иерархические признаки. Мы используем умеренный контекст инициализации (label_len=48) и минимальный фактор дистилляции (factor=3) в сочетании со стандартным числом голов (n_heads=8), чтобы избежать потери деталей и оценить, превзойдет ли глубокая, но менее параллельная модель неглубокие, но высокопараллельные конфигурации.",
    "seq_len":168,
    "label_len":48,
    "e_layers":6,
    "n_heads":8,
    "factor":3
  },
  {
    "trial":17,
    "timestamp":"2025-10-10T16:17:20.234",
    "mae":0.2771062553,
    "mse":0.127022922,
    "hypothesis":"Тестирование комбинации абсолютной максимальной истории (720 часов, 30 дней) с очень длинным контекстом инициализации декодера (label_len=336, две недели), что ранее показало эффективность на seq_len=336. Чтобы избежать переобучения на таком большом объеме данных, мы используем минимальную глубину энкодера (e_layers=2) и минимальный фактор дистилляции (factor=3), как в одной из лучших конфигураций. Мы используем n_heads=8, чтобы оценить, достаточна ли стандартная параллельность внимания для обработки 30-дневной истории при условии, что стабильность прогноза обеспечивается длинным label_len.",
    "seq_len":720,
    "label_len":336,
    "e_layers":2,
    "n_heads":8,
    "factor":3
  },
  {
    "trial":18,
    "timestamp":"2025-10-10T16:21:53.000",
    "mae":0.3028245866,
    "mse":0.1499013901,
    "hypothesis":"Эта конфигурация максимизирует емкость модели (e_layers=6, n_heads=16, factor=10) на последовательности, захватывающей полную недельную сезонность (seq_len=168). В отличие от предыдущих неудачных попыток с глубокими энкодерами на 168ч, мы используем максимальное число голов внимания (16) и максимальный фактор дистилляции (10). Минимальный label_len=24 используется, чтобы заставить максимально сложный энкодер агрессивно извлекать и сжимать всю необходимую информацию из недельной истории, а не полагаться на немедленный контекст.",
    "seq_len":168,
    "label_len":24,
    "e_layers":6,
    "n_heads":16,
    "factor":10
  },
  {
    "trial":19,
    "timestamp":"2025-10-10T16:27:20.409",
    "mae":0.2540315688,
    "mse":0.1074183509,
    "hypothesis":"Лучший результат был достигнут с использованием длинного контекста (label_len=168) на двухнедельной истории (seq_len=336) при минимальной глубине энкодера (e_layers=2) и стандартном внимании (n_heads=8). Мы тестируем, может ли увеличение параллельности внимания до максимального (n_heads=16), при сохранении всех остальных успешных параметров (seq_len=336, label_len=168, e_layers=2, factor=5), дополнительно улучшить извлечение комплексных признаков из двухнедельной истории и стабилизировать прогноз, превзойдя предыдущий рекорд.",
    "seq_len":336,
    "label_len":168,
    "e_layers":2,
    "n_heads":16,
    "factor":5
  },
  {
    "trial":20,
    "timestamp":"2025-10-10T16:37:31.160",
    "mae":0.3636449277,
    "mse":0.2065869272,
    "hypothesis":"Предыдущие успешные эксперименты с максимальной историей (seq_len=720) полагались на минимальную глубину энкодера (e_layers=2). Эта конфигурация тестирует противоположный подход: использование высокой сложности энкодера (e_layers=4, n_heads=16, factor=10) для агрессивного извлечения и дистилляции сложных долгосрочных паттернов из 30-дневной последовательности. Мы используем умеренный контекст инициализации (label_len=96, 4 дня), чтобы дать декодеру больше информации, чем минимальные 24 часа, но при этом заставить его опираться на признаки, извлеченные высокоемким энкодером.",
    "seq_len":720,
    "label_len":96,
    "e_layers":4,
    "n_heads":16,
    "factor":10
  },
  {
    "trial":21,
    "timestamp":"2025-10-10T16:45:48.622",
    "mae":0.3207187653,
    "mse":0.1638695151,
    "hypothesis":"Эта конфигурация исследует, может ли максимальная глубина энкодера (e_layers=6) улучшить результаты на самой успешной связке длины последовательности и контекста инициализации (seq_len=336, label_len=168). Предыдущие лучшие результаты использовали минимальную глубину (e_layers=2). Мы сохраняем высокую параллельность внимания (n_heads=16) и используем минимальный фактор дистилляции (factor=3), предполагая, что глубокий энкодер извлечет иерархические признаки сам по себе, и агрессивное сжатие не потребуется, что может сохранить критические детали, необходимые для стабильного прогноза.",
    "seq_len":336,
    "label_len":168,
    "e_layers":6,
    "n_heads":16,
    "factor":3
  },
  {
    "trial":22,
    "timestamp":"2025-10-10T16:59:56.995",
    "mae":0.3374603093,
    "mse":0.1699443609,
    "hypothesis":"Данная конфигурация предназначена для тестирования максимальной сложности энкодера (e_layers=6) на абсолютной максимальной истории (seq_len=720). В отличие от успешных экспериментов на 720ч, где использовались минимальные e_layers для предотвращения переобучения, мы проверяем гипотезу о том, что глубокая модель может извлечь более тонкие долгосрочные (месячные) паттерны. Мы используем умеренный контекст инициализации (label_len=96) и стандартное число голов (n_heads=8) и умеренный фактор дистилляции (factor=5), чтобы сбалансировать агрессивную емкость энкодера и избежать чрезмерного переобучения, которое может возникнуть при максимальных n_heads и factor.",
    "seq_len":720,
    "label_len":96,
    "e_layers":6,
    "n_heads":8,
    "factor":5
  },
  {
    "trial":23,
    "timestamp":"2025-10-10T17:07:30.062",
    "mae":0.4072015285,
    "mse":0.2363072485,
    "hypothesis":"Тестирование пределов емкости модели (e_layers=6, factor=10) на наиболее успешной конфигурации контекста (seq_len=336, label_len=168). Предыдущие успешные модели использовали минимальную глубину энкодера (e_layers=2). Гипотеза состоит в том, что максимальная глубина в сочетании с агрессивной дистилляцией и стабилизирующим очень длинным контекстом инициализации декодера (168 часов) может улучшить извлечение сложных иерархических признаков, не приводя к переобучению.",
    "seq_len":336,
    "label_len":168,
    "e_layers":6,
    "n_heads":8,
    "factor":10
  },
  {
    "trial":24,
    "timestamp":"2025-10-10T17:17:31.965",
    "mae":0.3064658344,
    "mse":0.1503220648,
    "hypothesis":"Исследование влияния агрессивной дистилляции (factor=10) и максимальной параллельности внимания (n_heads=16) на максимальной входной последовательности (720 часов), сохраняя при этом минимальную глубину энкодера (e_layers=2), что было ключом к успеху на длинных окнах. В отличие от предыдущего лидера на 720ч (factor=3), мы проверяем, может ли агрессивное сжатие лучше выделить месячные паттерны при условии, что декодеру дается только умеренный контекст (label_len=48).",
    "seq_len":720,
    "label_len":48,
    "e_layers":2,
    "n_heads":16,
    "factor":10
  },
  {
    "trial":25,
    "timestamp":"2025-10-10T17:20:56.569",
    "mae":0.5412842631,
    "mse":0.3772825301,
    "hypothesis":"Лучшие результаты были достигнуты при использовании минимальной глубины энкодера (e_layers=2) в сочетании с длинным контекстом инициализации декодера (label_len). Мы применяем эту успешную стратегию к недельному окну истории (seq_len=168), используя максимальный возможный контекст инициализации (label_len=96), что ранее не тестировалось с n_heads=8 и factor=5. Цель состоит в том, чтобы проверить, достаточно ли еженедельной истории для достижения высокой точности, если декодеру предоставлен максимально возможный стабилизирующий контекст, а минимальная глубина энкодера предотвращает переобучение.",
    "seq_len":168,
    "label_len":96,
    "e_layers":2,
    "n_heads":8,
    "factor":5
  },
  {
    "trial":26,
    "timestamp":"2025-10-10T17:32:43.383",
    "mae":0.3791529238,
    "mse":0.2158330083,
    "hypothesis":"Предыдущие успешные эксперименты с максимальной историей (seq_len=720) полагались на минимальную глубину энкодера (e_layers=2) для предотвращения шума. Эта конфигурация исследует противоположную крайность: сочетание максимальной сложности энкодера (e_layers=6, n_heads=16, factor=10) с длинным, стабилизирующим контекстом инициализации (label_len=168). Гипотеза состоит в том, что максимальная емкость модели позволит агрессивно извлечь и дистиллировать наиболее тонкие долгосрочные (месячные) паттерны из 30-дневной последовательности, а недельный контекст декодера предотвратит катастрофическое переобучение, наблюдавшееся в других высокосложных конфигурациях.",
    "seq_len":720,
    "label_len":168,
    "e_layers":6,
    "n_heads":16,
    "factor":10
  },
  {
    "trial":27,
    "timestamp":"2025-10-10T17:36:59.143",
    "mae":0.2712513804,
    "mse":0.1299457699,
    "hypothesis":"Эта конфигурация предназначена для исследования, может ли максимальная емкость модели компенсировать использование умеренно короткой истории (96 часов, 4 дня). Мы используем максимальную глубину энкодера (e_layers=6) и максимальное число голов внимания (n_heads=16) для агрессивного извлечения сложных суточных и двухдневных паттернов. Минимальный контекст инициализации (label_len=24) заставляет модель полагаться на извлеченные энкодером признаки. Мы выбрали высокий, но не максимальный фактор дистилляции (factor=8) в качестве компромисса, чтобы избежать чрезмерной потери информации, которая могла наблюдаться в предыдущих высокосложных, но неудачных экспериментах на этой длине последовательности (например, factor=10).",
    "seq_len":96,
    "label_len":24,
    "e_layers":6,
    "n_heads":16,
    "factor":8
  },
  {
    "trial":28,
    "timestamp":"2025-10-10T17:46:28.760",
    "mae":0.2947714031,
    "mse":0.1635767817,
    "hypothesis":"Данная конфигурация развивает стратегию, которая привела к лучшим результатам (использование минимальной глубины энкодера на длинных последовательностях в сочетании с длинным контекстом инициализации). Мы используем абсолютную максимальную историю (seq_len=720) и стабилизирующий недельный контекст (label_len=168). Сохраняя минимальную глубину (e_layers=2) для предотвращения переобучения на 30-дневной последовательности, мы максимизируем параллельность внимания (n_heads=16) и фактор дистилляции (factor=10). Это тестирует гипотезу о том, что агрессивное сжатие и параллельная обработка, контролируемые простой структурой энкодера, могут эффективно выделить критические долгосрочные (месячные) паттерны и превзойти предыдущие результаты, достигнутые с менее агрессивными параметрами на максимальных последовательностях.",
    "seq_len":720,
    "label_len":168,
    "e_layers":2,
    "n_heads":16,
    "factor":10
  },
  {
    "trial":29,
    "timestamp":"2025-10-10T17:56:19.300",
    "mae":0.2169543058,
    "mse":0.0823832229,
    "hypothesis":"Эта конфигурация исследует сочетание максимальной истории (720 часов, 30 дней) с очень длинным контекстом инициализации декодера (label_len=336, 2 недели). Успешные эксперименты на длинных последовательностях использовали минимальную глубину энкодера (e_layers=2) для предотвращения переобучения. Мы сохраняем минимальную глубину и стандартное число голов (n_heads=8), но увеличиваем фактор дистилляции до 5 (по сравнению с factor=3 в предыдущем успешном эксперименте на этой связке) в надежде, что умеренное увеличение агрессивности дистилляции поможет эффективнее выделить долгосрочные месячные паттерны, не нарушая стабильности, обеспечиваемой длинным label_len.",
    "seq_len":720,
    "label_len":336,
    "e_layers":2,
    "n_heads":8,
    "factor":5
  },
  {
    "trial":30,
    "timestamp":"2025-10-10T18:00:55.958",
    "mae":0.3764690459,
    "mse":0.2262853682,
    "hypothesis":"Эта конфигурация исследует пределы емкости модели (e_layers=6, n_heads=16, factor=8) на минимально возможной входной последовательности (seq_len=48, два суточных цикла), которая удовлетворяет ограничениям. Предыдущие эксперименты на этой длине использовали меньшую глубину. Мы максимизируем глубину энкодера (6 слоев) и параллельность внимания (16 голов), чтобы максимально извлечь суточные и краткосрочные паттерны. Использование высокого фактора дистилляции (factor=8) призвано агрессивно сжимать информацию, а минимальный label_len=24 заставляет модель полагаться исключительно на признаки, извлеченные этим высокоемким энкодером, проверяя, сможет ли сложность модели компенсировать недостаток истории.",
    "seq_len":48,
    "label_len":24,
    "e_layers":6,
    "n_heads":16,
    "factor":8
  },
  {
    "trial":31,
    "timestamp":"2025-10-10T18:03:15.505",
    "mae":0.243720293,
    "mse":0.1033124402,
    "hypothesis":"Testing the application of the 'minimal depth, maximal parallelization\/distillation' strategy, which proved successful on long sequences, to the shortest possible input sequence (48 hours). By using minimal depth (2 layers) we aim to prevent over-fitting, while maximizing attention heads (16) and distillation factor (10) should aggressively extract crucial short-term (daily) cyclical patterns from the minimal input window, relying on the efficiency of distillation rather than depth. This contrasts with previous high-complexity attempts on seq_len=48 which used higher e_layers (4 or 6).",
    "seq_len":48,
    "label_len":24,
    "e_layers":2,
    "n_heads":16,
    "factor":10
  },
  {
    "trial":32,
    "timestamp":"2025-10-10T18:07:22.414",
    "mae":0.5090366602,
    "mse":0.3639077842,
    "hypothesis":"This configuration tests the combination of a weekly history (168 hours) with a very long stabilization context (label_len=96, 4 days), a key factor in the top-performing configurations (seq_len=336\/720). We increase the encoder depth to 4 layers (higher than the previously successful minimal depth of 2) and use a high distillation factor (8) to ensure the model has sufficient capacity to capture and aggressively distill complex weekly seasonal patterns. We rely on the long label context to stabilize the prediction despite the increased encoder complexity, using a standard number of heads (8) to focus on deeper processing rather than maximal parallelism.",
    "seq_len":168,
    "label_len":96,
    "e_layers":4,
    "n_heads":8,
    "factor":8
  },
  {
    "trial":33,
    "timestamp":"2025-10-10T18:10:10.172",
    "mae":0.4008560181,
    "mse":0.230007872,
    "hypothesis":"Эта конфигурация направлена на улучшение результатов на умеренно длинной последовательности (96 часов, 4 дня), которая традиционно показывала более низкую эффективность, чем экстремально длинные или короткие окна. Мы применяем успешную стратегию минимальной глубины энкодера (e_layers=2) для предотвращения переобучения на сезонных паттернах. Комбинация максимальной параллельности (n_heads=16) и максимального фактора дистилляции (factor=10) призвана агрессивно извлечь и сжать ключевые суточные и многодневные паттерны. Используя длинный контекст инициализации (label_len=48), мы стабилизируем декодер, проверяя, сможет ли этот баланс между агрессивным сжатием и стабилизацией контекста превзойти предыдущие результаты на seq_len=96, которые полагались на высокую сложность энкодера (e_layers=6).",
    "seq_len":96,
    "label_len":48,
    "e_layers":2,
    "n_heads":16,
    "factor":10
  },
  {
    "trial":34,
    "timestamp":"2025-10-10T18:19:09.390",
    "mae":0.3003835976,
    "mse":0.1537583768,
    "hypothesis":"Эта конфигурация aims to leverage the successful strategy of minimal encoder depth (e_layers=2) combined with maximal parallelization (n_heads=16) to effectively process the longest possible input sequence (720 часов). Мы используем длинный контекст инициализации декодера (label_len=168), который доказал свою эффективность в стабилизации прогноза на длинных последовательностях (MSE 0.2295). Установка высокого фактора дистилляции (factor=8) позволит агрессивно сжать 30-дневную историю, надеясь, что минимальная глубина предотвратит переобучение, а агрессивное сжатие выделит ключевые долгосрочные (месячные) паттерны, что может превзойти предыдущий лучший результат, полученный с более низким фактором дистилляции (factor=5) на схожей конфигурации.",
    "seq_len":720,
    "label_len":168,
    "e_layers":2,
    "n_heads":16,
    "factor":8
  }
]