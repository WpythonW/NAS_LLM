[
  {
    "trial":1,
    "timestamp":"2025-10-10T15:31:25.762",
    "mae":0.734349072,
    "mse":0.6608821154,
    "hypothesis":"Эта конфигурация использует умеренно длинный входной горизонт (96ч) и глубокий энкодер (4 слоя), чтобы максимизировать извлечение сложных временных зависимостей. Высокий фактор (10) применяется для обеспечения наименьшей потери информации при разреженном внимании.",
    "seq_len":96,
    "label_len":48,
    "e_layers":4,
    "n_heads":8,
    "factor":10
  },
  {
    "trial":2,
    "timestamp":"2025-10-10T15:34:11.688",
    "mae":0.6108169556,
    "mse":0.4779782295,
    "hypothesis":"Конфигурация нацелена на использование длинного контекста (168ч, неделя) с минимальной глубиной энкодера (2 слоя) для повышения скорости обучения. Увеличенное количество голов (16) и низкий фактор (3) призваны проверить, может ли модель эффективно обрабатывать недельные паттерны, компенсируя неглубокую архитектуру широким вниманием и агрессивной разреженностью.",
    "seq_len":168,
    "label_len":24,
    "e_layers":2,
    "n_heads":16,
    "factor":3
  },
  {
    "trial":3,
    "timestamp":"2025-10-10T15:47:36.868",
    "mae":0.3985698521,
    "mse":0.2305030376,
    "hypothesis":"Utilizing a very long input sequence (336ч) aims to capture bi-weekly patterns inherent in the data. Мы используем среднюю глубину энкодера (3 слоя) и умеренный фактор разреженности (5), чтобы сбалансировать сложность модели и эффективность вычислений, проверяя, требуется ли для такого длинного контекста более консервативная разреженность, чем в лучшем историческом эксперименте.",
    "seq_len":336,
    "label_len":96,
    "e_layers":3,
    "n_heads":8,
    "factor":5
  },
  {
    "trial":4,
    "timestamp":"2025-10-10T15:53:25.990",
    "mae":0.5937220454,
    "mse":0.5171122551,
    "hypothesis":"This configuration tests whether a very deep model (6 слоев) can compensate for a short input sequence (48ч), focusing intensely on extracting short-range dependencies. Увеличенное количество голов (16) и высокий фактор (8) используются для обеспечения детального, но при этом широкого анализа локального контекста.",
    "seq_len":48,
    "label_len":24,
    "e_layers":6,
    "n_heads":16,
    "factor":8
  },
  {
    "trial":5,
    "timestamp":"2025-10-10T16:09:22.376",
    "mae":0.5163324475,
    "mse":0.3718755245,
    "hypothesis":"We test if utilizing the maximum available context (720ч, месяц) captures essential long-term periodicities, which proved crucial in the best historical experiment (336ч). Мы используем увеличенную глубину энкодера (4 слоя) и большее число голов (16) для эффективной обработки и агрегации информации из этого обширного временного окна, а также более высокий фактор (8) для консервативной разреженности.",
    "seq_len":720,
    "label_len":168,
    "e_layers":4,
    "n_heads":16,
    "factor":8
  },
  {
    "trial":6,
    "timestamp":"2025-10-10T16:15:59.278",
    "mae":0.667550981,
    "mse":0.5676951408,
    "hypothesis":"This configuration explores if an extremely deep architecture (6 слоев) can maximize feature extraction from a moderate weekly context (168ч), compensating for less context. Мы используем агрессивно низкий фактор разреженности (3) для ускорения вычислений и проверки, способна ли модель поддерживать высокую точность, полагаясь на глубокую обработку, а не на широкий выбор Q-точек.",
    "seq_len":168,
    "label_len":48,
    "e_layers":6,
    "n_heads":8,
    "factor":3
  },
  {
    "trial":7,
    "timestamp":"2025-10-10T16:30:43.787",
    "mae":0.3836722374,
    "mse":0.2291098833,
    "hypothesis":"By maximizing the input sequence length (720ч) and the architectural complexity (6 encoder layers, 16 heads), we aim to capture monthly periodicities and complex dependencies more effectively than previous deep models. We use a moderate factor (5) to balance information retrieval from the extensive context and computational efficiency.",
    "seq_len":720,
    "label_len":336,
    "e_layers":6,
    "n_heads":16,
    "factor":5
  },
  {
    "trial":8,
    "timestamp":"2025-10-10T16:34:37.617",
    "mae":0.4125988185,
    "mse":0.3302236497,
    "hypothesis":"We test if a shallow model (2 layers) can effectively process a moderate context (96ч) when compensated by a high number of attention heads (16) and the maximum retention factor (10). This setup minimizes computational depth while maximizing the breadth and information density of the sparse attention mechanism.",
    "seq_len":96,
    "label_len":24,
    "e_layers":2,
    "n_heads":16,
    "factor":10
  },
  {
    "trial":9,
    "timestamp":"2025-10-10T16:41:33.783",
    "mae":0.3935006261,
    "mse":0.2383144796,
    "hypothesis":"Мы проверяем, как максимальное удержание информации (factor=10) влияет на очень длинный контекст (336ч) при умеренной глубине энкодера (4 слоя). Это противопоставляется лучшим результатам, где использовался фактор 5, и проверяет, является ли более консервативная разреженность ключом к улучшению извлечения двукратных недельных паттернов.",
    "seq_len":336,
    "label_len":168,
    "e_layers":4,
    "n_heads":8,
    "factor":10
  },
  {
    "trial":10,
    "timestamp":"2025-10-10T16:48:42.065",
    "mae":0.7203394771,
    "mse":0.6525477171,
    "hypothesis":"Эта конфигурация максимизирует архитектурную сложность (6 слоев, 16 голов) для извлечения недельных закономерностей (168ч), одновременно используя самый агрессивный фактор разреженности (3). Мы проверяем, способна ли глубокая и широкая обработка компенсировать минимальное количество выбранных Q-точек для повышения вычислительной эффективности при сохранении точности.",
    "seq_len":168,
    "label_len":96,
    "e_layers":6,
    "n_heads":16,
    "factor":3
  },
  {
    "trial":11,
    "timestamp":"2025-10-10T16:56:09.430",
    "mae":0.6185562611,
    "mse":0.5063114762,
    "hypothesis":"Мы тестируем, может ли глубокая архитектура (6 слоев) эффективно обработать недельный контекст (168ч) и достичь топовых результатов, если использовать максимальное количество голов (16) и самый консервативный фактор разреженности (10) для минимизации потери информации.",
    "seq_len":168,
    "label_len":48,
    "e_layers":6,
    "n_heads":16,
    "factor":10
  },
  {
    "trial":12,
    "timestamp":"2025-10-10T17:04:31.281",
    "mae":0.3191345632,
    "mse":0.1758615971,
    "hypothesis":"Эта конфигурация проверяет, является ли максимальная длина входной последовательности (720ч) достаточной для высокой точности, даже при использовании самой мелкой архитектуры энкодера (2 слоя). Мы используем максимальное количество голов (16) и наиболее агрессивный фактор разреженности (3) для достижения высокой вычислительной эффективности.",
    "seq_len":720,
    "label_len":48,
    "e_layers":2,
    "n_heads":16,
    "factor":3
  },
  {
    "trial":13,
    "timestamp":"2025-10-10T17:13:56.842",
    "mae":0.4169357419,
    "mse":0.2725648284,
    "hypothesis":"Мы исследуем, улучшит ли комбинация максимальной длины контекста (720ч) со слегка более глубоким энкодером (3 слоя) и более консервативным фактором разреженности (8) результаты по сравнению с лучшим экспериментом, который использовал минимальную глубину (2 слоя) и агрессивную разреженность. Цель — сбалансировать обширную агрегацию контекста с более высокой емкостью извлечения признаков.",
    "seq_len":720,
    "label_len":96,
    "e_layers":3,
    "n_heads":16,
    "factor":8
  },
  {
    "trial":14,
    "timestamp":"2025-10-10T17:20:52.856",
    "mae":0.4885829985,
    "mse":0.3387320638,
    "hypothesis":"Проверяем, является ли максимальная глубина энкодера (6 слоев) в сочетании с агрессивной разреженностью (factor=3) оптимальной стратегией для обработки двухнедельного контекста (336ч). Эта конфигурация исследует, может ли глубокая и интенсивная обработка компенсировать агрессивный фактор для повышения вычислительной эффективности при сохранении высокой точности.",
    "seq_len":336,
    "label_len":48,
    "e_layers":6,
    "n_heads":8,
    "factor":3
  },
  {
    "trial":15,
    "timestamp":"2025-10-10T17:28:55.891",
    "mae":0.4027498662,
    "mse":0.2508167624,
    "hypothesis":"We combine the proven benefit of maximal context (720ч) and aggressive sparsity (factor=3) from the best experiment, but increase the encoder depth to 3 layers to allow for slightly more complex feature refinement. We reduce the number of attention heads (8) to test if the increased depth compensates for the reduced breadth of attention in this highly efficient setup.",
    "seq_len":720,
    "label_len":96,
    "e_layers":3,
    "n_heads":8,
    "factor":3
  },
  {
    "trial":16,
    "timestamp":"2025-10-10T17:33:04.402",
    "mae":0.5457980037,
    "mse":0.4128620923,
    "hypothesis":"This configuration tests the limit of architectural complexity (6 layers, 16 heads) to extract maximum information from a relatively short input sequence (96ч). We use the maximum conservation factor (10) to minimize information loss in the sparse attention mechanism, aiming to compensate for the limited context length with intense processing.",
    "seq_len":96,
    "label_len":24,
    "e_layers":6,
    "n_heads":16,
    "factor":10
  },
  {
    "trial":17,
    "timestamp":"2025-10-10T17:40:53.589",
    "mae":0.451706171,
    "mse":0.294002682,
    "hypothesis":"Исследуем, может ли максимальная глубина энкодера (6 слоев) в сочетании с широким вниманием (16 голов) компенсировать использование двухнедельного контекста (336ч) вместо месячного. Мы используем консервативный фактор (8) для снижения потерь информации при разреженности.",
    "seq_len":336,
    "label_len":48,
    "e_layers":6,
    "n_heads":16,
    "factor":8
  },
  {
    "trial":18,
    "timestamp":"2025-10-10T17:43:02.162",
    "mae":0.6258692741,
    "mse":0.4955691695,
    "hypothesis":"Тестируем, может ли самая простая архитектура энкодера (2 слоя, 8 голов) быть эффективной для обработки минимального контекста (48ч). Мы используем умеренный фактор разреженности (5) для баланса между скоростью и захватом локальных зависимостей.",
    "seq_len":48,
    "label_len":24,
    "e_layers":2,
    "n_heads":8,
    "factor":5
  },
  {
    "trial":19,
    "timestamp":"2025-10-10T17:51:03.224",
    "mae":0.466235429,
    "mse":0.3060426414,
    "hypothesis":"We explore if combining a bi-weekly context (336ч) with a moderately deep encoder (4 слоев) and a conservative sparsity factor (8) can achieve high performance. This tests whether deeper processing compensates for reduced context length compared to the best performing monthly context models.",
    "seq_len":336,
    "label_len":24,
    "e_layers":4,
    "n_heads":16,
    "factor":8
  },
  {
    "trial":20,
    "timestamp":"2025-10-10T17:55:51.966",
    "mae":0.5771762133,
    "mse":0.4440786839,
    "hypothesis":"We test the hypothesis that intense, highly conserved processing of minimal context (48ч, factor=10) can extract critical local dependencies. This uses maximal encoder depth (6 слоев) and contrasts with previous short-sequence failures by maximizing information retention and prioritizing depth over breadth (8 голов).",
    "seq_len":48,
    "label_len":24,
    "e_layers":6,
    "n_heads":8,
    "factor":10
  },
  {
    "trial":21,
    "timestamp":"2025-10-10T18:05:28.788",
    "mae":0.3742436767,
    "mse":0.2299329489,
    "hypothesis":"Тестирование влияния более глубокого, но сбалансированного энкодера (4 слоя) и умеренного фактора разреженности (5) на максимальную длину контекста (720ч). Это проверяет, требуется ли для улучшения результатов больше емкости для извлечения признаков и менее агрессивная разреженность, чем в топовом эксперименте.",
    "seq_len":720,
    "label_len":168,
    "e_layers":4,
    "n_heads":8,
    "factor":5
  },
  {
    "trial":22,
    "timestamp":"2025-10-10T18:11:10.054",
    "mae":0.6939046979,
    "mse":0.6326545477,
    "hypothesis":"Исследование того, может ли умеренно глубокая архитектура (4 слоя) с максимальной широтой внимания (16 голов) и консервативным фактором разреженности (8) эффективно использовать недельный контекст (168ч). Это противопоставляется плохим результатам предыдущих экспериментов на 168ч, которые часто использовали крайние значения глубины и фактора.",
    "seq_len":168,
    "label_len":24,
    "e_layers":4,
    "n_heads":16,
    "factor":8
  },
  {
    "trial":23,
    "timestamp":"2025-10-10T18:16:36.000",
    "mae":0.3354277015,
    "mse":0.1797967106,
    "hypothesis":"We test if a minimal encoder depth (2 layers) can effectively process the two-week context (336ч), provided we maximize attention breadth (16 heads) and use the maximum information retention factor (10). This setup contrasts the high-efficiency\/aggressive-sparsity approach of the best model (factor=3) by prioritizing detailed, but shallow, processing.",
    "seq_len":336,
    "label_len":96,
    "e_layers":2,
    "n_heads":16,
    "factor":10
  }
]