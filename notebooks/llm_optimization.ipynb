{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#%cd .. \n#!rm -rf NAS_LLM\n!git clone -b features https://github.com/WpythonW/NAS_LLM.git\n%cd NAS_LLM/\n!uv pip install --system -q -r pyproject.toml","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-29T21:37:07.384639Z","iopub.execute_input":"2025-10-29T21:37:07.384923Z","iopub.status.idle":"2025-10-29T21:37:52.905835Z","shell.execute_reply.started":"2025-10-29T21:37:07.384900Z","shell.execute_reply":"2025-10-29T21:37:52.905070Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'NAS_LLM'...\nremote: Enumerating objects: 221, done.\u001b[K\nremote: Counting objects: 100% (46/46), done.\u001b[K\nremote: Compressing objects: 100% (37/37), done.\u001b[K\nremote: Total 221 (delta 16), reused 30 (delta 9), pack-reused 175 (from 1)\u001b[K\nReceiving objects: 100% (221/221), 2.32 MiB | 13.13 MiB/s, done.\nResolving deltas: 100% (100/100), done.\n/kaggle/working/NAS_LLM\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%cd NAS_LLM/\n!cat llm_opt_toolkit/llm_requester.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T17:41:49.832017Z","iopub.execute_input":"2025-11-17T17:41:49.832714Z","iopub.status.idle":"2025-11-17T17:41:49.970284Z","shell.execute_reply.started":"2025-11-17T17:41:49.832679Z","shell.execute_reply":"2025-11-17T17:41:49.969621Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/NAS_LLM\nfrom google import genai\nfrom google.genai import types\nfrom config import GEMINI_API_KEY\nfrom pydantic import BaseModel\n\nfrom google import genai\nfrom google.genai import types\nfrom google.genai.errors import ServerError\nfrom pydantic import BaseModel\nimport time\n\ndef call_llm(prompt: str, ListConfigs: BaseModel, temperature: float = 1, thinking_budget: int = 2048, max_retries: int = 100):\n    client = genai.Client(api_key=GEMINI_API_KEY)\n    \n    config = types.GenerateContentConfig(\n        system_instruction=\"You are hyperparameter optimizer. Strictly follow rules in prompt.\",\n        response_mime_type=\"application/json\",\n        response_schema=ListConfigs,\n        temperature=temperature,\n        thinking_config=types.ThinkingConfig(thinking_budget=thinking_budget)\n    )\n\n    for attempt in range(max_retries):\n        try:\n            print('Attempting to call LLM, try', attempt + 1)\n            response = client.models.generate_content(\n                model=\"gemini-flash-latest\",\n                contents=prompt,\n                config=config\n            )\n            return response.parsed.list_of_configs\n        \n        except ServerError as e: time.sleep(2)\n            #raise Exception('WTF model didn-t respond??')","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T18:01:18.496430Z","iopub.execute_input":"2025-11-17T18:01:18.496754Z","iopub.status.idle":"2025-11-17T18:01:18.613016Z","shell.execute_reply.started":"2025-11-17T18:01:18.496732Z","shell.execute_reply":"2025-11-17T18:01:18.612114Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/NAS_LLM\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"%%writefile llm_opt_toolkit/optimization_journal.py\n\nimport os\nimport json\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\nfrom config import JOURNAL_DIR\n\nclass Journal:\n    def __init__(self, filename=None):\n        os.makedirs(JOURNAL_DIR, exist_ok=True)\n        \n        if filename is None:\n            filename = f'experiments_pred_{datetime.datetime.now()}.json'\n        \n        self.filepath = os.path.join(JOURNAL_DIR, filename)\n        self.entries = self._load()\n    \n    def _load(self):\n        if not os.path.exists(self.filepath):\n            return []\n        \n        with open(self.filepath, 'r') as f:\n            content = f.read().strip()\n            if not content:\n                return []\n            return json.loads(content)\n    \n    def _save(self):\n        with open(self.filepath, 'w') as f:\n            json.dump(self.entries, f, indent=2, ensure_ascii=False)\n    \n    def add(self, config, measures, trial: int):\n        if config.label_len >= config.seq_len:\n            raise ValueError(f\"label_len >= seq_len: {config.label_len} >= {config.seq_len}\")\n        \n        entry = {\n            'trial': int(trial),\n            'timestamp': datetime.now().isoformat(),\n            **{k: float(v) for k, v in measures.items()},\n            **{k: int(v) if isinstance(v, (np.integer, np.int64)) else v \n               for k, v in config.__dict__.items()}\n        }\n        self.entries.append(entry)\n        self._save()\n    \n    def get_history_table(self, last_n=1500) -> str:\n        if not self.entries:\n            return \"\"\n        \n        df = pd.DataFrame(self.entries[-last_n:]).drop(columns=['trial', 'timestamp'])\n        \n        top3_mse = df.nsmallest(4, 'mse_val')\n        #top3_mae = df.nsmallest(3, 'mae')\n        worst3_mse = df.nlargest(4, 'mse_val')\n        #worst3_mae = df.nlargest(3, 'mae')\n        \n        tables = [\n            \"\\n### История последних экспериментов\",\n            df.to_markdown(index=False, floatfmt=\".4f\"),\n            \"\\n### Топ 3 по MSE\",\n            top3_mse.to_markdown(index=False, floatfmt=\".4f\"),\n            #\"\\n### Топ 3 по MAE\",\n            #top3_mae.to_markdown(index=False, floatfmt=\".4f\"),\n            \"\\n### Худшие 3 по MSE\",\n            worst3_mse.to_markdown(index=False, floatfmt=\".4f\"),\n            #\"\\n### Худшие 3 по MAE\",\n            #worst3_mae.to_markdown(index=False, floatfmt=\".4f\")\n        ]\n        \n        return \"\\n\".join(tables) + \"\\n\"\n    \n    def count_trials(self) -> int:\n        return len(self.entries)\n    \n    def get_best(self):\n        if not self.entries:\n            return None\n        df = pd.DataFrame(self.entries)\n        return df.loc[df['mse_test'].idxmin()]\n    \n    def print_best(self):\n        best = self.get_best()\n        if best is None:\n            return\n        \n        print(f\"\\n>>>MSE={best['mse_test']:.4f} MAE={best['mae_test']:.4f} \"\n              f\"seq={int(best['seq_len'])} lbl={int(best['label_len'])} e={int(best['e_layers'])} \"\n              f\"h={int(best['n_heads'])} f={int(best['factor'])}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T18:01:35.872255Z","iopub.execute_input":"2025-11-17T18:01:35.872532Z","iopub.status.idle":"2025-11-17T18:01:35.879450Z","shell.execute_reply.started":"2025-11-17T18:01:35.872507Z","shell.execute_reply":"2025-11-17T18:01:35.878744Z"}},"outputs":[{"name":"stdout","text":"Overwriting llm_opt_toolkit/optimization_journal.py\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"%%writefile experiments/ETTh1-grid3_pred_len=168;th_b=8192;hypot.json\n[\n  {\n    \"trial\": 1,\n    \"timestamp\": \"2025-11-05T14:13:33.386135\",\n    \"mae_val\": 0.8383050560951233,\n    \"mse_val\": 0.891139805316925,\n    \"mae_test\": 0.49326202273368835,\n    \"mse_test\": 0.3694080710411072,\n    \"seq_len\": 96,\n    \"label_len\": 48,\n    \"e_layers\": 3,\n    \"n_heads\": 8,\n    \"factor\": 5,\n    \"d_model\": 512,\n    \"d_ff\": 2048,\n    \"d_layers\": 1,\n    \"dropout\": 0.1,\n    \"hypothesis\": \"Эта конфигурация использует среднюю длину входной последовательности (96 часов) и умеренную размерность модели, что подходит для захвата краткосрочных и среднесрочных зависимостей в часовых данных. Неглубокий декодер способствует быстрой генерации прогнозов, используя эффективно извлеченные признаки.\"\n  },\n  {\n    \"trial\": 2,\n    \"timestamp\": \"2025-11-05T14:20:09.188635\",\n    \"mae_val\": 0.48567381501197815,\n    \"mse_val\": 0.3630131185054779,\n    \"mae_test\": 0.3964079022407532,\n    \"mse_test\": 0.23787617683410645,\n    \"seq_len\": 336,\n    \"label_len\": 96,\n    \"e_layers\": 4,\n    \"n_heads\": 16,\n    \"factor\": 3,\n    \"d_model\": 768,\n    \"d_ff\": 3072,\n    \"d_layers\": 2,\n    \"dropout\": 0.2,\n    \"hypothesis\": \"Данный крупный сетап нацелен на анализ недельных сезонных паттернов (336 часов ввода) с высокой сложностью модели (d_model=768, 4 слоя энкодера, 2 слоя декодера). Повышенный уровень dropout (0.2) используется для предотвращения переобучения при работе с длинными последовательностями и большим количеством параметров.\"\n  },\n  {\n    \"trial\": 3,\n    \"timestamp\": \"2025-11-05T14:30:58.209652\",\n    \"mae_val\": 0.3431564271450043,\n    \"mse_val\": 0.1862528920173645,\n    \"mae_test\": 0.3331473171710968,\n    \"mse_test\": 0.17399954795837402,\n    \"seq_len\": 720,\n    \"label_len\": 168,\n    \"e_layers\": 6,\n    \"n_heads\": 16,\n    \"factor\": 8,\n    \"d_model\": 512,\n    \"d_ff\": 2048,\n    \"d_layers\": 3,\n    \"dropout\": 0.1,\n    \"hypothesis\": \"Эта конфигурация предназначена для захвата долгосрочных, месячных зависимостей (720 часов ввода), используя максимальную глубину (6 слоев энкодера) и высокую емкость внимания. Умеренный d_model и dropout используются для балансировки сложности и предотвращения переобучения при работе с очень длинными последовательностями.\"\n  },\n  {\n    \"trial\": 4,\n    \"timestamp\": \"2025-11-05T14:31:33.908912\",\n    \"mae_val\": 0.8040050864219666,\n    \"mse_val\": 0.7671905755996704,\n    \"mae_test\": 0.44883689284324646,\n    \"mse_test\": 0.29831990599632263,\n    \"seq_len\": 48,\n    \"label_len\": 24,\n    \"e_layers\": 2,\n    \"n_heads\": 8,\n    \"factor\": 10,\n    \"d_model\": 256,\n    \"d_ff\": 1024,\n    \"d_layers\": 1,\n    \"dropout\": 0.05,\n    \"hypothesis\": \"Данный минималистичный сетап ориентирован на краткосрочное прогнозирование (48 часов ввода) и эффективность, используя наименьшую размерность модели (d_model=256) и минимальное количество слоев. Низкий dropout выбран для максимальной чувствительности к недавним паттернам при минимизации вычислительных затрат.\"\n  },\n  {\n    \"trial\": 5,\n    \"timestamp\": \"2025-11-05T14:34:56.951254\",\n    \"mae_val\": 0.8682805895805359,\n    \"mse_val\": 0.930160403251648,\n    \"mae_test\": 0.46394723653793335,\n    \"mse_test\": 0.3237917721271515,\n    \"seq_len\": 168,\n    \"label_len\": 48,\n    \"e_layers\": 4,\n    \"n_heads\": 8,\n    \"factor\": 5,\n    \"d_model\": 512,\n    \"d_ff\": 3072,\n    \"d_layers\": 2,\n    \"dropout\": 0.1,\n    \"hypothesis\": \"Эта конфигурация балансирует недельную длину входной последовательности (168 часов) с увеличенной сложностью модели (4 слоя энкодера, большой d_ff=3072) для эффективного захвата недельных паттернов. Умеренный dropout и глубина декодера (2) выбраны для стабильного обучения и хорошей генерализации.\"\n  },\n  {\n    \"trial\": 6,\n    \"timestamp\": \"2025-11-05T15:33:04.688766\",\n    \"mae_val\": 0.367705374956131,\n    \"mse_val\": 0.22592321038246155,\n    \"mae_test\": 0.3315505385398865,\n    \"mse_test\": 0.16558507084846497,\n    \"seq_len\": 720,\n    \"label_len\": 336,\n    \"e_layers\": 3,\n    \"n_heads\": 16,\n    \"factor\": 10,\n    \"d_model\": 768,\n    \"d_ff\": 1024,\n    \"d_layers\": 3,\n    \"dropout\": 0.2,\n    \"hypothesis\": \"Конфигурация нацелена на анализ очень долгосрочных зависимостей (720 часов) с максимальной размерностью модели (d_model=768) и высоким уровнем регуляризации (dropout=0.2). Используется максимальная длина стартового токена (336) и максимальный фактор (10) для проверки устойчивости при высокой сложности.\"\n  },\n  {\n    \"trial\": 7,\n    \"timestamp\": \"2025-11-05T15:38:33.385166\",\n    \"mae_val\": 0.6993342638015747,\n    \"mse_val\": 0.6214092969894409,\n    \"mae_test\": 0.40045973658561707,\n    \"mse_test\": 0.25122174620628357,\n    \"seq_len\": 168,\n    \"label_len\": 96,\n    \"e_layers\": 6,\n    \"n_heads\": 16,\n    \"factor\": 5,\n    \"d_model\": 768,\n    \"d_ff\": 2048,\n    \"d_layers\": 2,\n    \"dropout\": 0.05,\n    \"hypothesis\": \"Эта конфигурация использует максимальную глубину энкодера (6 слоев) и высокую размерность модели (768), чтобы максимально эффективно извлечь недельные и более краткосрочные паттерны из входной последовательности длиной 168 часов. Низкий dropout (0.05) выбран для минимизации потерь информации при обучении высокоемкой модели.\"\n  },\n  {\n    \"trial\": 8,\n    \"timestamp\": \"2025-11-05T15:41:43.726551\",\n    \"mae_val\": 0.828657865524292,\n    \"mse_val\": 0.8752637505531311,\n    \"mae_test\": 0.48146140575408936,\n    \"mse_test\": 0.3351838290691376,\n    \"seq_len\": 48,\n    \"label_len\": 24,\n    \"e_layers\": 4,\n    \"n_heads\": 16,\n    \"factor\": 3,\n    \"d_model\": 512,\n    \"d_ff\": 3072,\n    \"d_layers\": 3,\n    \"dropout\": 0.1,\n    \"hypothesis\": \"Конфигурация фокусируется на краткосрочном прогнозировании (48 часов ввода) с использованием повышенной сложности (глубокий энкодер/декодер, 16 голов, большой d_ff), чтобы проверить, способна ли высокая емкость модели улучшить результаты для очень коротких временных рядов. Минимальный фактор (3) и умеренный dropout (0.1) используются для точного захвата немедленных зависимостей.\"\n  },\n  {\n    \"trial\": 9,\n    \"timestamp\": \"2025-11-05T15:42:34.046940\",\n    \"mae_val\": 0.823093593120575,\n    \"mse_val\": 0.8192238807678223,\n    \"mae_test\": 0.3975936472415924,\n    \"mse_test\": 0.2491505891084671,\n    \"seq_len\": 96,\n    \"label_len\": 24,\n    \"e_layers\": 2,\n    \"n_heads\": 8,\n    \"factor\": 8,\n    \"d_model\": 256,\n    \"d_ff\": 2048,\n    \"d_layers\": 1,\n    \"dropout\": 0.1,\n    \"hypothesis\": \"Эта конфигурация тестирует способность минималистичной модели (d_model=256, 2 слоя энкодера, 1 слой декодера) эффективно извлекать зависимости из входной последовательности средней длины (96 часов). Использование относительно большого d_ff должно компенсировать малый размер d_model.\"\n  },\n  {\n    \"trial\": 10,\n    \"timestamp\": \"2025-11-05T15:54:57.466995\",\n    \"mae_val\": 0.41521313786506653,\n    \"mse_val\": 0.25993233919143677,\n    \"mae_test\": 0.3552573025226593,\n    \"mse_test\": 0.18821638822555542,\n    \"seq_len\": 720,\n    \"label_len\": 48,\n    \"e_layers\": 6,\n    \"n_heads\": 16,\n    \"factor\": 3,\n    \"d_model\": 768,\n    \"d_ff\": 3072,\n    \"d_layers\": 3,\n    \"dropout\": 0.2,\n    \"hypothesis\": \"Эта максимальная по емкости модель (6 слоев, d_model=768, d_ff=3072) предназначена для агрессивного извлечения долгосрочных зависимостей из 720-часовой последовательности. Использование минимального фактора (3) и высокого dropout (0.2) проверяет эффективность плотного внимания при сильной регуляризации.\"\n  },\n  {\n    \"trial\": 11,\n    \"timestamp\": \"2025-11-05T15:58:42.406090\",\n    \"mae_val\": 0.5020892024040222,\n    \"mse_val\": 0.35555213689804077,\n    \"mae_test\": 0.37847641110420227,\n    \"mse_test\": 0.2187207192182541,\n    \"seq_len\": 336,\n    \"label_len\": 168,\n    \"e_layers\": 4,\n    \"n_heads\": 8,\n    \"factor\": 10,\n    \"d_model\": 512,\n    \"d_ff\": 1024,\n    \"d_layers\": 2,\n    \"dropout\": 0.1,\n    \"hypothesis\": \"Эта конфигурация предназначена для анализа недельных/двухнедельных паттернов (336 часов) с использованием максимального фактора разреженного внимания (10) и умеренного размера модели, чтобы проверить эффективность экстракции длинных, но разреженных зависимостей.\"\n  },\n  {\n    \"trial\": 12,\n    \"timestamp\": \"2025-11-05T16:02:30.766910\",\n    \"mae_val\": 0.7222349047660828,\n    \"mse_val\": 0.6402153372764587,\n    \"mae_test\": 0.3664668798446655,\n    \"mse_test\": 0.20866729319095612,\n    \"seq_len\": 96,\n    \"label_len\": 48,\n    \"e_layers\": 6,\n    \"n_heads\": 16,\n    \"factor\": 5,\n    \"d_model\": 768,\n    \"d_ff\": 3072,\n    \"d_layers\": 3,\n    \"dropout\": 0.05,\n    \"hypothesis\": \"Данный ультра-сложный сетап для средней длины ввода (96 часов) тестирует, может ли максимальная емкость модели (глубокий энкодер, большой d_model) компенсировать короткую историю, используя низкий dropout для агрессивного обучения.\"\n  },\n  {\n    \"trial\": 13,\n    \"timestamp\": \"2025-11-05T16:06:43.014856\",\n    \"mae_val\": 0.5343169569969177,\n    \"mse_val\": 0.37626540660858154,\n    \"mae_test\": 0.3367091715335846,\n    \"mse_test\": 0.16934828460216522,\n    \"seq_len\": 336,\n    \"label_len\": 48,\n    \"e_layers\": 4,\n    \"n_heads\": 16,\n    \"factor\": 8,\n    \"d_model\": 768,\n    \"d_ff\": 2048,\n    \"d_layers\": 2,\n    \"dropout\": 0.05,\n    \"hypothesis\": \"Эта конфигурация использует 2-недельную историю (336 часов) в сочетании с высокой емкостью модели (d_model=768, 16 голов) и низким уровнем регуляризации для агрессивного обучения и точного извлечения среднесрочных сезонных паттернов.\"\n  },\n  {\n    \"trial\": 14,\n    \"timestamp\": \"2025-11-05T16:09:02.444269\",\n    \"mae_val\": 0.6989176273345947,\n    \"mse_val\": 0.6344751715660095,\n    \"mae_test\": 0.3482370376586914,\n    \"mse_test\": 0.18601401150226593,\n    \"seq_len\": 96,\n    \"label_len\": 24,\n    \"e_layers\": 4,\n    \"n_heads\": 16,\n    \"factor\": 5,\n    \"d_model\": 512,\n    \"d_ff\": 3072,\n    \"d_layers\": 3,\n    \"dropout\": 0.2,\n    \"hypothesis\": \"Эта конфигурация направлена на улучшение краткосрочного прогнозирования (96 часов ввода) за счет использования глубокой архитектуры с максимальной емкостью FFN (3072) и высоким уровнем dropout, чтобы проверить, может ли избыточная сложность модели эффективно извлекать скрытые зависимости из ограниченной истории.\"\n  },\n  {\n    \"trial\": 15,\n    \"timestamp\": \"2025-11-05T16:12:32.010874\",\n    \"mae_val\": 0.5579916834831238,\n    \"mse_val\": 0.40886515378952026,\n    \"mae_test\": 0.3611898720264435,\n    \"mse_test\": 0.19857865571975708,\n    \"seq_len\": 336,\n    \"label_len\": 24,\n    \"e_layers\": 6,\n    \"n_heads\": 8,\n    \"factor\": 3,\n    \"d_model\": 768,\n    \"d_ff\": 1024,\n    \"d_layers\": 1,\n    \"dropout\": 0.1,\n    \"hypothesis\": \"Эта конфигурация использует максимальную глубину энкодера (6) и максимальный d_model (768) для извлечения комплексных двухнедельных паттернов (336 часов), но с минимальной инициализацией декодера (24) и неглубоким декодером (1), чтобы проверить, достаточно ли богатого представления энкодера для точного прогнозирования.\"\n  },\n  {\n    \"trial\": 16,\n    \"timestamp\": \"2025-11-05T16:14:23.934463\",\n    \"mae_val\": 0.8296902179718018,\n    \"mse_val\": 0.9026740789413452,\n    \"mae_test\": 0.5492066740989685,\n    \"mse_test\": 0.4354010224342346,\n    \"seq_len\": 48,\n    \"label_len\": 24,\n    \"e_layers\": 3,\n    \"n_heads\": 16,\n    \"factor\": 8,\n    \"d_model\": 512,\n    \"d_ff\": 2048,\n    \"d_layers\": 2,\n    \"dropout\": 0.2,\n    \"hypothesis\": \"Этот сетап тестирует способность модели средней емкости (d_model=512, 3 слоя энкодера, 2 слоя декодера) эффективно работать с очень коротким входным окном (48 часов), используя высокий dropout (0.2) и большое количество голов внимания (16) для предотвращения переобучения на малом объеме данных.\"\n  },\n  {\n    \"trial\": 17,\n    \"timestamp\": \"2025-11-05T16:18:01.122129\",\n    \"mae_val\": 0.49428603053092957,\n    \"mse_val\": 0.3527902662754059,\n    \"mae_test\": 0.3594265878200531,\n    \"mse_test\": 0.1934441775083542,\n    \"seq_len\": 336,\n    \"label_len\": 96,\n    \"e_layers\": 6,\n    \"n_heads\": 8,\n    \"factor\": 8,\n    \"d_model\": 256,\n    \"d_ff\": 3072,\n    \"d_layers\": 2,\n    \"dropout\": 0.1,\n    \"hypothesis\": \"Эта конфигурация тестирует, может ли максимальная глубина энкодера (6 слоев) компенсировать малую размерность модели (d_model=256) при извлечении двухнедельных паттернов (336 часов). Использование большого d_ff (3072) должно увеличить экспрессивность, а фактор 8 позволяет эффективно работать с разреженным вниманием на длинных последовательностях.\"\n  },\n  {\n    \"trial\": 18,\n    \"timestamp\": \"2025-11-05T16:20:46.902876\",\n    \"mae_val\": 0.9135962128639221,\n    \"mse_val\": 1.0036064386367798,\n    \"mae_test\": 0.4969310164451599,\n    \"mse_test\": 0.37123924493789673,\n    \"seq_len\": 96,\n    \"label_len\": 48,\n    \"e_layers\": 4,\n    \"n_heads\": 16,\n    \"factor\": 10,\n    \"d_model\": 768,\n    \"d_ff\": 2048,\n    \"d_layers\": 1,\n    \"dropout\": 0.05,\n    \"hypothesis\": \"Конфигурация нацелена на агрессивное краткосрочное прогнозирование (96 часов) с использованием максимальной емкости модели (d_model=768) и минимального dropout. При этом используется максимальный фактор разреженности (10) и неглубокий декодер (1 слой), чтобы проверить, достаточно ли агрессивной настройки внимания для короткой истории.\"\n  },\n  {\n    \"trial\": 19,\n    \"timestamp\": \"2025-11-05T16:27:02.037471\",\n    \"mae_val\": 0.8260221481323242,\n    \"mse_val\": 0.8060671091079712,\n    \"mae_test\": 0.4262866675853729,\n    \"mse_test\": 0.2791563868522644,\n    \"seq_len\": 168,\n    \"label_len\": 96,\n    \"e_layers\": 4,\n    \"n_heads\": 16,\n    \"factor\": 5,\n    \"d_model\": 768,\n    \"d_ff\": 2048,\n    \"d_layers\": 3,\n    \"dropout\": 0.1,\n    \"hypothesis\": \"Эта конфигурация тестирует максимальную глубину декодера (3 слоя) в сочетании с высокой размерностью модели (d_model=768) для оптимизации недельного прогнозирования (seq_len=168), что может улучшить качество генерации признаков. Используется умеренное количество слоев энкодера для баланса между сложностью и временем обучения.\"\n  },\n  {\n    \"trial\": 20,\n    \"timestamp\": \"2025-11-05T16:29:28.604257\",\n    \"mae_val\": 0.4351305067539215,\n    \"mse_val\": 0.2840036451816559,\n    \"mae_test\": 0.38352170586586,\n    \"mse_test\": 0.22181740403175354,\n    \"seq_len\": 720,\n    \"label_len\": 48,\n    \"e_layers\": 4,\n    \"n_heads\": 8,\n    \"factor\": 8,\n    \"d_model\": 256,\n    \"d_ff\": 1024,\n    \"d_layers\": 1,\n    \"dropout\": 0.05,\n    \"hypothesis\": \"Данный минималистичный сетап предназначен для проверки, может ли Informer эффективно обрабатывать очень длинные последовательности (720 часов), используя минимальную размерность модели (d_model=256) и неглубокий декодер, полагаясь на высокую разреженность внимания (factor=8) и низкий dropout.\"\n  },\n  {\n    \"trial\": 21,\n    \"timestamp\": \"2025-11-05T16:35:19.537015\",\n    \"mae_val\": 0.9039735198020935,\n    \"mse_val\": 0.9421115517616272,\n    \"mae_test\": 0.509093165397644,\n    \"mse_test\": 0.3956725299358368,\n    \"seq_len\": 168,\n    \"label_len\": 96,\n    \"e_layers\": 6,\n    \"n_heads\": 8,\n    \"factor\": 10,\n    \"d_model\": 768,\n    \"d_ff\": 3072,\n    \"d_layers\": 3,\n    \"dropout\": 0.2,\n    \"hypothesis\": \"Эта конфигурация максимизирует емкость модели (глубина 6/3, d_model=768, d_ff=3072) для агрессивного извлечения сложных недельных паттернов (168 часов). Высокий dropout (0.2) и максимальный фактор разреженности (10) используются для обеспечения устойчивости и эффективной обработки длинных зависимостей.\"\n  },\n  {\n    \"trial\": 22,\n    \"timestamp\": \"2025-11-05T16:38:40.995375\",\n    \"mae_val\": 0.4794762134552002,\n    \"mse_val\": 0.3325682580471039,\n    \"mae_test\": 0.40526166558265686,\n    \"mse_test\": 0.2549435794353485,\n    \"seq_len\": 720,\n    \"label_len\": 96,\n    \"e_layers\": 4,\n    \"n_heads\": 8,\n    \"factor\": 5,\n    \"d_model\": 512,\n    \"d_ff\": 1024,\n    \"d_layers\": 2,\n    \"dropout\": 0.05,\n    \"hypothesis\": \"Эта конфигурация нацелена на эффективное извлечение долгосрочных зависимостей (720 часов) с помощью умеренно глубокой модели (4/2 слоя) и минимальной размерности FFN (1024). Низкий dropout (0.05) и умеренный фактор (5) используются для максимальной чувствительности к паттернам при относительно низкой вычислительной сложности.\"\n  },\n  {\n    \"trial\": 23,\n    \"timestamp\": \"2025-11-05T16:42:25.408874\",\n    \"mae_val\": 0.4821157455444336,\n    \"mse_val\": 0.33508536219596863,\n    \"mae_test\": 0.41619423031806946,\n    \"mse_test\": 0.2735535204410553,\n    \"seq_len\": 720,\n    \"label_len\": 24,\n    \"e_layers\": 3,\n    \"n_heads\": 8,\n    \"factor\": 5,\n    \"d_model\": 256,\n    \"d_ff\": 3072,\n    \"d_layers\": 2,\n    \"dropout\": 0.1,\n    \"hypothesis\": \"Эта конфигурация тестирует эффективность минималистичной по размерности модели (d_model=256) при обработке очень длинной истории (720 часов), компенсируя малый размер d_model высокой емкостью FFN и полагаясь на минимальную инициализацию декодера (label_len=24).\"\n  },\n  {\n    \"trial\": 24,\n    \"timestamp\": \"2025-11-05T16:44:06.194194\",\n    \"mae_val\": 0.7605529427528381,\n    \"mse_val\": 0.7087579369544983,\n    \"mae_test\": 0.40412789583206177,\n    \"mse_test\": 0.26155099272727966,\n    \"seq_len\": 168,\n    \"label_len\": 24,\n    \"e_layers\": 3,\n    \"n_heads\": 8,\n    \"factor\": 3,\n    \"d_model\": 768,\n    \"d_ff\": 2048,\n    \"d_layers\": 1,\n    \"dropout\": 0.2,\n    \"hypothesis\": \"Данный сетап использует максимальную размерность модели (768) и минимальный фактор внимания (3) для плотного захвата недельных паттернов (168 часов). Высокий dropout и неглубокий декодер используются для регуляризации сложного энкодера и ускорения прогнозирования.\"\n  },\n  {\n    \"trial\": 25,\n    \"timestamp\": \"2025-11-05T16:46:52.034838\",\n    \"mae_val\": 0.7909469604492188,\n    \"mse_val\": 0.7946986556053162,\n    \"mae_test\": 0.3898372948169708,\n    \"mse_test\": 0.24791356921195984,\n    \"seq_len\": 168,\n    \"label_len\": 24,\n    \"e_layers\": 4,\n    \"n_heads\": 16,\n    \"factor\": 5,\n    \"d_model\": 512,\n    \"d_ff\": 3072,\n    \"d_layers\": 3,\n    \"dropout\": 0.1,\n    \"hypothesis\": \"Эта конфигурация исследует эффективность глубокого декодера (3 слоя) и высокой емкости FFN (3072) в сочетании с высокой параллельностью внимания (16 голов) для захвата недельных паттернов (168ч) при минимальном стартовом токене.\"\n  },\n  {\n    \"trial\": 26,\n    \"timestamp\": \"2025-11-05T16:52:07.044085\",\n    \"mae_val\": 0.392183780670166,\n    \"mse_val\": 0.24219843745231628,\n    \"mae_test\": 0.3888546824455261,\n    \"mse_test\": 0.23418128490447998,\n    \"seq_len\": 720,\n    \"label_len\": 96,\n    \"e_layers\": 6,\n    \"n_heads\": 16,\n    \"factor\": 10,\n    \"d_model\": 256,\n    \"d_ff\": 2048,\n    \"d_layers\": 3,\n    \"dropout\": 0.2,\n    \"hypothesis\": \"Эта конфигурация проверяет, может ли максимальная глубина (6/3 слоев) и агрессивная регуляризация (0.2) компенсировать минимальный размер модели (d_model=256) при моделировании очень длинных последовательностей (720ч) с максимальной разреженностью внимания.\"\n  },\n  {\n    \"trial\": 27,\n    \"timestamp\": \"2025-11-05T17:02:43.343499\",\n    \"mae_val\": 0.4326220750808716,\n    \"mse_val\": 0.2675260603427887,\n    \"mae_test\": 0.3522462248802185,\n    \"mse_test\": 0.19106750190258026,\n    \"seq_len\": 720,\n    \"label_len\": 96,\n    \"e_layers\": 4,\n    \"n_heads\": 16,\n    \"factor\": 3,\n    \"d_model\": 768,\n    \"d_ff\": 2048,\n    \"d_layers\": 3,\n    \"dropout\": 0.05,\n    \"hypothesis\": \"Данная конфигурация использует максимальную размерность модели и глубокий декодер, обученные на максимальной истории (720ч). Плотное внимание (factor=3) и минимальный dropout (0.05) нацелены на агрессивный захват комплексных долгосрочных зависимостей.\"\n  },\n  {\n    \"trial\": 28,\n    \"timestamp\": \"2025-11-05T17:04:33.482089\",\n    \"mae_val\": 0.5494969487190247,\n    \"mse_val\": 0.40723511576652527,\n    \"mae_test\": 0.35991916060447693,\n    \"mse_test\": 0.1993093192577362,\n    \"seq_len\": 336,\n    \"label_len\": 168,\n    \"e_layers\": 6,\n    \"n_heads\": 8,\n    \"factor\": 5,\n    \"d_model\": 256,\n    \"d_ff\": 3072,\n    \"d_layers\": 1,\n    \"dropout\": 0.2,\n    \"hypothesis\": \"Эта конфигурация исследует, может ли максимальная глубина энкодера (6 слоев) компенсировать минимальный d_model (256) для извлечения двухнедельных паттернов, используя широкий FFN и высокий dropout для регуляризации.\"\n  },\n  {\n    \"trial\": 29,\n    \"timestamp\": \"2025-11-05T17:12:36.090913\",\n    \"mae_val\": 0.36136186122894287,\n    \"mse_val\": 0.20471155643463135,\n    \"mae_test\": 0.32098788022994995,\n    \"mse_test\": 0.15742142498493195,\n    \"seq_len\": 336,\n    \"label_len\": 168,\n    \"e_layers\": 6,\n    \"n_heads\": 16,\n    \"factor\": 3,\n    \"d_model\": 768,\n    \"d_ff\": 3072,\n    \"d_layers\": 2,\n    \"dropout\": 0.1,\n    \"hypothesis\": \"Эта конфигурация максимизирует модельную емкость (d_model=768, e_layers=6) и использует плотное внимание (factor=3), чтобы агрессивно захватить комплексные, высокоточные зависимости на двухнедельном временном горизонте (336 часов).\"\n  },\n  {\n    \"trial\": 30,\n    \"timestamp\": \"2025-11-05T17:19:20.469157\",\n    \"mae_val\": 0.3954492211341858,\n    \"mse_val\": 0.24077993631362915,\n    \"mae_test\": 0.4042089283466339,\n    \"mse_test\": 0.23727548122406006,\n    \"seq_len\": 720,\n    \"label_len\": 336,\n    \"e_layers\": 2,\n    \"n_heads\": 8,\n    \"factor\": 8,\n    \"d_model\": 256,\n    \"d_ff\": 3072,\n    \"d_layers\": 3,\n    \"dropout\": 0.05,\n    \"hypothesis\": \"Данный сетап тестирует способность минималистичного энкодера (d_model=256, e_layers=2) обрабатывать очень длинные последовательности (720 часов), компенсируя малый размер широким FFN (3072), глубоким декодером и минимальным уровнем регуляризации.\"\n  }\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:19:13.744935Z","iopub.execute_input":"2025-11-17T21:19:13.745228Z","iopub.status.idle":"2025-11-17T21:19:13.764115Z","shell.execute_reply.started":"2025-11-17T21:19:13.745204Z","shell.execute_reply":"2025-11-17T21:19:13.763338Z"}},"outputs":[{"name":"stdout","text":"Writing experiments/ETTh1-grid3_pred_len=168;th_b=8192;hypot.json\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport os\nimport subprocess\n\nuser_secrets = UserSecretsClient()\nGEMINI_KEY = user_secrets.get_secret(\"GEMINI_KEY\")\nos.environ['GEMINI_API_KEY'] = GEMINI_KEY\n\npred_len = 168\ngrid_name = \"grid3\"\ndataset = 'ETTh1'\n\nresult = subprocess.run(\n    ['tail', '-n', '25', f\"experiments/{dataset}-{grid_name}_pred_len={pred_len};th_b=8192;hypot.json\"],\n    capture_output=True, text=True\n)\nprint(result.stdout)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:19:29.052768Z","iopub.execute_input":"2025-11-17T21:19:29.053266Z","iopub.status.idle":"2025-11-17T21:19:29.140547Z","shell.execute_reply.started":"2025-11-17T21:19:29.053240Z","shell.execute_reply":"2025-11-17T21:19:29.139952Z"}},"outputs":[{"name":"stdout","text":"    \"d_model\": 768,\n    \"d_ff\": 3072,\n    \"d_layers\": 2,\n    \"dropout\": 0.1,\n    \"hypothesis\": \"Эта конфигурация максимизирует модельную емкость (d_model=768, e_layers=6) и использует плотное внимание (factor=3), чтобы агрессивно захватить комплексные, высокоточные зависимости на двухнедельном временном горизонте (336 часов).\"\n  },\n  {\n    \"trial\": 30,\n    \"timestamp\": \"2025-11-05T17:19:20.469157\",\n    \"mae_val\": 0.3954492211341858,\n    \"mse_val\": 0.24077993631362915,\n    \"mae_test\": 0.4042089283466339,\n    \"mse_test\": 0.23727548122406006,\n    \"seq_len\": 720,\n    \"label_len\": 336,\n    \"e_layers\": 2,\n    \"n_heads\": 8,\n    \"factor\": 8,\n    \"d_model\": 256,\n    \"d_ff\": 3072,\n    \"d_layers\": 3,\n    \"dropout\": 0.05,\n    \"hypothesis\": \"Данный сетап тестирует способность минималистичного энкодера (d_model=256, e_layers=2) обрабатывать очень длинные последовательности (720 часов), компенсируя малый размер широким FFN (3072), глубоким декодером и минимальным уровнем регуляризации.\"\n  }\n]\n\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"subprocess.run([\n    \"python\", \"optimize.py\",\n    \"--dataset\", f\"data/{dataset}_prepared.csv\",\n    \"--pred_len\", str(pred_len),\n    \"--journal\", f\"{dataset}-{grid_name}_pred_len={pred_len};th_b=8192;hypot.json\",\n    \"--n_batches\", \"3\",\n    \"--batch_size\", \"2\",\n    \"--temperature\", \"1\",\n    \"--thinking_budget\", \"8192\",\n    \"--grid_name\", grid_name\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:19:41.143341Z","iopub.execute_input":"2025-11-17T21:19:41.143910Z","iopub.status.idle":"2025-11-17T22:16:36.611503Z","shell.execute_reply.started":"2025-11-17T21:19:41.143886Z","shell.execute_reply":"2025-11-17T22:16:36.610708Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nPRED_LEN=168ч | 2025-11-17 21:19:45.466066\n============================================================\n\nAttempting to call LLM, try 1\n[31/36] seq_len=168 label_len=96 e_layers=6 n_heads=16 factor=3 d_model=512 d_ff=3072 d_layers=3 dropout=0.05 Use GPU: cuda:0\ntrain 11859\nval 1575\ntest 3317\n\titers: 100, epoch: 1 | loss: 0.1857644\n\tspeed: 0.2949s/iter; left time: 625.5272s\n\titers: 200, epoch: 1 | loss: 0.2285168\n\tspeed: 0.2797s/iter; left time: 565.3697s\n\titers: 300, epoch: 1 | loss: 0.1545940\n\tspeed: 0.2892s/iter; left time: 555.4618s\nEpoch: 1 cost time: 106.00029802322388\nEpoch: 1, Steps: 370 | Train Loss: 0.2238136 Vali Loss: 0.8656921 Test Loss: 0.3196433\nValidation loss decreased (inf --> 0.865692).  Saving model ...\nUpdating learning rate to 0.0001\n\titers: 100, epoch: 2 | loss: 0.1142543\n\tspeed: 0.6839s/iter; left time: 1197.4987s\n\titers: 200, epoch: 2 | loss: 0.1047616\n\tspeed: 0.3039s/iter; left time: 501.6721s\n\titers: 300, epoch: 2 | loss: 0.0841803\n\tspeed: 0.3053s/iter; left time: 473.5923s\nEpoch: 2 cost time: 113.00149488449097\nEpoch: 2, Steps: 370 | Train Loss: 0.1114614 Vali Loss: 0.9522000 Test Loss: 0.2895295\nEarlyStopping counter: 1 out of 3\nUpdating learning rate to 5e-05\n\titers: 100, epoch: 3 | loss: 0.0703169\n\tspeed: 0.6883s/iter; left time: 950.5434s\n\titers: 200, epoch: 3 | loss: 0.0724405\n\tspeed: 0.3037s/iter; left time: 388.9867s\n\titers: 300, epoch: 3 | loss: 0.0591946\n\tspeed: 0.3049s/iter; left time: 360.0316s\nEpoch: 3 cost time: 112.49708795547485\nEpoch: 3, Steps: 370 | Train Loss: 0.0639021 Vali Loss: 0.8387986 Test Loss: 0.2879713\nValidation loss decreased (0.865692 --> 0.838799).  Saving model ...\nUpdating learning rate to 2.5e-05\n\titers: 100, epoch: 4 | loss: 0.0543235\n\tspeed: 0.6928s/iter; left time: 700.3775s\n\titers: 200, epoch: 4 | loss: 0.0555918\n\tspeed: 0.3064s/iter; left time: 279.1320s\n\titers: 300, epoch: 4 | loss: 0.0491072\n\tspeed: 0.3033s/iter; left time: 246.0146s\nEpoch: 4 cost time: 112.67193341255188\nEpoch: 4, Steps: 370 | Train Loss: 0.0510827 Vali Loss: 0.7170400 Test Loss: 0.2481072\nValidation loss decreased (0.838799 --> 0.717040).  Saving model ...\nUpdating learning rate to 1.25e-05\n\titers: 100, epoch: 5 | loss: 0.0439058\n\tspeed: 0.6953s/iter; left time: 445.7107s\n\titers: 200, epoch: 5 | loss: 0.0475999\n\tspeed: 0.3041s/iter; left time: 164.4998s\n\titers: 300, epoch: 5 | loss: 0.0372601\n\tspeed: 0.3065s/iter; left time: 135.1825s\nEpoch: 5 cost time: 112.85484099388123\nEpoch: 5, Steps: 370 | Train Loss: 0.0429114 Vali Loss: 0.7506828 Test Loss: 0.2576059\nEarlyStopping counter: 1 out of 3\nUpdating learning rate to 6.25e-06\n\titers: 100, epoch: 6 | loss: 0.0418786\n\tspeed: 0.6883s/iter; left time: 186.5197s\n\titers: 200, epoch: 6 | loss: 0.0400921\n\tspeed: 0.3056s/iter; left time: 52.2561s\n\titers: 300, epoch: 6 | loss: 0.0387307\n\tspeed: 0.3040s/iter; left time: 21.5809s\nEpoch: 6 cost time: 112.63259148597717\nEpoch: 6, Steps: 370 | Train Loss: 0.0384087 Vali Loss: 0.7536756 Test Loss: 0.2583747\nEarlyStopping counter: 2 out of 3\nUpdating learning rate to 3.125e-06\nval 1575\ntest shape: (49, 32, 168, 1) (49, 32, 168, 1)\ntest shape: (1568, 168, 1) (1568, 168, 1)\nmse:0.7182216644287109, mae:0.7561336159706116\ntest 3317\ntest shape: (103, 32, 168, 1) (103, 32, 168, 1)\ntest shape: (3296, 168, 1) (3296, 168, 1)\nmse:0.24762427806854248, mae:0.3952986001968384\n→ MSE_val=0.7182 MAE_val=0.7561\n\n[32/36] seq_len=720 label_len=48 e_layers=2 n_heads=8 factor=10 d_model=256 d_ff=1024 d_layers=1 dropout=0.1 Use GPU: cuda:0\ntrain 11307\nval 1575\ntest 3317\n\titers: 100, epoch: 1 | loss: 0.1674704\n\tspeed: 0.1090s/iter; left time: 220.1092s\n\titers: 200, epoch: 1 | loss: 0.1805222\n\tspeed: 0.1091s/iter; left time: 209.3842s\n\titers: 300, epoch: 1 | loss: 0.2019757\n\tspeed: 0.1094s/iter; left time: 199.0210s\nEpoch: 1 cost time: 38.77087211608887\nEpoch: 1, Steps: 353 | Train Loss: 0.1973069 Vali Loss: 0.2612067 Test Loss: 0.1617907\nValidation loss decreased (inf --> 0.261207).  Saving model ...\nUpdating learning rate to 0.0001\n\titers: 100, epoch: 2 | loss: 0.1365722\n\tspeed: 0.2629s/iter; left time: 438.0120s\n\titers: 200, epoch: 2 | loss: 0.1309364\n\tspeed: 0.1099s/iter; left time: 172.1655s\n\titers: 300, epoch: 2 | loss: 0.1400623\n\tspeed: 0.1096s/iter; left time: 160.6208s\nEpoch: 2 cost time: 38.73856806755066\nEpoch: 2, Steps: 353 | Train Loss: 0.1361866 Vali Loss: 0.3781576 Test Loss: 0.2164562\nEarlyStopping counter: 1 out of 3\nUpdating learning rate to 5e-05\n\titers: 100, epoch: 3 | loss: 0.1143612\n\tspeed: 0.2608s/iter; left time: 342.4694s\n\titers: 200, epoch: 3 | loss: 0.1115051\n\tspeed: 0.1096s/iter; left time: 132.9481s\n\titers: 300, epoch: 3 | loss: 0.0901957\n\tspeed: 0.1097s/iter; left time: 122.1290s\nEpoch: 3 cost time: 38.72343969345093\nEpoch: 3, Steps: 353 | Train Loss: 0.1095962 Vali Loss: 0.2272141 Test Loss: 0.1454441\nValidation loss decreased (0.261207 --> 0.227214).  Saving model ...\nUpdating learning rate to 2.5e-05\n\titers: 100, epoch: 4 | loss: 0.1001111\n\tspeed: 0.2624s/iter; left time: 251.8861s\n\titers: 200, epoch: 4 | loss: 0.0881377\n\tspeed: 0.1096s/iter; left time: 94.2737s\n\titers: 300, epoch: 4 | loss: 0.1080100\n\tspeed: 0.1096s/iter; left time: 83.2785s\nEpoch: 4 cost time: 38.704628705978394\nEpoch: 4, Steps: 353 | Train Loss: 0.0985667 Vali Loss: 0.3174471 Test Loss: 0.1786825\nEarlyStopping counter: 1 out of 3\nUpdating learning rate to 1.25e-05\n\titers: 100, epoch: 5 | loss: 0.0930785\n\tspeed: 0.2615s/iter; left time: 158.7229s\n\titers: 200, epoch: 5 | loss: 0.0753222\n\tspeed: 0.1099s/iter; left time: 55.7270s\n\titers: 300, epoch: 5 | loss: 0.0809456\n\tspeed: 0.1091s/iter; left time: 44.4212s\nEpoch: 5 cost time: 38.67408013343811\nEpoch: 5, Steps: 353 | Train Loss: 0.0926792 Vali Loss: 0.3251476 Test Loss: 0.1888247\nEarlyStopping counter: 2 out of 3\nUpdating learning rate to 6.25e-06\n\titers: 100, epoch: 6 | loss: 0.0961407\n\tspeed: 0.2608s/iter; left time: 66.2420s\n\titers: 200, epoch: 6 | loss: 0.0876246\n\tspeed: 0.1108s/iter; left time: 17.0687s\n\titers: 300, epoch: 6 | loss: 0.0968116\n\tspeed: 0.1093s/iter; left time: 5.9028s\nEpoch: 6 cost time: 38.72766184806824\nEpoch: 6, Steps: 353 | Train Loss: 0.0901690 Vali Loss: 0.3014738 Test Loss: 0.1811720\nEarlyStopping counter: 3 out of 3\nEarly stopping\nval 1575\ntest shape: (49, 32, 168, 1) (49, 32, 168, 1)\ntest shape: (1568, 168, 1) (1568, 168, 1)\nmse:0.2266232818365097, mae:0.38383445143699646\ntest 3317\ntest shape: (103, 32, 168, 1) (103, 32, 168, 1)\ntest shape: (3296, 168, 1) (3296, 168, 1)\nmse:0.1460462361574173, mae:0.31114792823791504\n→ MSE_val=0.2266 MAE_val=0.3838\n\n------------------------------------------------------------\nAttempting to call LLM, try 1\n[33/36] seq_len=168 label_len=48 e_layers=6 n_heads=16 factor=3 d_model=768 d_ff=2048 d_layers=1 dropout=0.1 Use GPU: cuda:0\ntrain 11859\nval 1575\ntest 3317\n\titers: 100, epoch: 1 | loss: 0.1831692\n\tspeed: 0.2290s/iter; left time: 485.6486s\n\titers: 200, epoch: 1 | loss: 0.1754136\n\tspeed: 0.2333s/iter; left time: 471.5924s\n\titers: 300, epoch: 1 | loss: 0.1789339\n\tspeed: 0.2277s/iter; left time: 437.4449s\nEpoch: 1 cost time: 85.198894739151\nEpoch: 1, Steps: 370 | Train Loss: 0.2265966 Vali Loss: 0.9921998 Test Loss: 0.3474601\nValidation loss decreased (inf --> 0.992200).  Saving model ...\nUpdating learning rate to 0.0001\n\titers: 100, epoch: 2 | loss: 0.1297610\n\tspeed: 0.5265s/iter; left time: 921.8660s\n\titers: 200, epoch: 2 | loss: 0.1404936\n\tspeed: 0.2296s/iter; left time: 379.1045s\n\titers: 300, epoch: 2 | loss: 0.1075368\n\tspeed: 0.2284s/iter; left time: 354.2446s\nEpoch: 2 cost time: 85.0103075504303\nEpoch: 2, Steps: 370 | Train Loss: 0.1220048 Vali Loss: 0.6503745 Test Loss: 0.2233286\nValidation loss decreased (0.992200 --> 0.650375).  Saving model ...\nUpdating learning rate to 5e-05\n\titers: 100, epoch: 3 | loss: 0.0656923\n\tspeed: 0.5296s/iter; left time: 731.3952s\n\titers: 200, epoch: 3 | loss: 0.0724039\n\tspeed: 0.2293s/iter; left time: 293.7455s\n\titers: 300, epoch: 3 | loss: 0.0708129\n\tspeed: 0.2293s/iter; left time: 270.8178s\nEpoch: 3 cost time: 84.79539942741394\nEpoch: 3, Steps: 370 | Train Loss: 0.0720350 Vali Loss: 0.7841270 Test Loss: 0.2922306\nEarlyStopping counter: 1 out of 3\nUpdating learning rate to 2.5e-05\n\titers: 100, epoch: 4 | loss: 0.0606769\n\tspeed: 0.5236s/iter; left time: 529.4066s\n\titers: 200, epoch: 4 | loss: 0.0629766\n\tspeed: 0.2297s/iter; left time: 209.2719s\n\titers: 300, epoch: 4 | loss: 0.0516947\n\tspeed: 0.2289s/iter; left time: 185.6237s\nEpoch: 4 cost time: 84.92978811264038\nEpoch: 4, Steps: 370 | Train Loss: 0.0570645 Vali Loss: 0.7694517 Test Loss: 0.2937668\nEarlyStopping counter: 2 out of 3\nUpdating learning rate to 1.25e-05\n\titers: 100, epoch: 5 | loss: 0.0525678\n\tspeed: 0.5257s/iter; left time: 336.9802s\n\titers: 200, epoch: 5 | loss: 0.0528395\n\tspeed: 0.2291s/iter; left time: 123.9253s\n\titers: 300, epoch: 5 | loss: 0.0513846\n\tspeed: 0.2299s/iter; left time: 101.4006s\nEpoch: 5 cost time: 84.9778892993927\nEpoch: 5, Steps: 370 | Train Loss: 0.0510912 Vali Loss: 0.7326351 Test Loss: 0.2778171\nEarlyStopping counter: 3 out of 3\nEarly stopping\nval 1575\ntest shape: (49, 32, 168, 1) (49, 32, 168, 1)\ntest shape: (1568, 168, 1) (1568, 168, 1)\nmse:0.650489330291748, mae:0.7250641584396362\ntest 3317\ntest shape: (103, 32, 168, 1) (103, 32, 168, 1)\ntest shape: (3296, 168, 1) (3296, 168, 1)\nmse:0.22312377393245697, mae:0.3804992437362671\n→ MSE_val=0.6505 MAE_val=0.7251\n\n[34/36] seq_len=720 label_len=336 e_layers=4 n_heads=8 factor=5 d_model=256 d_ff=1024 d_layers=3 dropout=0.2 Use GPU: cuda:0\ntrain 11307\nval 1575\ntest 3317\n\titers: 100, epoch: 1 | loss: 0.1871715\n\tspeed: 0.1969s/iter; left time: 397.4961s\n\titers: 200, epoch: 1 | loss: 0.1541338\n\tspeed: 0.1983s/iter; left time: 380.5258s\n\titers: 300, epoch: 1 | loss: 0.1674083\n\tspeed: 0.1992s/iter; left time: 362.3209s\nEpoch: 1 cost time: 70.08899426460266\nEpoch: 1, Steps: 353 | Train Loss: 0.1963840 Vali Loss: 0.3453421 Test Loss: 0.2535627\nValidation loss decreased (inf --> 0.345342).  Saving model ...\nUpdating learning rate to 0.0001\n\titers: 100, epoch: 2 | loss: 0.1324610\n\tspeed: 0.4573s/iter; left time: 761.8219s\n\titers: 200, epoch: 2 | loss: 0.1305663\n\tspeed: 0.1989s/iter; left time: 311.5065s\n\titers: 300, epoch: 2 | loss: 0.1053467\n\tspeed: 0.1989s/iter; left time: 291.6563s\nEpoch: 2 cost time: 70.24676418304443\nEpoch: 2, Steps: 353 | Train Loss: 0.1264062 Vali Loss: 0.2306609 Test Loss: 0.1692915\nValidation loss decreased (0.345342 --> 0.230661).  Saving model ...\nUpdating learning rate to 5e-05\n\titers: 100, epoch: 3 | loss: 0.1012561\n\tspeed: 0.4554s/iter; left time: 597.9905s\n\titers: 200, epoch: 3 | loss: 0.0998091\n\tspeed: 0.1982s/iter; left time: 240.4159s\n\titers: 300, epoch: 3 | loss: 0.0864098\n\tspeed: 0.1980s/iter; left time: 220.3540s\nEpoch: 3 cost time: 69.90737700462341\nEpoch: 3, Steps: 353 | Train Loss: 0.0977844 Vali Loss: 0.2132764 Test Loss: 0.1524621\nValidation loss decreased (0.230661 --> 0.213276).  Saving model ...\nUpdating learning rate to 2.5e-05\n\titers: 100, epoch: 4 | loss: 0.1020304\n\tspeed: 0.4551s/iter; left time: 436.8612s\n\titers: 200, epoch: 4 | loss: 0.0806742\n\tspeed: 0.1984s/iter; left time: 170.6612s\n\titers: 300, epoch: 4 | loss: 0.0815535\n\tspeed: 0.1986s/iter; left time: 150.9137s\nEpoch: 4 cost time: 70.21050667762756\nEpoch: 4, Steps: 353 | Train Loss: 0.0848715 Vali Loss: 0.2120513 Test Loss: 0.1654221\nValidation loss decreased (0.213276 --> 0.212051).  Saving model ...\nUpdating learning rate to 1.25e-05\n\titers: 100, epoch: 5 | loss: 0.0749205\n\tspeed: 0.4565s/iter; left time: 277.1207s\n\titers: 200, epoch: 5 | loss: 0.0732078\n\tspeed: 0.2007s/iter; left time: 101.7544s\n\titers: 300, epoch: 5 | loss: 0.0746005\n\tspeed: 0.1986s/iter; left time: 80.8171s\nEpoch: 5 cost time: 70.27241826057434\nEpoch: 5, Steps: 353 | Train Loss: 0.0789991 Vali Loss: 0.2169250 Test Loss: 0.1738423\nEarlyStopping counter: 1 out of 3\nUpdating learning rate to 6.25e-06\n\titers: 100, epoch: 6 | loss: 0.0782677\n\tspeed: 0.4561s/iter; left time: 115.8464s\n\titers: 200, epoch: 6 | loss: 0.0679796\n\tspeed: 0.1990s/iter; left time: 30.6529s\n\titers: 300, epoch: 6 | loss: 0.0711869\n\tspeed: 0.1987s/iter; left time: 10.7308s\nEpoch: 6 cost time: 70.42410945892334\nEpoch: 6, Steps: 353 | Train Loss: 0.0762463 Vali Loss: 0.2055916 Test Loss: 0.1628438\nValidation loss decreased (0.212051 --> 0.205592).  Saving model ...\nUpdating learning rate to 3.125e-06\nval 1575\ntest shape: (49, 32, 168, 1) (49, 32, 168, 1)\ntest shape: (1568, 168, 1) (1568, 168, 1)\nmse:0.20533522963523865, mae:0.35131019353866577\ntest 3317\ntest shape: (103, 32, 168, 1) (103, 32, 168, 1)\ntest shape: (3296, 168, 1) (3296, 168, 1)\nmse:0.1631692498922348, mae:0.3221040666103363\n→ MSE_val=0.2053 MAE_val=0.3513\n\n------------------------------------------------------------\nAttempting to call LLM, try 1\n[35/36] seq_len=720 label_len=168 e_layers=6 n_heads=8 factor=5 d_model=256 d_ff=2048 d_layers=3 dropout=0.2 Use GPU: cuda:0\ntrain 11307\nval 1575\ntest 3317\n\titers: 100, epoch: 1 | loss: 0.2248621\n\tspeed: 0.2252s/iter; left time: 454.7391s\n\titers: 200, epoch: 1 | loss: 0.2552527\n\tspeed: 0.2346s/iter; left time: 450.2013s\n\titers: 300, epoch: 1 | loss: 0.1346199\n\tspeed: 0.2233s/iter; left time: 406.1818s\nEpoch: 1 cost time: 80.24438095092773\nEpoch: 1, Steps: 353 | Train Loss: 0.2098455 Vali Loss: 0.3852918 Test Loss: 0.1994591\nValidation loss decreased (inf --> 0.385292).  Saving model ...\nUpdating learning rate to 0.0001\n\titers: 100, epoch: 2 | loss: 0.1491296\n\tspeed: 0.5029s/iter; left time: 837.7634s\n\titers: 200, epoch: 2 | loss: 0.1539444\n\tspeed: 0.2260s/iter; left time: 353.8393s\n\titers: 300, epoch: 2 | loss: 0.1367685\n\tspeed: 0.2238s/iter; left time: 328.0463s\nEpoch: 2 cost time: 79.35659766197205\nEpoch: 2, Steps: 353 | Train Loss: 0.1317601 Vali Loss: 0.3080772 Test Loss: 0.1771637\nValidation loss decreased (0.385292 --> 0.308077).  Saving model ...\nUpdating learning rate to 5e-05\n\titers: 100, epoch: 3 | loss: 0.1052484\n\tspeed: 0.5007s/iter; left time: 657.4438s\n\titers: 200, epoch: 3 | loss: 0.1011191\n\tspeed: 0.2249s/iter; left time: 272.8621s\n\titers: 300, epoch: 3 | loss: 0.1009781\n\tspeed: 0.2262s/iter; left time: 251.7798s\nEpoch: 3 cost time: 79.37857627868652\nEpoch: 3, Steps: 353 | Train Loss: 0.0974808 Vali Loss: 0.2232985 Test Loss: 0.1990862\nValidation loss decreased (0.308077 --> 0.223298).  Saving model ...\nUpdating learning rate to 2.5e-05\n\titers: 100, epoch: 4 | loss: 0.0852474\n\tspeed: 0.5016s/iter; left time: 481.5587s\n\titers: 200, epoch: 4 | loss: 0.0868737\n\tspeed: 0.2237s/iter; left time: 192.4141s\n\titers: 300, epoch: 4 | loss: 0.0819838\n\tspeed: 0.2234s/iter; left time: 169.7734s\nEpoch: 4 cost time: 78.96666622161865\nEpoch: 4, Steps: 353 | Train Loss: 0.0842757 Vali Loss: 0.2472534 Test Loss: 0.2490163\nEarlyStopping counter: 1 out of 3\nUpdating learning rate to 1.25e-05\n\titers: 100, epoch: 5 | loss: 0.0880376\n\tspeed: 0.5014s/iter; left time: 304.3545s\n\titers: 200, epoch: 5 | loss: 0.0820576\n\tspeed: 0.2250s/iter; left time: 114.0951s\n\titers: 300, epoch: 5 | loss: 0.0753829\n\tspeed: 0.2264s/iter; left time: 92.1643s\nEpoch: 5 cost time: 79.57226753234863\nEpoch: 5, Steps: 353 | Train Loss: 0.0779048 Vali Loss: 0.2428957 Test Loss: 0.2686003\nEarlyStopping counter: 2 out of 3\nUpdating learning rate to 6.25e-06\n\titers: 100, epoch: 6 | loss: 0.0763883\n\tspeed: 0.5010s/iter; left time: 127.2458s\n\titers: 200, epoch: 6 | loss: 0.0701114\n\tspeed: 0.2256s/iter; left time: 34.7366s\n\titers: 300, epoch: 6 | loss: 0.0703661\n\tspeed: 0.2234s/iter; left time: 12.0617s\nEpoch: 6 cost time: 79.17458939552307\nEpoch: 6, Steps: 353 | Train Loss: 0.0749092 Vali Loss: 0.2183492 Test Loss: 0.2657202\nValidation loss decreased (0.223298 --> 0.218349).  Saving model ...\nUpdating learning rate to 3.125e-06\nval 1575\ntest shape: (49, 32, 168, 1) (49, 32, 168, 1)\ntest shape: (1568, 168, 1) (1568, 168, 1)\nmse:0.21922126412391663, mae:0.38293835520744324\ntest 3317\ntest shape: (103, 32, 168, 1) (103, 32, 168, 1)\ntest shape: (3296, 168, 1) (3296, 168, 1)\nmse:0.26531070470809937, mae:0.4271477162837982\n→ MSE_val=0.2192 MAE_val=0.3829\n\n[36/36] seq_len=168 label_len=24 e_layers=4 n_heads=16 factor=10 d_model=768 d_ff=3072 d_layers=1 dropout=0.1 Use GPU: cuda:0\ntrain 11859\nval 1575\ntest 3317\n\titers: 100, epoch: 1 | loss: 0.2002064\n\tspeed: 0.2474s/iter; left time: 524.8081s\n\titers: 200, epoch: 1 | loss: 0.1920674\n\tspeed: 0.2492s/iter; left time: 503.7139s\n\titers: 300, epoch: 1 | loss: 0.1960395\n\tspeed: 0.2495s/iter; left time: 479.2694s\nEpoch: 1 cost time: 92.09955191612244\nEpoch: 1, Steps: 370 | Train Loss: 0.2211647 Vali Loss: 0.9338270 Test Loss: 0.2645378\nValidation loss decreased (inf --> 0.933827).  Saving model ...\nUpdating learning rate to 0.0001\n\titers: 100, epoch: 2 | loss: 0.1624470\n\tspeed: 0.5779s/iter; left time: 1011.8183s\n\titers: 200, epoch: 2 | loss: 0.0933724\n\tspeed: 0.2492s/iter; left time: 411.5083s\n\titers: 300, epoch: 2 | loss: 0.0919038\n\tspeed: 0.2483s/iter; left time: 385.0913s\nEpoch: 2 cost time: 91.91838097572327\nEpoch: 2, Steps: 370 | Train Loss: 0.1156680 Vali Loss: 1.2086378 Test Loss: 0.4153687\nEarlyStopping counter: 1 out of 3\nUpdating learning rate to 5e-05\n\titers: 100, epoch: 3 | loss: 0.0726154\n\tspeed: 0.5769s/iter; left time: 796.6678s\n\titers: 200, epoch: 3 | loss: 0.0715827\n\tspeed: 0.2495s/iter; left time: 319.6681s\n\titers: 300, epoch: 3 | loss: 0.0588637\n\tspeed: 0.2490s/iter; left time: 294.0645s\nEpoch: 3 cost time: 92.01353931427002\nEpoch: 3, Steps: 370 | Train Loss: 0.0685827 Vali Loss: 0.9620883 Test Loss: 0.3364116\nEarlyStopping counter: 2 out of 3\nUpdating learning rate to 2.5e-05\n\titers: 100, epoch: 4 | loss: 0.0525768\n\tspeed: 0.5757s/iter; left time: 582.0327s\n\titers: 200, epoch: 4 | loss: 0.0501366\n\tspeed: 0.2489s/iter; left time: 226.7897s\n\titers: 300, epoch: 4 | loss: 0.0500264\n\tspeed: 0.2485s/iter; left time: 201.5551s\nEpoch: 4 cost time: 91.90335202217102\nEpoch: 4, Steps: 370 | Train Loss: 0.0548417 Vali Loss: 0.8726409 Test Loss: 0.3095337\nValidation loss decreased (0.933827 --> 0.872641).  Saving model ...\nUpdating learning rate to 1.25e-05\n\titers: 100, epoch: 5 | loss: 0.0464547\n\tspeed: 0.5817s/iter; left time: 372.8925s\n\titers: 200, epoch: 5 | loss: 0.0531416\n\tspeed: 0.2491s/iter; left time: 134.7441s\n\titers: 300, epoch: 5 | loss: 0.0561237\n\tspeed: 0.2485s/iter; left time: 109.5842s\nEpoch: 5 cost time: 92.11530208587646\nEpoch: 5, Steps: 370 | Train Loss: 0.0491542 Vali Loss: 0.9781008 Test Loss: 0.3322508\nEarlyStopping counter: 1 out of 3\nUpdating learning rate to 6.25e-06\n\titers: 100, epoch: 6 | loss: 0.0416978\n\tspeed: 0.5766s/iter; left time: 156.2474s\n\titers: 200, epoch: 6 | loss: 0.0455393\n\tspeed: 0.2495s/iter; left time: 42.6631s\n\titers: 300, epoch: 6 | loss: 0.0522204\n\tspeed: 0.2483s/iter; left time: 17.6272s\nEpoch: 6 cost time: 91.98375082015991\nEpoch: 6, Steps: 370 | Train Loss: 0.0466456 Vali Loss: 0.9065667 Test Loss: 0.3156312\nEarlyStopping counter: 2 out of 3\nUpdating learning rate to 3.125e-06\nval 1575\ntest shape: (49, 32, 168, 1) (49, 32, 168, 1)\ntest shape: (1568, 168, 1) (1568, 168, 1)\nmse:0.8714888691902161, mae:0.8431127071380615\ntest 3317\ntest shape: (103, 32, 168, 1) (103, 32, 168, 1)\ntest shape: (3296, 168, 1) (3296, 168, 1)\nmse:0.3098931908607483, mae:0.44893085956573486\n→ MSE_val=0.8715 MAE_val=0.8431\n\n------------------------------------------------------------\n\n>>>MSE=0.1460 MAE=0.3111 seq=720 lbl=48 e=2 h=8 f=10\n\n","output_type":"stream"},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"CompletedProcess(args=['python', 'optimize.py', '--dataset', 'data/ETTh1_prepared.csv', '--pred_len', '168', '--journal', 'ETTh1-grid3_pred_len=168;th_b=8192;hypot.json', '--n_batches', '3', '--batch_size', '2', '--temperature', '1', '--thinking_budget', '8192', '--grid_name', 'grid3'], returncode=0)"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:16:05.419952Z","iopub.execute_input":"2025-11-17T21:16:05.420136Z","iopub.status.idle":"2025-11-17T21:16:05.536721Z","shell.execute_reply.started":"2025-11-17T21:16:05.420118Z","shell.execute_reply":"2025-11-17T21:16:05.535819Z"}},"outputs":[{"name":"stdout","text":" config.py\n data\n'ETTh1-grid3_pred_len=24;th_b=8192;hypot.json'\n experiments\n Informer2020\n informer_checkpoints\n llm_opt_toolkit\n nas-informer.ipynb\n optimize.py\n'pred_len=24;th_b=8192;hypot.json'\n'pred_len=24;th_b=8192;hypot.json.1'\n'pred_len=24;th_b=8192;hypot.json.2'\n __pycache__\n pyproject.toml\n README.md\n results\n test_functions.ipynb\n uv.lock\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}